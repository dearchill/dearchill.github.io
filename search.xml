<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>snorkel初探</title>
    <url>/2019/11/25/snorkel%E5%88%9D%E6%8E%A2/</url>
    <content><![CDATA[<p>远程监督是关系抽取中最常采用的方法，它的核心思想是将文本与大规模知识图谱进行实体对齐，利用知识图谱已有的实体间关系对文本进行标注，可以有效解决有监督学习抽取样本过少的问题。而snorkel是斯坦福大学deepdive框架的后续项目，它将deepdive中的远程监督和弱监督的思想进一步完善，并用纯python的形式构成了一套完整的学习框架。我们的关系抽取项目采用snorkel与深度学习相结合的方法。</p>
<a id="more"></a>
<p>人工标记训练数据费时费力，所以最好的方式是利用外部知识库、模式/规则或其他分类器来启发式地生成训练数据。从本质上来讲，这些都是以编程方式生成训练数据的方法，或者更简洁地说就是编程训练数据。</p>
<p>远程监督基本假设是：如果从知识图谱中可获取三元组R(E1, E2)（注：R代表关系，E1、E2代表两个实体），且E1和E2共现与句子S中，则S表达了E1和E2间的关系R，标注为训练正例。由于假设过强，会出现wrong label的问题，常采用多实例学习的方法解决，将所有包含该关系的句子组成一个bag然后筛选句子生成训练样本。后来提出multi-instance multi-labels，即一个句子只能表达E1和E2的一种关系，但是不同的句子可以表达多个关系即labels，为同时挖掘实体对的多个关系提供了可能。</p>
<p>snorkel采用的方法是用户编写标签函数(LFs)，然后利用生成模型学习标签函数准确性，输出概率训练标签，来训练神经网络判别模型，流程如下：</p>
<img src="/2019/11/25/snorkel初探/1.png">
<img src="/2019/11/25/snorkel初探/2.png">
<img src="/2019/11/25/snorkel初探/3.png">
]]></content>
  </entry>
  <entry>
    <title>脑区连接关系提取</title>
    <url>/2019/11/21/%E8%84%91%E5%8C%BA%E8%BF%9E%E6%8E%A5%E5%85%B3%E7%B3%BB%E6%8F%90%E5%8F%96/</url>
    <content><![CDATA[<p>大多数生物医学领域的关系提取都是面向通用的基因、化学物质、疾病间的，很少研究大脑神经投射和连接性的。调研找到的文献不多，这里汇总一下：</p>
<a id="more"></a>
<h4 id="Automated-recognition-of-brain-region-mentions-in-neuroscience-literature-2009"><a href="#Automated-recognition-of-brain-region-mentions-in-neuroscience-literature-2009" class="headerlink" title="Automated recognition of brain region mentions in neuroscience literature(2009)"></a>Automated recognition of brain region mentions in neuroscience literature(2009)</h4><p>课题意义是建立与脑区有关的基因表达和模拟连接，为此他们提出了whitetext项目，手工标注了1377篇神经科学文献摘要中的18242个脑区，然后采用几种方法识别并评估了识别精度：基于词典（被词典大小严重限制导致召回率低）和CRF；还分析了词窗大小、词干化处理和缩写扩展对识别精度的影响。</p>
<ol>
<li><p>语料库建立方法：依据关键词检索、随机挑选Journal of Comparative Neurology期刊文章、mesh匹配的方法选择了1377篇尽可能提及脑区的文章，以xml格式存储，然后使用缩写扩增算法将文中提及的全称或缩写统一用”全称(缩写)”的形式扩增。然后请人依照脑图谱和词典手工标注，标注范围不能太宽泛(不标注system级别的)，也不能太精细(不具体到核团的某一层)，标注示例： “motor related areas of the hippocampus”.</p>
</li>
<li><p>基于词典的匹配： 使用的词典：Neuronames，Nomenclatures of Canonical Mouse，Rat Brain Atlases and the Ontology of Human and Macaque Neuroanatomy.  匹配方法：GATE Gazetteer</p>
</li>
<li><p>条件随机场：相比HMM优点是当前状态的概率依据整个输入序列tokens计算而不仅仅是前一状态的token，通过定义feature function组合features</p>
</li>
<li><p>features的选择：拼写特征(如大写字母或者数字)和词性特征POS tagging；词干特征（用GENIA biomedical corpus训练的TreeTagger标注）；上下文特征：n-grams，word window；还有一些描述脑区边界的特征词：(e.g. bank, sulci, surface,area), neuroanatomical directions (e.g. dorsal, superior), root neuroscience terms (e.g. chiasm, raphe, striated). 识别结果如下：，</p>
<img src="/2019/11/21/脑区连接关系提取/1.png">
</li>
</ol>
<h4 id="Application-and-evaluation-of-automated-methods-to-extract-neuroanatomical-connectivity-statements-from-free-text-2012"><a href="#Application-and-evaluation-of-automated-methods-to-extract-neuroanatomical-connectivity-statements-from-free-text-2012" class="headerlink" title="Application and evaluation of automated methods to extract neuroanatomical connectivity statements from free text(2012)"></a>Application and evaluation of automated methods to extract neuroanatomical connectivity statements from free text(2012)</h4><p>接上一篇，继续标注了1377篇the Journal of Comparative Neurology文献的神经模拟连接，只考虑连接关系，不考虑连接属性、强度和方向，然后用基于词语共现和规则的方法，基于POS、依存解析和句法特征的机器学习方法抽取关系做测试，证实了之前抽取PPI的几种核方法可以用于抽取脑连接关系，并建立了完整的脑区识别和关系识别流程。这是大规模文本挖掘在神经解剖学连接抽取的首次应用。</p>
<ol>
<li><p>金标准标注：先标注所有脑区，然后标注所有的脑区连接，不标注白质和神经冲动，仅标注突触或直接连接。</p>
</li>
<li><p>基于共现的方法：在单个句子内共现或整个摘要内共现；基于规则：限制脑区提及数目，限制连接关键词(‘afferent’, ‘efferent’, ‘projects’, ‘projection’, ‘pathway’ or ‘inputs’)</p>
</li>
<li><p>四种基于核函数的统计机器学习方法：句法和依存解析核，all-paths graph kernel，k-band shortest path spectrum kernel，使用单词共现和POS的浅层语义核 (SLK)，精度比较如图：</p>
<img src="/2019/11/21/脑区连接关系提取/2.png">
</li>
<li><p>然后用slk做关系抽取，将抽取的连接关系与现存的脑模拟连接数据库如<a href="https://bams1.org" target="_blank" rel="noopener">BAMS</a>作比较（normalization)，抽取及规范化流程如下：</p>
<img src="/2019/11/21/脑区连接关系提取/3.png">
<p>将提取到的关系映射到标准化的BAMS词典比较困难（63％），采用关系同义词匹配或是用共现代替关系的方法可以改善匹配率，但会降低准确率。</p>
</li>
</ol>
<h4 id="Text-mining-for-neuroanatomy-using-WhiteText-with-an-updated-corpus-and-a-new-web-application-2015"><a href="#Text-mining-for-neuroanatomy-using-WhiteText-with-an-updated-corpus-and-a-new-web-application-2015" class="headerlink" title="Text mining for neuroanatomy using WhiteText with an updated corpus and a new web application(2015)"></a>Text mining for neuroanatomy using WhiteText with an updated corpus and a new web application(2015)</h4><p>接上两篇，先总结了一下过去的结果：</p>
<img src="/2019/11/21/脑区连接关系提取/4.png">
<ol>
<li>脑区识别：对标注的1377篇摘要进行脑区NER，采用条件随机场，八折交叉验证，recalled 76% of brain region mentions at 81% precision</li>
<li>脑区标准化/规范化：采用了5个跨物种的神经科学词典（NeuroNames, NIFSTD, the Brede Database, BAMS and AMBA)，它们提供了大约1000个脑区的11,909 个名称。尝试词典匹配（先精确匹配，然后忽略词序的词袋匹配，最后词干匹配），设计修改规则提高精度（P 95％ R 63％）</li>
<li>脑区连接：多数提及的脑区连接是负样本(no connectivity )，采用浅层语义核(slk)能达到P 50％ R 70％</li>
</ol>
<p>然后用半自动抽取+手工修正的方式创建了新数据集（加入了1828篇摘要和2111个连接关系），用MScanner 工具扩展标注其他期刊的8264篇JCN文献模拟连接信息。</p>
<img src="/2019/11/21/脑区连接关系提取/5.png">
<p>最后还将结果展示在<a href="https://whitetext.msl.ubc.ca" target="_blank" rel="noopener">网页上</a>，提供查询和导出（Indexing 71306 sentences with 68957 connections between 88088 brain region mentions.），查询关键词是与NIFSTD中的名词匹配的。</p>
<p>不足之处：仅摘要、单个句子、关系无方向性</p>
<h4 id="Extracting-Relation-between-Brain-Region-pairs-from-White-Text-2017"><a href="#Extracting-Relation-between-Brain-Region-pairs-from-White-Text-2017" class="headerlink" title="Extracting Relation between Brain Region pairs from White Text(2017)"></a>Extracting Relation between Brain Region pairs from White Text(2017)</h4><p>创新之处是运用graph Kernel的方法抽取在同一个句子中出现不止两个脑区实体间的关系。</p>
<img src="/2019/11/21/脑区连接关系提取/6.png">
<p>方法就是对whitetext语料，选择词的tf-idf作特征，加入基于图的浅层语义核SLK还有解析树核DTK得到特征向量，用SVM分类器输出每对实体的关系。其中baseline为只用tf-idf或BOW作特征，结果如下：</p>
<img src="/2019/11/21/脑区连接关系提取/7.png">
<h4 id="Large-scale-extraction-of-brain-connectivity-from-the-neuroscientific-literature-2015"><a href="#Large-scale-extraction-of-brain-connectivity-from-the-neuroscientific-literature-2015" class="headerlink" title="Large-scale extraction of brain connectivity from the neuroscientific literature(2015)"></a>Large-scale extraction of brain connectivity from the neuroscientific literature(2015)</h4><h5 id="Introduction："><a href="#Introduction：" class="headerlink" title="Introduction："></a>Introduction：</h5><ol>
<li>神经科学的结果、知识和数据分散在期刊中，访问效率很低。脑连接性数据有这些来源：1. Allen Mouse Brain Connectivity Atlas(AMBCA)，2. BAMS（包含600多篇手工整理的文献数据）3. 神经科学文章，采用传统手工搜索的方法有一些局限：命名系统的多样性导致手动搜索的不确定性，还有召回率低（缺少缩写扩展和同义词扩展），精度低（共现关系不代表相关关系）。</li>
<li>信息抽取综述：IE = NER+RE，其中NER最简单的方式是直接匹配（Lexical matching ）字典中的实体（缺乏同义词，而且过于具体，导致召回率低），稍复杂的方式是有监督学习；RE多采用基于规则和基于机器学习(P 50.3% R 70.1%)</li>
</ol>
<h5 id="Methods-and-Results"><a href="#Methods-and-Results" class="headerlink" title="Methods and Results:"></a>Methods and Results:</h5><ol>
<li><p>Brain region NERs: 词典匹配的方法处理同义词、扩展缩写；机器学习的方法在CRF上发掘更多特征</p>
</li>
<li><p>Connectivity Extractors: FILTER的方法采用过滤器过滤脑区共现的句子，KERNEL的方法同采用浅层语义解析，RULES的方法基于九种规则类似于‘projection from the region A (of the region B) to the region C and the region D’ </p>
</li>
<li><p>NER结果：基于词典的方式无论采用精确匹配还是粗略匹配召回率都不高，但是用于非常大的语料库注重精度时，可以采用基于词典匹配的方法</p>
<img src="/2019/11/21/脑区连接关系提取/8.png"> 
</li>
<li><p>RE结果（不考虑有1/4是跨句子的连接关系）：基于FILTER召回率高，基于RULES精确率高，基于KERNEL适中，对于大规模的语料库需要提高精度可以采用组合的方式。</p>
<img src="/2019/11/21/脑区连接关系提取/9.png"> 
</li>
</ol>
<h5 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation:"></a>Evaluation:</h5><img src="/2019/11/21/脑区连接关系提取/10.png"> 
<p>对13.2 million篇PubMed摘要和630 216篇PMC全文，用上图pipeline做了提取，并统计了内部模型一致性和外部数据库AMBCA匹配一致性。</p>
<img src="/2019/11/21/脑区连接关系提取/11.png"> 
<p>最后总结这项工作的意义不是替代人抽取关系，而是提供一个平台让研究人员关注更可能出现的连接性关系，还有对模型不一致的情况进行手动筛选和评估（包括POS和NEG关系）。开源了抽取框架<a href="https://github.com/BlueBrain/bluima" target="_blank" rel="noopener">bluima</a></p>
<p>之后可以在框架内改进模型比如加入深度学习。</p>
]]></content>
  </entry>
  <entry>
    <title>知识库流程综述v1</title>
    <url>/2019/07/30/%E7%9F%A5%E8%AF%86%E5%BA%93%E6%B5%81%E7%A8%8B%E7%BB%BC%E8%BF%B0v1/</url>
    <content><![CDATA[<p>这篇应该算是知识库构建的整个流程，之后可能需要跟老师交流后增减一些东西。</p>
<a id="more"></a>
<ol>
<li><p>文献数据获取及预处理：</p>
<p>思路：先获取所有与脑区核团相关的Pubmed文献，脑区核团名称来自于Allen本体，获取的数据格式一般为txt，之后，对获取数据进行数据预处理，转化为json格式或其他需要的格式。</p>
<p>难点：①获取完整全面的文献数据，需要考虑各个核团名称的缩写词、同义词；</p>
<p>②获取的数据编码为utf-8，但部分数据存在乱码现象，需要做一个预处理。</p>
<p>思考：①只获取与核团相关的文献构建知识库可能不够，所以需要获取更多的文献导入进知识库中；</p>
<p>②需要获取PMC的文献，对全文数据进行检索，使数据库变得更加丰富。</p>
</li>
<li><p>NER（实体抽取）：</p>
<p>接下来，使用现有工具对获取到的文献进行命名实体识别，具体工具有pubtator和saber，这两个工具主要识别基因（gene）、疾病（disease）、化学物质（chemical）三类实体。</p>
<p>Pubtator：将获取文献的pmid整理成一个txt，输入给pubtator，返回标注后的结果，该方法速度快，1000篇文献需要30-40s；</p>
<p>Saber：将获取的原始文献输入给saber，返回标注结果，该方法的速度较慢，10篇文献需要20s左右。</p>
<p>难点：①saber方法返回速度较慢，处理所有文献需要的时间过长，需要考虑是否继续使用该方法。具体思路：先将获取的文献进行抽样，将抽样的文献输入给该系统，进行NER，将获取的结果与pubtator进行比较，提取出saber识别的但pubtator不存在的实体进行评估，将该实体与标准字典进行匹配，计算数据的准确率，以此确认该方法是否继续使用。</p>
<p>②需要考虑获取的三种类别是否能够满足知识库的需求，是否需要继续寻找其他可用工具。</p>
</li>
<li><p>NEL（实体链接）（进行中）：</p>
<p>获取的命名实体数据主要包括实体名称，外部数据库编号，将这部分数据与标准字典（UMLS）进行匹配，将匹配到的数据保留下来，未匹配到的数据进行剔除。</p>
<p>难点：①首先要构建标准字典，讨论确定标准字典的数据集；</p>
<p>②实体名称字段进行精确匹配会存在很大的问题。首先，文献中实体名称不一定是标准名称，有可能出现匹配不到的情况；其次，识别保存结果可能与标准名称存在格式差异，也会导致匹配不到相应结果的情况；</p>
<p>③外部数据库编号匹配也可能存在问题。首先，不同NER系统给出的外部数据库编号之间可能存在差异，因此，和标准数据库匹配时可能存在不匹配的问题。</p>
</li>
<li><p>RE（关系抽取）（进行中）：</p>
<p>利用现有工具对文献中实体进行关系抽取，现有工具利用Pubtator抽取文献中的实体，之后对实体进行关系抽取。接下来，需要将抽取的关系与标准数据库（SemmedDB）进行匹配。</p>
<p>难点：①现有工具的选择和确认：初步有几个项目和工具，bran，GNBR和snorkel，需要整合到标准库上</p>
<p>②标准关系类别定义：UMLS Semantic Network。现有工具抽取的关系定义与标准关系可能存在差异，需要将抽取的关系先与标准关系进行对应，此处需要人为筛选定义映射关系。之后，对抽取的关系进行筛选，将不存在的实体关系剔除掉，筛选方案需要继续讨论；</p>
<p>③需要评估关系抽取的精度，对文献检索的参考意义有多大</p>
</li>
<li><p>与本体匹配（方法已实现）：</p>
<p>暂时选用的是biolink本体，因为没有发现更适用的神经科学本体，手工制定也不太现实</p>
</li>
<li><p>数据存储：</p>
<p>将获取的文献、命名实体和实体关系全部导入MongoDB数据库中，以json格式数据进行保存；还将标准化后的关系导入图数据库neo4j方便展示</p>
</li>
<li><p>数据检索（未做）：</p>
<p>将数据库中的数据导入进elasticsearch中，利用elasticsearch进行数据检索，输入关键词后，返回相应的文献，并将文献中的实体（关系）标注出来。</p>
<p>难点：①elasticsearch搜索是基于关键词检索，会将满足关键词的信息返回，并以一定的排序算法将文献排序下来，需要思考如何改进排序算法，利用实体和关系将用户最需要的文献返回回来；</p>
<p>②需要将文献中的实体进行高亮标注，并以不同的颜色代表不同的实体（此步骤需要前端人员编写相应的函数）；</p>
<p>③需要思考如何将获取的关系在搜索结果中做进一步展示。</p>
</li>
<li><p>数据表示（未做）：</p>
<p>将检索的结果以网页可视化的形式展示出来。</p>
<p>难点：①需要懂前端的人员进行构建。</p>
</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>文献阅读0726</title>
    <url>/2019/07/26/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB0726/</url>
    <content><![CDATA[<p>最后是注意力机制在RE的应用。</p>
<a id="more"></a>
<h4 id="Attention-based-Neural-Networks-for-Chemical-Protein-Relation-Extraction-2017"><a href="#Attention-based-Neural-Networks-for-Chemical-Protein-Relation-Extraction-2017" class="headerlink" title="Attention-based Neural Networks for Chemical Protein Relation Extraction(2017)"></a>Attention-based Neural Networks for Chemical Protein Relation Extraction(2017)</h4><h4 id="Learning-local-and-global-contexts-using-a-convolutional-recurrent-network-model-for-relation-classification-in-biomedical-text-2017"><a href="#Learning-local-and-global-contexts-using-a-convolutional-recurrent-network-model-for-relation-classification-in-biomedical-text-2017" class="headerlink" title="Learning local and global contexts using a convolutional recurrent network model for relation classification in biomedical text(2017)"></a>Learning local and global contexts using a convolutional recurrent network model for relation classification in biomedical text(2017)</h4><h4 id="Attending-to-All-Mention-Pairs-for-Full-Abstract-Biological-Relation-Extraction-2017"><a href="#Attending-to-All-Mention-Pairs-for-Full-Abstract-Biological-Relation-Extraction-2017" class="headerlink" title="Attending to All Mention Pairs for Full Abstract Biological Relation Extraction(2017)"></a>Attending to All Mention Pairs for Full Abstract Biological Relation Extraction(2017)</h4><h4 id="Simultaneously-Self-Attending-to-All-Mentions-for-Full-Abstract-Biological-Relation-Extraction-2018"><a href="#Simultaneously-Self-Attending-to-All-Mentions-for-Full-Abstract-Biological-Relation-Extraction-2018" class="headerlink" title="Simultaneously Self-Attending to All Mentions for Full-Abstract Biological Relation Extraction(2018)"></a>Simultaneously Self-Attending to All Mentions for Full-Abstract Biological Relation Extraction(2018)</h4><h4 id="Extracting-chemical-protein-relations-usingattention-based-neural-networks-2018"><a href="#Extracting-chemical-protein-relations-usingattention-based-neural-networks-2018" class="headerlink" title="Extracting chemical-protein relations usingattention-based neural networks(2018)"></a>Extracting chemical-protein relations usingattention-based neural networks(2018)</h4><h4 id="Combining-Context-and-Knowledge-Representations-for-Chemical-disease-Relation-Extraction-2018"><a href="#Combining-Context-and-Knowledge-Representations-for-Chemical-disease-Relation-Extraction-2018" class="headerlink" title="Combining Context and Knowledge Representations for Chemical-disease Relation Extraction(2018)"></a>Combining Context and Knowledge Representations for Chemical-disease Relation Extraction(2018)</h4><h4 id="A-document-level-neural-model-integrated-domain-knowledge-for-chemical-induced-disease-relations-2018"><a href="#A-document-level-neural-model-integrated-domain-knowledge-for-chemical-induced-disease-relations-2018" class="headerlink" title="A document level neural model integrated domain knowledge for chemical-induced disease relations(2018)"></a>A document level neural model integrated domain knowledge for chemical-induced disease relations(2018)</h4>
<h4 id="Chemical-induced-disease-relation-extraction-with-dependency-information-T-and-prior-knowledge-2018"><a href="#Chemical-induced-disease-relation-extraction-with-dependency-information-T-and-prior-knowledge-2018" class="headerlink" title="Chemical-induced disease relation extraction with dependency information T and prior knowledge(2018)"></a>Chemical-induced disease relation extraction with dependency information T and prior knowledge(2018)</h4><h4 id="Distantly-Supervised-Biomedical-Knowledge-Acquisition-via-Knowledge-Graph-Based-Attention-2019"><a href="#Distantly-Supervised-Biomedical-Knowledge-Acquisition-via-Knowledge-Graph-Based-Attention-2019" class="headerlink" title="Distantly Supervised Biomedical Knowledge Acquisition via Knowledge Graph Based Attention(2019)"></a>Distantly Supervised Biomedical Knowledge Acquisition via Knowledge Graph Based Attention(2019)</h4><h4 id="Relation-Extraction-using-Explicit-Context-Conditioning-2019"><a href="#Relation-Extraction-using-Explicit-Context-Conditioning-2019" class="headerlink" title="Relation Extraction using Explicit Context Conditioning(2019)"></a>Relation Extraction using Explicit Context Conditioning(2019)</h4><h4 id="Transfer-Learning-for-Scientific-Data-Chain-Extraction-in-Small-Chemical-Corpus-with-BERT-CRF-Model-2019"><a href="#Transfer-Learning-for-Scientific-Data-Chain-Extraction-in-Small-Chemical-Corpus-with-BERT-CRF-Model-2019" class="headerlink" title="Transfer Learning for Scientific Data Chain Extraction in Small Chemical Corpus with BERT-CRF Model(2019)"></a>Transfer Learning for Scientific Data Chain Extraction in Small Chemical Corpus with BERT-CRF Model(2019)</h4><h4 id><a href="#" class="headerlink" title=" "></a> </h4>]]></content>
  </entry>
  <entry>
    <title>文献阅读0725</title>
    <url>/2019/07/25/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB0725/</url>
    <content><![CDATA[<p>这篇是NER和RE的联合抽取模型。</p>
<a id="more"></a>
<h4 id="A-hybrid-deep-learning-approach-for-medical-relation-extraction-2018"><a href="#A-hybrid-deep-learning-approach-for-medical-relation-extraction-2018" class="headerlink" title="A hybrid deep learning approach for medical relation extraction(2018)"></a>A hybrid deep learning approach for medical relation extraction(2018)</h4><h4 id="A-hybrid-model-based-on-neural-networks-for-biomedical-relation-extraction-2018"><a href="#A-hybrid-model-based-on-neural-networks-for-biomedical-relation-extraction-2018" class="headerlink" title="A hybrid model based on neural networks for biomedical relation extraction(2018)"></a>A hybrid model based on neural networks for biomedical relation extraction(2018)</h4><h4 id="Automatic-extraction-of-gene-disease-associations-from-literature-using-joint-ensemble-learning-2018"><a href="#Automatic-extraction-of-gene-disease-associations-from-literature-using-joint-ensemble-learning-2018" class="headerlink" title="Automatic extraction of gene-disease associations from literature using joint ensemble learning(2018)"></a>Automatic extraction of gene-disease associations from literature using joint ensemble learning(2018)</h4><h4 id="Joint-entity-recognition-and-relation-extraction-as-a-multi-head-selection-problem-2018"><a href="#Joint-entity-recognition-and-relation-extraction-as-a-multi-head-selection-problem-2018" class="headerlink" title="Joint entity recognition and relation extraction as a multi-head selection problem(2018)"></a>Joint entity recognition and relation extraction as a multi-head selection problem(2018)</h4>]]></content>
  </entry>
  <entry>
    <title>文献阅读0724</title>
    <url>/2019/07/24/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB0724/</url>
    <content><![CDATA[<p>这篇是递归神经网络和树/图LSTM。</p>
<a id="more"></a>
<h4 id="Chemical-gene-relation-extraction-using-recursive-neural-network-2018"><a href="#Chemical-gene-relation-extraction-using-recursive-neural-network-2018" class="headerlink" title="Chemical-gene relation extraction using recursive neural network(2018)"></a>Chemical-gene relation extraction using recursive neural network(2018)</h4><h5 id="一句话总结"><a href="#一句话总结" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>提出了一种treeLSTM和叫Stack-augmented Parser Interpreter Neural Network (SPINN)的神经网络</p>
<h4 id="Drug-drug-interaction-extraction-from-the-literature-using-a-recursive-neural-network-2018"><a href="#Drug-drug-interaction-extraction-from-the-literature-using-a-recursive-neural-network-2018" class="headerlink" title="Drug drug interaction extraction from the literature using a recursive neural network(2018)"></a>Drug drug interaction extraction from the literature using a recursive neural network(2018)</h4><h5 id="一句话总结-1"><a href="#一句话总结-1" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>采用treeLSTM的变体递归神经网络识别DDI四类关系（advice, effect, mechanism, and int），其中除了句子依存解析外，还加入了位置嵌入信息、子树包含特征和集成方法提高精度</p>
<h5 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h5><p>模型如图，其中子树包含特征是指：如果一种目标药物存在于当前节点的叶子节点时特征的置1，否则置0</p>
<img src="/2019/07/24/文献阅读0724/1.png">
<h4 id="Mapping-Text-to-Knowledge-Graph-Entities-using-Multi-Sense-LSTMs-2018"><a href="#Mapping-Text-to-Knowledge-Graph-Entities-using-Multi-Sense-LSTMs-2018" class="headerlink" title="Mapping Text to Knowledge Graph Entities using Multi-Sense LSTMs(2018)"></a>Mapping Text to Knowledge Graph Entities using Multi-Sense LSTMs(2018)</h4><h4 id="Syntax-based-Transfer-Learning-for-the-Task-of-Biomedical-Relation-Extraction-2018"><a href="#Syntax-based-Transfer-Learning-for-the-Task-of-Biomedical-Relation-Extraction-2018" class="headerlink" title="Syntax-based Transfer Learning for the Task of Biomedical Relation Extraction(2018)"></a>Syntax-based Transfer Learning for the Task of Biomedical Relation Extraction(2018)</h4><h4 id="Biomedical-Event-Extraction-based-on-Knowledge-driven-Tree-LSTM-2019"><a href="#Biomedical-Event-Extraction-based-on-Knowledge-driven-Tree-LSTM-2019" class="headerlink" title="Biomedical Event Extraction based on Knowledge-driven Tree-LSTM(2019)"></a>Biomedical Event Extraction based on Knowledge-driven Tree-LSTM(2019)</h4><h4 id="BERE-AN-ACCURATE-DISTANTLY-SUPERVISED-BIOMEDICAL-ENTITY-RELATION-EXTRACTION-NETWORK-2019"><a href="#BERE-AN-ACCURATE-DISTANTLY-SUPERVISED-BIOMEDICAL-ENTITY-RELATION-EXTRACTION-NETWORK-2019" class="headerlink" title="BERE: AN ACCURATE DISTANTLY SUPERVISED BIOMEDICAL ENTITY RELATION EXTRACTION NETWORK(2019)"></a>BERE: AN ACCURATE DISTANTLY SUPERVISED BIOMEDICAL ENTITY RELATION EXTRACTION NETWORK(2019)</h4><h5 id="一句话总结-2"><a href="#一句话总结-2" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>采用了Gumbel Tree-GRU模型学习句子结构并整合实体嵌入信息，模型也加入了单词和句子级别的注意力机制提高关系抽取精度，在DDI’13数据集上取得了最好效果。</p>
<img src="/2019/07/24/文献阅读0724/2.png">
<h4 id="Cross-Sentence-N-ary-Relation-Extraction-with-Graph-LSTMs-2017"><a href="#Cross-Sentence-N-ary-Relation-Extraction-with-Graph-LSTMs-2017" class="headerlink" title="Cross-Sentence N -ary Relation Extraction with Graph LSTMs(2017)"></a>Cross-Sentence N -ary Relation Extraction with Graph LSTMs(2017)</h4><h4 id="N-ary-Relation-Extraction-using-Graph-State-LSTM-2018"><a href="#N-ary-Relation-Extraction-using-Graph-State-LSTM-2018" class="headerlink" title="N -ary Relation Extraction using Graph State LSTM(2018)"></a>N -ary Relation Extraction using Graph State LSTM(2018)</h4>]]></content>
  </entry>
  <entry>
    <title>文献阅读0723</title>
    <url>/2019/07/23/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB0723/</url>
    <content><![CDATA[<p>这篇是双向LSTM的。(慢慢补充)</p>
<a id="more"></a>
<h4 id="Leveraging-Biomedical-Resources-in-Bi-LSTM-for-Drug-Drug-Interaction-Extraction-2018"><a href="#Leveraging-Biomedical-Resources-in-Bi-LSTM-for-Drug-Drug-Interaction-Extraction-2018" class="headerlink" title="Leveraging Biomedical Resources in Bi-LSTM for Drug-Drug Interaction Extraction(2018)"></a>Leveraging Biomedical Resources in Bi-LSTM for Drug-Drug Interaction Extraction(2018)</h4><h4 id="Feature-Assisted-bi-directional-LSTM-Model-for-Protein-Protein-Interaction-Identification-from-Biomedical-Texts-2018"><a href="#Feature-Assisted-bi-directional-LSTM-Model-for-Protein-Protein-Interaction-Identification-from-Biomedical-Texts-2018" class="headerlink" title="Feature Assisted bi-directional LSTM Model for Protein-Protein Interaction Identification from Biomedical Texts(2018)"></a>Feature Assisted bi-directional LSTM Model for Protein-Protein Interaction Identification from Biomedical Texts(2018)</h4><h5 id="一句话总结"><a href="#一句话总结" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>加了最短依存路径SDP的Bi-LSTM提取PPI</p>
<img src="/2019/07/23/文献阅读0723/1.png">
<h4 id="Improving-the-learning-of-chemical-protein-interactions-from-literature-using-transfer-learning-and-specialized-word-embeddings-2018"><a href="#Improving-the-learning-of-chemical-protein-interactions-from-literature-using-transfer-learning-and-specialized-word-embeddings-2018" class="headerlink" title="Improving the learning of chemical-protein interactions from literature using transfer learning and specialized word embeddings(2018)"></a>Improving the learning of chemical-protein interactions from literature using transfer learning and specialized word embeddings(2018)</h4><h4 id="CONTEXT-AWARENESS-AND-EMBEDDING-FOR-BIOMEDICAL-EVENT-EXTRACTION-2019"><a href="#CONTEXT-AWARENESS-AND-EMBEDDING-FOR-BIOMEDICAL-EVENT-EXTRACTION-2019" class="headerlink" title="CONTEXT AWARENESS AND EMBEDDING FOR BIOMEDICAL EVENT EXTRACTION(2019)"></a>CONTEXT AWARENESS AND EMBEDDING FOR BIOMEDICAL EVENT EXTRACTION(2019)</h4><h4 id="Combining-relation-extraction-with-function-detection-for-BEL-statement-extraction-2019"><a href="#Combining-relation-extraction-with-function-detection-for-BEL-statement-extraction-2019" class="headerlink" title="Combining relation extraction with function detection for BEL statement extraction(2019)"></a>Combining relation extraction with function detection for BEL statement extraction(2019)</h4><h4 id="Exploring-semi-supervised-variational-autoencoders-for-biomedical-relation-extraction-2019"><a href="#Exploring-semi-supervised-variational-autoencoders-for-biomedical-relation-extraction-2019" class="headerlink" title="Exploring semi-supervised variational autoencoders for biomedical relation extraction(2019)"></a>Exploring semi-supervised variational autoencoders for biomedical relation extraction(2019)</h4><h4 id="From-POS-tagging-to-dependency-parsing-for-biomedical-event-extraction-2019"><a href="#From-POS-tagging-to-dependency-parsing-for-biomedical-event-extraction-2019" class="headerlink" title="From POS tagging to dependency parsing for biomedical event extraction(2019)"></a>From POS tagging to dependency parsing for biomedical event extraction(2019)</h4>]]></content>
  </entry>
  <entry>
    <title>文献阅读0722</title>
    <url>/2019/07/22/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB0722/</url>
    <content><![CDATA[<p>在snorkel上卡了壳，不会写LF。。。回过头读一些CNN/LSTM/Attention+SDP+远程监督的文章补充基础知识吧，这也是通用领域做关系抽取的思路。</p>
<p>就按主体模型分几天记吧，这篇是CNN(之前写的BioRE文献阅读报告里面详述了模型演化过程，这里只是选了两篇运用到生物医学领域的)</p>
<a id="more"></a>
<h4 id="Convolutional-neural-networks-for-chemical-disease-relation-extraction-are-improved-with-character-based-word-embeddings-2018"><a href="#Convolutional-neural-networks-for-chemical-disease-relation-extraction-are-improved-with-character-based-word-embeddings-2018" class="headerlink" title="Convolutional neural networks for chemical-disease relation extraction are improved with character-based word embeddings(2018)"></a>Convolutional neural networks for chemical-disease relation extraction are improved with character-based word embeddings(2018)</h4><h5 id="一句话总结"><a href="#一句话总结" class="headerlink" title="一句话总结"></a>一句话总结</h5><p> CNN+CNNchar 和 CNN+LSTMchar 的模型对比，证实了字符嵌入的有效性。其实就是在15年提出的PCNN基础上加了一层char embedding，这在NER里面已经很常见了。</p>
<img src="/2019/07/22/文献阅读0722/1.png">
<h5 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h5><p>在BC5CDR上取得了SOTA的结果。</p>
<img src="/2019/07/22/文献阅读0722/2.png">
<h4 id="Biomedical-Event-Extraction-Using-Convolutional-Neural-Networks-and-Dependency-Parsing-2018"><a href="#Biomedical-Event-Extraction-Using-Convolutional-Neural-Networks-and-Dependency-Parsing-2018" class="headerlink" title="Biomedical Event Extraction Using Convolutional Neural Networks and Dependency Parsing(2018)"></a>Biomedical Event Extraction Using Convolutional Neural Networks and Dependency Parsing(2018)</h4><h5 id="一句话总结-1"><a href="#一句话总结-1" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>在他们先前开发的用SVM做NRE的TEES系统上加入了一个抽取生物医学关系和事件的CNN子模块，利用了多种输入和依存解析嵌入。</p>
<h5 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h5><ol>
<li><p>数据集和模型：</p>
<p>BioNLP，DDI and BioCreative corpora</p>
</li>
<li><p>抽取流程：</p>
<p>TEES基于事件或关系的图表示，其中命名实体和触发词是节点，关系是边。抽取过程包括：entity detection（识别候选实体或触发词）、edge detection（关系检测）、unmerging stage（判定是否为真实事件）、modifier detection(检测事件方式：否定或是对立)</p>
</li>
<li><p>模型：</p>
<p>输入的嵌入包括词嵌入、POS嵌入、距离嵌入、相对位置嵌入（当前token在识别关系词的前、后或中，是否是实体一部分）、实体特征嵌入（当前token是否是候选实体、触发词等，在关系检测中使用）、最短路径嵌入(指示token是否在两实体的最短依存路径中及扮演的关系，在关系抽取中使用)</p>
<img src="/2019/07/22/文献阅读0722/3.png">
</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>文献阅读0719</title>
    <url>/2019/07/19/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB0719/</url>
    <content><![CDATA[<p>找到两篇直接面向文献中关系抽取的文献，对做工程很有启发性，不单单只是测试新模型提高数据集精度。</p>
<a id="more"></a>

<h4 id="An-Insight-Extraction-System-on-BioMedical-Literature-with-Deep-Neural-Networks-2017"><a href="#An-Insight-Extraction-System-on-BioMedical-Literature-with-Deep-Neural-Networks-2017" class="headerlink" title="An Insight Extraction System on BioMedical Literature with Deep Neural Networks(2017)"></a>An Insight Extraction System on BioMedical Literature with Deep Neural Networks(2017)</h4><h5 id="一句话总结"><a href="#一句话总结" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>按照知识表示的思路，提出了一个相似度测度方法来评价实体间的相关性然后排序预测Cause-Effect关系</p>
<img src="/2019/07/19/文献阅读0719/1.png">
<h5 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h5><ol>
<li>首先用生物医学词典（关键词匹配）和浅层语义解析器（实体边界预测）做NER</li>
<li>然后用上下文相似度模型和语义相似度模型来对实体A、B、上下文和关系R建模并排名。思路是先用BiLSTM和attention机制获得带有上下文信息表示的实体A和B，然后输入到relational similarity model中计算A B还有关系R的相似性测度，训练使得R与A B在低维空间中有相近的表示。</li>
</ol>
<h4 id="Deep-learning-of-mutation-gene-drug-relations-from-the-literature-2018"><a href="#Deep-learning-of-mutation-gene-drug-relations-from-the-literature-2018" class="headerlink" title="Deep learning of mutation-gene-drug relations from the literature(2018)"></a>Deep learning of mutation-gene-drug relations from the literature(2018)</h4>]]></content>
  </entry>
  <entry>
    <title>文献阅读0717</title>
    <url>/2019/07/17/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB0717/</url>
    <content><![CDATA[<p>新文章固然好，但模型复杂且不稳定，难以用于大规模PubMed关系的抽取。deepdive及后续的snorkel是个好工具，看到好多大厂也都在用，老师一直强调我们是做工程的，尽量要快要能用，所以还是回到snorkel吧。</p>
<a id="more"></a>
<h4 id="Semi-supervised-Ensemble-Learning-with-Weak-Supervision-for-Biomedical-Relation-Extraction-2019"><a href="#Semi-supervised-Ensemble-Learning-with-Weak-Supervision-for-Biomedical-Relation-Extraction-2019" class="headerlink" title="Semi-supervised Ensemble Learning with Weak Supervision for Biomedical Relation Extraction(2019)"></a>Semi-supervised Ensemble Learning with Weak Supervision for Biomedical Relation Extraction(2019)</h4><h5 id="一句话总结"><a href="#一句话总结" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>用半监督集成学习模型来产生弱标签然后弱监督学习做关系抽取。<a href="https://github.com/littlewine/snorkel-ml/" target="_blank" rel="noopener">项目地址</a></p>
<h5 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h5><ol>
<li><p>目标是抽取生物医学摘要中的regulation(CPR)和induce(CID)语义关系对</p>
<img src="/2019/07/17/文献阅读0717/2.png">
</li>
<li><p>为避免复杂的feature engineering使用ANN，但容易过拟合，所以用集成学习提高泛化能力</p>
</li>
<li><p>半监督集成学习：将半监督和集成学习结合在一起是个新颖的思路，集成学习可以加强半监督学习的表现，无监督标签可以增加learners的多样性，反过来降噪，这是种co-training的方式</p>
</li>
<li><p>思路是(1)用小型带标签数据集训练base learners，来预测更多的unlabeled data为weak labels；(2)加denoiser，然后在此基础上用弱监督学习训练更强大的meta-learner(noise-aware discriminative model)</p>
</li>
</ol>
<h5 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h5><ol>
<li>数据收集：假设已有大小为m的金标准数据集DB（拆分成train dev test）但是远不够训练模型得到很好的效果，我们获取一个大小为M(&gt;&gt;m)的无标签数据集DU</li>
<li>建立K个基础学习器：通过改变features、文本特征表示和机器学习模型等来构建多个base learner，尽量不要使用相似的分类器(这是通过K*K的相似度矩阵来聚类计算的)</li>
<li>预测DU，得到K*M的预测标签（这里用到snorkel来推断候选实体对关系），然后用denoiser将投票矩阵变为M个标签</li>
<li>训练一个双向LSTM的meta-learner</li>
</ol>
<h5 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h5><p>展示了用机器学习模型集成学习产生弱标签对关系抽取的影响，还有参数的选择，降噪方法的选择，meta-learner的表现，如图：</p>
<img src="/2019/07/17/文献阅读0717/2.png" title="base-learner的精度，集成学习产生弱标签的精度，以及meta-learner的精度">
<p>结果展示这种弱监督方式几乎可以达到全部有监督的精度，所以可以用本文提出的模型做数据扩增Unlabeled dataset expansion，而且迁移领域只需要标注少量的金标准数据集即可。鉴于F1 score通常不具有说服力，本文也提出了对marginal weak labels更好的评估方法。</p>
<p>(文章的思路很巧妙，效果也不错。看到开源的项目是用jupyter notebook写的，准备用几天时间好好学习一下)</p>
<h4 id="A-global-network-of-biomedical-relationships-derived-from-text-2018"><a href="#A-global-network-of-biomedical-relationships-derived-from-text-2018" class="headerlink" title="A global network of biomedical relationships derived from text(2018)"></a>A global network of biomedical relationships derived from text(2018)</h4><p>Zenodo: Proposed a labeled, weighted network of structured biomedical relationships for all Medline abstracts using NCBI’s PubTator, the Stanford dependency parser and ensemble biclustering algorithm (EBC) .</p>
<p>抽取到的PubMed级别的chemical-disease-gene三者间的关系：<a href="https://zenodo.org/record/3346007" target="_blank" rel="noopener">https://zenodo.org/record/3346007</a></p>
<h4 id="Reusing-label-functions-to-extract-multiple-types-of-relationships-from-biomedical-abstracts-at-scale-2019"><a href="#Reusing-label-functions-to-extract-multiple-types-of-relationships-from-biomedical-abstracts-at-scale-2019" class="headerlink" title="Reusing label functions to extract multiple types of relationships from biomedical abstracts at scale(2019)"></a>Reusing label functions to extract multiple types of relationships from biomedical abstracts at scale(2019)</h4><h5 id="一句话总结-1"><a href="#一句话总结-1" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>通过制定snorkel LF大规模抽取PubMed生物医学关系。<a href="https://github.com/greenelab/snorkeling" target="_blank" rel="noopener">项目地址</a></p>
<p>准备借鉴这个做之后的项目。</p>
]]></content>
  </entry>
  <entry>
    <title>文献阅读0716</title>
    <url>/2019/07/16/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB0716/</url>
    <content><![CDATA[<p>继续BioRE，看几篇最新的。</p>
<a id="more"></a>
<h4 id="BO-LSTM-classifying-relations-via-long-short-term-memory-networks-along-biomedical-ontologies-2019"><a href="#BO-LSTM-classifying-relations-via-long-short-term-memory-networks-along-biomedical-ontologies-2019" class="headerlink" title="BO-LSTM: classifying relations via long short-term memory networks along biomedical ontologies(2019)"></a>BO-LSTM: classifying relations via long short-term memory networks along biomedical ontologies(2019)</h4><h5 id="一句话总结"><a href="#一句话总结" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>利用领域本体匹配和溯源来提高DDI关系分类精度。<a href="https://github.com/lasigeBioTM/BOLSTM" target="_blank" rel="noopener">项目地址</a></p>
<h5 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h5><img src="/2019/07/16/文献阅读0716/1.png" title="BO-LSTM Model architecture">
<p>模型如图，假设已经标注好实体，任务是识别候选实体对的关系，我们先用spaCy得到最短依存路径SDP，对SDP中的每个词得到它们的词嵌入；wordnet上义词（也就是描述的类别）；还有与本体中的概念匹配（用距离相似度）和找概念原型，按照从通用概念—&gt;单词自身的实体链来表示；找到候选实体的概念链中公共的概念。将这些都作为嵌入的组件通过LSTM网络，来测试外部本体对 SemEval 2013 Task 9 DDI识别精度的影响。</p>
<h5 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h5><p>如图分别在关系检测和关系分类上的结果，关系检测忽略关系类型，关系分类则更难些要求识别出positive和它们的类型(advise, effect, mechanism, int)</p>
<img src="/2019/07/16/文献阅读0716/3.png">
<img src="/2019/07/16/文献阅读0716/2.png">
<h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>这个框架思路(就是用领域本体匹配实体作为embedding来学习能提高识别精度)具有可扩展性，比如提取基因-表型关系时，可以利用Gene Ontology，只要有标注好的数据集即可拿来训练然后预测。</p>
<p>初期准备用这个项目实战，但是GitHub有些问题，正在提交issues和作者交流中…</p>
<h4 id="REflex-Flexible-Framework-for-Relation-Extraction-in-Multiple-Domains-2019"><a href="#REflex-Flexible-Framework-for-Relation-Extraction-in-Multiple-Domains-2019" class="headerlink" title="REflex: Flexible Framework for Relation Extraction in Multiple Domains(2019)"></a>REflex: Flexible Framework for Relation Extraction in Multiple Domains(2019)</h4><h5 id="一句话总结-1"><a href="#一句话总结-1" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>针对生物医学领域关系抽取文献多而杂且都不开源不具有普遍性的特点，提出了一个关系抽取的通用框架！！研究了预处理、模型训练、参数调节、结果评估这些步骤的影响，主要在三个数据集上做了测试：Semeval 2010，DDI Extraction，i2b2/VA 2010 relations。这篇文章条理清晰，代码开源，泛化性强，是难得的好文章。<a href="https://github.com/geetickachauhan/relation-extraction" target="_blank" rel="noopener">项目地址</a></p>
<h5 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h5><img src="/2019/07/16/文献阅读0716/5.png" title="workflow">
<ol>
<li>预处理：分句分词，大写转小写，去除停用词，数字统一化，NER blinding(就是把句子中的实体按照关系识别的要求规范化，比如for semeval were <em>ENTITY</em>, for ddi were <em>DRUG</em> and for i2b2 were <em>PROBLEM</em>, <em>TREATMENT</em> and <em>TEST</em>)，用spacy和ScispaCy标记这些特定类型</li>
<li>建模：选用的基本模型是带位置嵌入和ranking loss的卷积神经网络CRCNN，测试了piecewise max-pooling，ELMo还有BERT等的影响。其中ELMo可以用token级别的embedding，BERT可选两种：token级别的和句子CLS级别的。</li>
<li>训练：两种调参方式，手动调参和随机搜索</li>
<li>评估：分类和检测两个任务</li>
</ol>
<h5 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h5><img src="/2019/07/16/文献阅读0716/4.png">
<ol>
<li>好的预处理比模型更重要</li>
<li>由于split bias单独报告一个测试集的精度会有问题，最好选用具有显著性的交叉验证</li>
<li>Contextualized embeddings很有用，所以featurizing embedding(ELMo，BERT等)很重要，在通过卷积层前把它们和词嵌入连接起来很有效</li>
<li>调参很关键，手动和随机搜索差不多但随机搜索需要设置经验范围</li>
<li>为新数据集依照正负样本的imbalance选择相应的评估准则</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>文献阅读0715</title>
    <url>/2019/07/15/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB0715/</url>
    <content><![CDATA[<p>本着思路优先、生物医学领域优先和开源代码优先的原则，这周准备写一些关系抽取的文献。因为关系抽取的文献多又杂，方法五花八门，不像NER模型简单应用起来也方便，所以看起来比较吃力。废话不多说了正文如下：</p>
<a id="more"></a>
<h4 id="Relation-Extraction-from-Biomedical-Literature-with-Minimal-Supervision-and-Grouping-Strategy-2014"><a href="#Relation-Extraction-from-Biomedical-Literature-with-Minimal-Supervision-and-Grouping-Strategy-2014" class="headerlink" title="Relation Extraction from Biomedical Literature with Minimal Supervision and Grouping Strategy(2014)"></a>Relation Extraction from Biomedical Literature with Minimal Supervision and Grouping Strategy(2014)</h4><h5 id="一句话总结"><a href="#一句话总结" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>文章提出了一个新颖的远程监督模型，不需要手动标注数据，运用了外部知识库UMLS和分组策略，基于统计模型而非启发式规则，抽取文献中的基因和脑区间的基因表达关系。</p>
<h5 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h5><ol>
<li><p>实体标注：标注基因（BioTagger）和脑区实体(NIF和脑区词典)</p>
</li>
<li><p>分组策略：远程监督通常假设抽取到的实体对是独立的，但是在生物医学领域很容易出现平行的实体对，所以文章采用Stanford parser解析句子获取出现在相似位置的名词和名词短语（通常是and相连的），并把连续的实体组成一组然后表示它们与其他实体的关系。</p>
<img src="/2019/07/15/文献阅读0715/1.png" title="以and相连的名词和名词短语">
</li>
<li><p>特征提取：词法特征（两实体的词袋，和实体间单词的词袋特征）和句法特征（最短依存路径）。</p>
</li>
<li><p>远程监督关系抽取(模型训练)：远程监督假设文本中至少出现一个实体对表现出知识库中相应实体对的关系，这里利用了生物医学领域的知识库UMLS，然后将关系对用无向图建模。</p>
<img src="/2019/07/15/文献阅读0715/2.png">
<p>建立条件概率公式如下，z(i)表示从每个句子x(i)抽取得到的关系，然后用y表示建立关系到知识库的映射：</p>
<img src="/2019/07/15/文献阅读0715/3.png">
<p>我理解的公式的意思是：(3)是只要有句子表现出知识库中的r(e1,e2)关系就Φcorpus设定为1，然后(2)是一个对数线性模型来衡量实体对的关系类型（是基因表达关系还是其他关系），(1)然后把基因表达关系映射到知识库中的基因表达关系上，训练公式(2)中的θ参数使结果概率最大化。</p>
<p>(The model first predicts the relation types for the entity pair corresponding to each of its evidences.<br>Then, the predictions of relation types at sentential-level are aggregated to approximate the relation types for the entity pair at corpus-level. As long as one of the sentences is classified into the <em>geneExpression</em> category, the <em>geneExpression</em> relation is returned at corpus-level; otherwise, only <em>otherRelation</em> is returned.)</p>
</li>
</ol>
<h5 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h5><p>找了10000篇全文，抽取大约得到30,000个有基因-脑区关系的句子，用分组得到包含大约7700个实体对，然后金标准标选了259个句子(114个基因表达，143个其他关系)用作模型测试，最后得到的精度大致和有监督的SVM相当。</p>
<h4 id="Large-scale-extraction-of-gene-interactions-from-full-text-literature-using-DeepDive-2016"><a href="#Large-scale-extraction-of-gene-interactions-from-full-text-literature-using-DeepDive-2016" class="headerlink" title="Large-scale extraction of gene interactions from full-text literature using DeepDive(2016)"></a>Large-scale extraction of gene interactions from full-text literature using DeepDive(2016)</h4><h5 id="一句话总结-1"><a href="#一句话总结-1" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>用deepdive远程监督抽取100000篇plos全文的基因间的关系(包括PPI和转录因子间的关系)。<a href="https://github.com/edoughty/deepdive_genegene_app" target="_blank" rel="noopener">项目地址</a></p>
<h5 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h5><img src="/2019/07/15/文献阅读0715/4.png" title="预处理、构建候选实体对、deepdive推理计算概率、专家微调">
<ol>
<li>文献预处理：使用Stanford CoreNLP分句、分词、POS、NER、DP</li>
<li>提取候选实体对：基于共现关系提取出现在HGNC的候选基因对，并提取特征(词窗特征、依存特征等)</li>
<li>远程监督：运用外部数据库为候选基因对设labels为True，False or Unknown。为防止过拟合，设定一些高频的已知为True的基因对为Unknown，再随机设置一些unknown为负例</li>
<li>用deepdive推理：将带is_correct label的候选基因对输入，50％的标签不动作为权重特征学习和校准，其他的用factor graph推测概率(deepdive的原理待补充)</li>
<li>系统微调：每次迭代做错误分析和预测，用滚雪球策略增加正例(如果候选对被预测为真，加入到training中)</li>
<li>两种评估：金标准PPI数据集测F值，以及随机挑选的正负基因对curation</li>
</ol>
<h4 id="Distant-Supervision-for-Large-Scale-Extraction-of-Gene–Disease-Associations-from-Literature-Using-DeepDive-2018"><a href="#Distant-Supervision-for-Large-Scale-Extraction-of-Gene–Disease-Associations-from-Literature-Using-DeepDive-2018" class="headerlink" title="Distant Supervision for Large-Scale Extraction of Gene–Disease Associations from Literature Using DeepDive(2018)"></a>Distant Supervision for Large-Scale Extraction of Gene–Disease Associations from Literature Using DeepDive(2018)</h4><p>用deepdive抽取了879585篇PubMed摘要中的75595个独特基因-疾病关系，没什么新意，思路和上一篇文章基本一样，放下结果吧。</p>
<img src="/2019/07/15/文献阅读0715/5.png">
]]></content>
  </entry>
  <entry>
    <title>文献阅读0712</title>
    <url>/2019/07/12/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB0712/</url>
    <content><![CDATA[<p>阅读文献要紧跟前沿，不能闭门造车。生物医学nlp领域也随着ELMo, bert等Contextualized embedding的出现推陈出新，涌现了最前沿的几篇词向量、语言模型和预训练方法等的文章，这里按时间顺序做个记录慢慢补充。</p>
<a id="more"></a>
<h4 id="BioBERT-a-pre-trained-biomedical-language-representation-model-for-biomedical-text-mining-2019"><a href="#BioBERT-a-pre-trained-biomedical-language-representation-model-for-biomedical-text-mining-2019" class="headerlink" title="BioBERT: a pre-trained biomedical language representation model for biomedical text mining(2019)"></a>BioBERT: a pre-trained biomedical language representation model for biomedical text mining(2019)</h4><h5 id="一句话总结"><a href="#一句话总结" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>BioBERT全称生物医学双向Transformer编码器，也算是一种用迁移学习（双向语言模型）来解决数据缺失的方法。</p>
<h5 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h5><img src="/2019/07/12/文献阅读0712/1.png" title="训练和微调">
<p>用PubMed和PMC预训练BERT的方法就不说了，在微调的时候，常采用WordPiece tokenization的方式，将新词用频繁的子词表示然后通过BioBERT。对NER，直接在biobert的输出加一层网络计算BIO/BIOES标签概率，也不需要CRF；对NRE，可以视为句子分类任务，也是在使用[CLS]表示的bert输出后加一层网络，其中对实体的处理eg: @GENE$</p>
<h5 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h5><ol>
<li>ner：BERT &lt; BioBERT (+PubMed) &lt; BioBERT (+ PMC) &lt; SOTA models &lt; BioBERT (+PubMed + PMC).</li>
<li>nre：通常是SOTA models &lt;BERT &lt; BioBERT. 一般来说数据集越小提升效果越明显</li>
</ol>
<p>这是它的后续文章，用BERT配合Pubtator做NER，标注了所有的PubMed数据集</p>
<h4 id="A-Neural-Named-Entity-Recognition-and-Multi-Type-Normalization-Tool-for-Biomedical-Text-Mining-2019"><a href="#A-Neural-Named-Entity-Recognition-and-Multi-Type-Normalization-Tool-for-Biomedical-Text-Mining-2019" class="headerlink" title="A Neural Named Entity Recognition and Multi-Type Normalization Tool for Biomedical Text Mining(2019)"></a>A Neural Named Entity Recognition and Multi-Type Normalization Tool for Biomedical Text Mining(2019)</h4><h5 id="一句话总结-1"><a href="#一句话总结-1" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>传统的ner工具很难应对新实体，也不考虑被标注成多个类型的重叠实体，bern基于biobert标注然后运用决策规则和实体规范化技术解决这些问题。</p>
<h5 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h5><img src="/2019/07/12/文献阅读0712/2.png">
<p>先用tmTool和tmVar2.0处理输入的PMID或原文识别mutation，然后用biobert识别其他几种实体，对重叠实体，对重叠实体按照概率决策，对多个类型的实体规范化处理并分配统一id，输出标注后的文档(json或pubtator)。</p>
<ol>
<li><p>决策机制：下图左图是重叠实体的比例，右图是决策方式</p>
<img src="/2019/07/12/文献阅读0712/3.png">
</li>
<li><p>实体规范化技术：</p>
<img src="/2019/07/12/文献阅读0712/4.png">
<p>比如可以用bern快速发现HGNC IDs for genes, 和MeSH IDs for diseases</p>
</li>
</ol>
<h5 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h5><p><a href="https://bern.korea.ac.kr" target="_blank" rel="noopener">网址</a> <a href="https://bern.korea.ac.kr/pubmed/&lt;one or more PMIDs&gt;[/&lt;pubtator or json&gt;]" target="_blank" rel="noopener">API</a></p>
<h4 id="SCIBERT-Pretrained-Contextualized-Embeddings-for-Scientific-Text-2019"><a href="#SCIBERT-Pretrained-Contextualized-Embeddings-for-Scientific-Text-2019" class="headerlink" title="SCIBERT: Pretrained Contextualized Embeddings for Scientific Text(2019)"></a>SCIBERT: Pretrained Contextualized Embeddings for Scientific Text(2019)</h4><h5 id="一句话总结-2"><a href="#一句话总结-2" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>类似于BioBERT，建立方法是用Semantic Scholar的1.14M原文在BERT预训练，然后在多个领域的多种类型的任务数据集上做了精度测试</p>
<h5 id="方法-2"><a href="#方法-2" class="headerlink" title="方法"></a>方法</h5><ol>
<li><p>基于领域特定的文献语料库建立了新的WordPiece词汇库：SCIVOCAB，和原来BERT的BASEVOCAB作对比</p>
</li>
<li><p>加下游任务的方式是：</p>
<p>We apply a multilayer BiLSTM to token embeddings. For <strong>text classification</strong>, we apply a multilayer perceptron on the first and last BiLSTM states. For <strong>sequence tagging</strong>, we use a CRF on top of the BiLSTM, as done in (Ma and Hovy, 2016). For <strong>dependency parsing</strong> we use the biaffine attention model from Dozat and Manning (2017).  然后直接用预训练好的权重不加fine-tuning来预测精度</p>
</li>
</ol>
<h5 id="结果-2"><a href="#结果-2" class="headerlink" title="结果"></a>结果</h5><p>我们只关心生物医学类的</p>
<img src="/2019/07/12/文献阅读0712/5.png">
<h4 id="Improving-Chemical-Named-Entity-Recognition-in-Patents-with-Contextualized-Word-Embeddings-2019"><a href="#Improving-Chemical-Named-Entity-Recognition-in-Patents-with-Contextualized-Word-Embeddings-2019" class="headerlink" title="Improving Chemical Named Entity Recognition in Patents with Contextualized Word Embeddings(2019)"></a>Improving Chemical Named Entity Recognition in Patents with Contextualized Word Embeddings(2019)</h4><p>将ELMo(利用语言模型获得的一个上下文相关的预训练表示)直接当做特征拼接到具体任务模型的词向量输入（不同于GPT和BERT这种微调的方法），识别化学专利中的实体。识别模型如图：</p>
<img src="/2019/07/12/文献阅读0712/6.png">
<p>文章还证实了使用专门的化学专利词嵌入比PubMed-PMC-word2vec效果要好(废话)。</p>
]]></content>
  </entry>
  <entry>
    <title>文献阅读0710</title>
    <url>/2019/07/10/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB0710/</url>
    <content><![CDATA[<p>之前一直没写到过用规则和领域字典做NER，还有NER后的normalization，这次补充一下。</p>
<a id="more"></a>
<h4 id="Learning-Named-Entity-Tagger-using-Domain-Specific-Dictionary-2018"><a href="#Learning-Named-Entity-Tagger-using-Domain-Specific-Dictionary-2018" class="headerlink" title="Learning Named Entity Tagger using Domain-Specific Dictionary(2018)"></a>Learning Named Entity Tagger using Domain-Specific Dictionary(2018)</h4><h5 id="一句话总结"><a href="#一句话总结" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>没有直接用dictionary作为features而是采用类似于RE中远程监督的方式产生大量数据集然后训练。去噪的方式有两种：一是在将原来的LSTM+CRF替换成revised fuzzy CRF处理多个可能标签的情况（其中标签是用词典匹配的），二是提出一个具有Tie or Break模式的新模型AutoNER预测产生输出</p>
<h5 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h5><ol>
<li>Fuzzy-LSTM-CRF with Modified IOBES</li>
</ol>
<img src="/2019/07/10/文献阅读0710/1.png">
<ol>
<li>AutoNER with “Tie or Break”：区别于BIOES，这里只关心相邻两个tokens之间的关系是连在一起还是分开的</li>
</ol>
<img src="/2019/07/10/文献阅读0710/2.png">
<p>这样匹配的好处是可以更多的利用字符串匹配得到的信息：当一个entity被部分匹配的时候，其中部分正确的Tie信息得以被利用；当一个unigram entity被错误匹配的时候，Tie or Break部分并不会出错。而unigram entity正是字符串匹配中最容易匹配错误的部分。</p>
<h4 id="Recognizing-irregular-entities-in-biomedical-text-via-deep-neural-networks-2018"><a href="#Recognizing-irregular-entities-in-biomedical-text-via-deep-neural-networks-2018" class="headerlink" title="Recognizing irregular entities in biomedical text via deep neural networks(2018)"></a>Recognizing irregular entities in biomedical text via deep neural networks(2018)</h4><h5 id="一句话总结-1"><a href="#一句话总结-1" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>提出一个NerOne模型，识别文献中带有不规则实体部分的规则实体（见下图）。在原来双向LSTM+CRF的基础上，通过计算最短依存路径确定是否应该加入一个分类子模块（另一个双向LSTM+CRF）来对不规则实体分类。</p>
<img src="/2019/07/10/文献阅读0710/3.png">
<h4 id="A-Neural-Multi-Task-Learning-Framework-to-Jointly-Model-Medical-Named-Entity-Recognition-and-Normalization-2018"><a href="#A-Neural-Multi-Task-Learning-Framework-to-Jointly-Model-Medical-Named-Entity-Recognition-and-Normalization-2018" class="headerlink" title="A Neural Multi-Task Learning Framework to Jointly Model Medical Named Entity Recognition and Normalization(2018)"></a>A Neural Multi-Task Learning Framework to Jointly Model Medical Named Entity Recognition and Normalization(2018)</h4><h5 id="一句话总结-2"><a href="#一句话总结-2" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>提出一个新颖的多任务神经网络框架同时做NER和NEN，通过建立NER和NEN的反馈机制将逐级任务转为并行任务(也就是多任务共享权重)</p>
<h5 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h5><p>模型如图，base还是char CNN+BiLSTM+CRF，然后引入平行的NER和NEN任务作输出，然后互相反馈</p>
<img src="/2019/07/10/文献阅读0710/4.png">
<p>反馈机制计算式如下：</p>
<img src="/2019/07/10/文献阅读0710/5.png">
<h4 id><a href="#" class="headerlink" title=" "></a> </h4>]]></content>
  </entry>
  <entry>
    <title>文献阅读0709</title>
    <url>/2019/07/09/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB0709/</url>
    <content><![CDATA[<p>今天读了几篇词向量和预训练模型的文献，简单记一下。</p>
<a id="more"></a>
<h4 id="How-to-Train-Good-Word-Embeddings-for-Biomedical-NLP-2016"><a href="#How-to-Train-Good-Word-Embeddings-for-Biomedical-NLP-2016" class="headerlink" title="How to Train Good Word Embeddings for Biomedical NLP(2016)"></a>How to Train Good Word Embeddings for Biomedical NLP(2016)</h4><h5 id="一句话总结"><a href="#一句话总结" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>这篇文章研究了用word2vec训练生物医学领域词向量时，训练数据集、模型结构和高阶参数对词向量质量的影响。评估方法用到了内部评估（单词相似测度）和外部评估（两个NER数据集）。</p>
<h5 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h5><ol>
<li>数据集大小的影响：选用了三种：PubMed数据集，PMC数据集，PubMed+PMC数据集，发现更大的数据集不一定能训练出更好的词向量，效果最好的是PubMed</li>
<li>模型结构：对比了skip-gram（当前词预测上下文）和CBOW，发现skip-gram训练出来的要好一些</li>
<li>6个高阶参数：negative sampling（对当前词随机采样N个负例进行预测），Subsampling(去除频率高的词的共现)，min-count(最低出现的频数），学习率，词向量维度，上下文的词窗大小。</li>
<li>内部评估：UMNSRS单词相似度数据集，对出现的单词对，用模型计算余弦相似度然后再计算相关系数</li>
<li>外部评估：用训练好的词嵌入向量加简单的前馈神经网络做下游NER任务，不做fine-tuning</li>
</ol>
<h5 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h5><p>具体参数的调整对结果的影响就不放上来了，最终选择的最佳参数和对应的精度如图。选这篇文章主要是之前看好多NER文献都在词向量部分提到这篇调参的启发，不过想不明白为什么还是每次都用词向量wikipedia-pubmed-and-PMC-w2v而不是这篇测试效果最好的wikipedia-PubMed，可能fine-tuning后效果提升更明显吧</p>
<img src="/2019/07/09/文献阅读0709/1.png" title="最佳参数和相应的精度">
<h4 id="A-Comparison-of-Word-Embeddings-for-the-Biomedical-Natural-Language-Processing-2018"><a href="#A-Comparison-of-Word-Embeddings-for-the-Biomedical-Natural-Language-Processing-2018" class="headerlink" title="A Comparison of Word Embeddings for the Biomedical Natural Language Processing(2018)"></a>A Comparison of Word Embeddings for the Biomedical Natural Language Processing(2018)</h4><h5 id="一句话总结-1"><a href="#一句话总结-1" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>评估了从四种数据源（电子医疗、PubMed、Wikipedia、Google news）训练的词向量在运用到内部评估和外部下游任务(临床信息提取，生物医学信息提取，关系抽取)时的表现，发现没有通用的提高生物医学下游nlp任务的词嵌入模型。</p>
<h5 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h5><ol>
<li>内部评估使用的相似性测度的方式是：选择三个类别（disorder, symptom, drug）的目标词汇，从四种词向量中取出5个最相关的词，结果显然是用电子医疗和PubMed训练的词向量要好过Wikipedia和Google news；还有选择377个语义相关的词对，用四种词向量表示，观察词簇的密集程度，用相似性测度衡量（UMNSRS）</li>
<li>外部评估分三部分<ul>
<li>Clinical Information Extraction：检测骨折信息数据集；i2b2吸烟状态数据集。结果是通用词向量效果不一定比相关领域训练的词向量差</li>
<li>Biomedical Information Retrieval：提供医疗决策信息。结果是四种词向量几乎都无精度提升</li>
<li>drug-drug interaction (DDI) extraction：Google News词向量效果最好(虽然都差不多但还是很神奇)</li>
</ul>
</li>
</ol>
<h4 id="Comparing-CNN-and-LSTM-character-level-embeddings-in-BiLSTM-CRF-models-for-chemical-and-disease-named-entity-recognition-2018"><a href="#Comparing-CNN-and-LSTM-character-level-embeddings-in-BiLSTM-CRF-models-for-chemical-and-disease-named-entity-recognition-2018" class="headerlink" title="Comparing CNN and LSTM character-level embeddings in BiLSTM-CRF models for chemical and disease named entity recognition(2018)"></a>Comparing CNN and LSTM character-level embeddings in BiLSTM-CRF models for chemical and disease named entity recognition(2018)</h4><h5 id="一句话总结-2"><a href="#一句话总结-2" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>比较了同样的BiLSTM+CRF模型用char-CNN和char-bilstm对CDNER的影响，发现无太大差别不过CNN可以显著降低模型复杂度明显提高计算效率。</p>
<p>接着介绍一下NCBI NIH的几个词向量工作。</p>
<h4 id="BioWordVec-improving-biomedical-word-embeddings-with-subword-information-and-MeSH-2018"><a href="#BioWordVec-improving-biomedical-word-embeddings-with-subword-information-and-MeSH-2018" class="headerlink" title="BioWordVec, improving biomedical word embeddings with subword information and MeSH(2018)"></a>BioWordVec, improving biomedical word embeddings with subword information and MeSH(2018)</h4><h5 id="一句话总结-3"><a href="#一句话总结-3" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>传统直接用领域相关的语料训练词向量如word2vec或者glove，即使glove是context相关的能解决一词多义问题，但也还是没有利用到文本内部的结构信息，本文用MeSH条款提供subword信息，能够有效增加OOV的词嵌入表示，极大提升词嵌入性能。</p>
<h5 id="方法-2"><a href="#方法-2" class="headerlink" title="方法"></a>方法</h5><img src="/2019/07/09/文献阅读0709/2.png">
<ol>
<li>采样MeSH条款序列：利用随机游走采样算法node2vec把用RDF构建的MeSH术语图转换成有序的术语序列，然后再把对应的id转成单词，这样链表就转成了具有语义关系的类似于句子的有向单词序列。</li>
<li>子词嵌入模型：在统一的字符嵌入空间中学习文本序列和MeSH术语序列，也就是同时用PubMed和MeSH训练n-grams。用了类似于CBOW模型的fastText算法，学习分布式的字符表示，然后每个单词的词向量是这些n-grams的组合和原Vc的点乘，训练方式也类似于word2vec，只不过是训练字符，优化损失函数也是两个skip-gram损失函数的叠加。</li>
</ol>
<h5 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h5><img src="/2019/07/09/文献阅读0709/3.png">
<h5 id="应用案例"><a href="#应用案例" class="headerlink" title="应用案例"></a>应用案例</h5><ol>
<li>句子对间相似度计算：多采用将句子中的每个单词转换为词向量然后平均求相似度</li>
<li><strong>生物医学关系抽取</strong>：这是我关注的重点。文章在二元关系PPI和多元关系DDI上做了测试，采用带dropout的CNN模型，还有复杂的RNN模型测试，均取得了最高的F-score。但在RNN上精度提升不明显，可能是SOTA的RNN模型是集成了最短路径依赖信息和POS嵌入的多层双向LSTM，降低了词嵌入层的重要性。</li>
</ol>
<img src="/2019/07/09/文献阅读0709/4.png" title="DDI 2013提取结果对比">
<h4 id="BioSentVec-creating-sentence-embeddings-for-biomedical-texts-2018"><a href="#BioSentVec-creating-sentence-embeddings-for-biomedical-texts-2018" class="headerlink" title="BioSentVec: creating sentence embeddings for biomedical texts(2018)"></a>BioSentVec: creating sentence embeddings for biomedical texts(2018)</h4><h5 id="一句话总结-4"><a href="#一句话总结-4" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>使用PubMed和MIMIC-III临床数据库类似于CBOW模型训练sent2vec，能在给定任意句子作为输入的情况下生成句子向量，能在高维空间中更好表征语义信息。在句子相似性和多标签句子分类数据集上做了测试。</p>
<img src="/2019/07/09/文献阅读0709/5.png" title="在句子分类数据集上采用的模型和精度">
<h4 id="Transfer-Learning-in-Biomedical-Natural-Language-Processing-An-Evaluation-of-BERT-and-ELMo-on-Ten-Benchmarking-Datasets-2019"><a href="#Transfer-Learning-in-Biomedical-Natural-Language-Processing-An-Evaluation-of-BERT-and-ELMo-on-Ten-Benchmarking-Datasets-2019" class="headerlink" title="Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets(2019)"></a>Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets(2019)</h4>]]></content>
  </entry>
  <entry>
    <title>文献阅读0708</title>
    <url>/2019/07/08/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB0708/</url>
    <content><![CDATA[<p>上篇主要是用多任务学习做NER的文献，这次关注一下迁移学习。正文如下：</p>
 <a id="more"></a>
<h4 id="Transfer-learning-for-biomedical-named-entity-recognition-with-neural-networks-2018"><a href="#Transfer-learning-for-biomedical-named-entity-recognition-with-neural-networks-2018" class="headerlink" title="Transfer learning for biomedical named entity recognition with neural networks(2018)"></a>Transfer learning for biomedical named entity recognition with neural networks(2018)</h4><h5 id="一句话总结"><a href="#一句话总结" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>深度学习大量依赖金标准数据集GSCs，带噪声的银标准数据集SSCs能够用于迁移学习弥补金标准数据集较少的情况，目标数据集越小迁移带来的精度提高越明显，目标GSC数据集较大时反而可能会恶化精度。<a href="https://github.com/
BaderLab/Transfer-Learning-BNER-Bioinformatics-2018/" target="_blank" rel="noopener">项目地址</a></p>
<h5 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h5><p>迁移学习的原理就是在”source”数据集上学到的知识能够帮助提高”target”数据集的表现，提高泛化能力，减少训练次数，模型本身baseline用的是<a href="https://github.com/Franck-Dernoncourt/NeuroNER" target="_blank" rel="noopener">neuroner</a>，一个BiLSTM字符嵌入+BiLSTM词嵌入+CRF的网络，注意一点就是训练时要从银标准数据集中剔除那些出现在金标准数据集中的PubMed ID。</p>
<h5 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h5><p>在23个金标准数据集上与baseline相比平均误差降低了11％</p>
<h4 id="Effective-Use-of-Bidirectional-Language-Modeling-for-Transfer-Learning-in-Biomedical-Named-Entity-Recognition-2018"><a href="#Effective-Use-of-Bidirectional-Language-Modeling-for-Transfer-Learning-in-Biomedical-Named-Entity-Recognition-2018" class="headerlink" title="Effective Use of Bidirectional Language Modeling for Transfer Learning in Biomedical Named Entity Recognition(2018)"></a>Effective Use of Bidirectional Language Modeling for Transfer Learning in Biomedical Named Entity Recognition(2018)</h4><h5 id="一句话总结-1"><a href="#一句话总结-1" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>在无标签数据上运用双向语言模型来预训练初始化模型权重，对金标准数据NER做迁移学习来提高精度。</p>
<h5 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h5><p>基本模型也是char-textCNN+word-BiLSTM+CRF，对每套数据集，用training和validation训练双向语言模型，双向语言模型的训练方法是最小化前后语言模型的平均交叉熵CElm = −λ(log pf (w1:n) + log pb(wn:1))。然后在此权重参数基础上训练NER模型微调，得到decoder CRF的参数。</p>
<h5 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h5><img src="/2019/07/08/文献阅读0708/1.png" title="4个数据集上的效果提升">
<p>效果比之前那个多任务学习的精度要好，而且只针对当前数据集训练，没有用到别的数据集信息。最后还绘制了learning curve曲线展示了无论target数据集有多大，都能带来精度的提升。</p>
<h4 id="Towards-reliable-named-entity-recognition-in-the-biomedical-domain-2019"><a href="#Towards-reliable-named-entity-recognition-in-the-biomedical-domain-2019" class="headerlink" title="Towards reliable named entity recognition in the biomedical domain(2019)"></a>Towards reliable named entity recognition in the biomedical domain(2019)</h4><h5 id="一句话总结-2"><a href="#一句话总结-2" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>一个很大的问题是在金标准数据集训练好的NER模型很难泛化到其他数据集上（主要还是训练数据太少了），更别说直接用到生物医学非结构化文本抽取了。这篇文章用了几个方法来improve regularization，包括Variational Dropout、多任务学习和迁移学习，然后综合这些模型的优点开源了一个Python工具包。<a href="https://github.com/BaderLab/saber" target="_blank" rel="noopener">工具地址</a>  <a href="https://github.com/BaderLab/Towards-reliable-BioNER" target="_blank" rel="noopener">项目地址</a></p>
<h5 id="方法-2"><a href="#方法-2" class="headerlink" title="方法"></a>方法</h5><ol>
<li><p>Variational Dropout：文章说之前的模型用dropout主要是在字符嵌入层和词嵌入层的参数上，很少有用到LSTM层的？？？没有啊我之前看的都用在了LSTM层。但确实LSTM层的dropout不太一样，因为直接使用会损害RNNs对文本的长距离依赖性。而2016年提出的variational dropout能够解决这一问题。因为它不是在每个time step都随机丢弃一些单元，而是在跨多个time steps丢弃相同的单元(因为LSTM是按照时间序列展开的，最终dropout会作用在同一层的连续位置，如图显示相同颜色的线代表在整个序列使用相同的dropout mask。我理解的就是有的一整个输入句子整个都被dropout，有的整个句子都不dropout，这样可以保证连续性)</p>
<img src="/2019/07/08/文献阅读0708/2.png" title="Naive dropout v.s. variational dropout">
</li>
<li><p>迁移学习：同最上面那篇，用的数据集也是CALBC-SSC-III</p>
</li>
<li><p>多任务学习：用的是共享权重的方法，target-specific CRF前的所有字符层和单词层的隐层单元都共享权重，然后每个数据集计算各自的损失并优化，当所有的数据集都训练一遍（顺序随机，总共需要二项式系数次）算一个epoch。为什么没用之前那篇多任务文章提到的优化总的损失函数我也不知道，可能分别计算效果好吧。</p>
</li>
</ol>
<h5 id="结果-2"><a href="#结果-2" class="headerlink" title="结果"></a>结果</h5><p>文章花大量篇幅展示了baseline+VD，baseline+TL，MTL模型与baseline相比在In-corpus (IC) performance和Out-of-corpus (OOC) performance的效果提升，并分析了这些方法对提高泛化能力的作用。最后总结了OOC相比IC平均会减少31.16%的精度，加上这些方法的作用能够平均挽回10％的精度。开源的模型对用在PubMed大规模文本抽取有效。</p>
]]></content>
  </entry>
  <entry>
    <title>文献阅读0706</title>
    <url>/2019/07/06/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB0706/</url>
    <content><![CDATA[<p>受<a href="https://github.com/BrambleXu/knowledge-graph-learning" target="_blank" rel="noopener">awesome-knowledge-graph</a>这个项目的启发，看到自己项目里也存了大概一百篇文献，所以决定在博客记录下每天看过的文献，其中一部分是之前看过的，不过模型没有上手很容易忘记，所以算是重看整理，主要是想督促自己多写点东西。</p>
<a id="more"></a>
<h4 id="A-neural-network-multi-task-learning-approach-to-biomedical-named-entity-recognition-2016"><a href="#A-neural-network-multi-task-learning-approach-to-biomedical-named-entity-recognition-2016" class="headerlink" title="A neural network multi-task learning approach to biomedical named entity recognition(2016)"></a>A neural network multi-task learning approach to biomedical named entity recognition(2016)</h4><h5 id="一句话总结"><a href="#一句话总结" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>生物医学领域命名实体识别的金标准数据集量少，数据集间具有高度相关性但标注的又是不同的内容(Anatomy, Chemical, Disease, Gene/Protein and Species, Cell Type)，一个自然的思路是能否利用多任务学习提高NER精度，文章以卷积神经网络为基础构建了单任务模型和两个多任务模型验证了这一点。<a href="https://github.com/cambridgeltl/MTL-Bioinformatics-2016" target="_blank" rel="noopener">项目地址</a></p>
<h5 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h5><ol>
<li><p>预训练词嵌入向量：用的是<a href="http://bio.nlplab.org" target="_blank" rel="noopener">wikipedia-pubmed-and-PMC-w2v</a>，注意OOV的处理</p>
</li>
<li><p>数据集：整合了15个ner数据集（包括BIO和BIOES两种格式）和1个GENIA-PoS-Tagging数据集</p>
</li>
<li><p>Baseline model ：全连接前馈神经网络，隐层大小300，激活函数ReLU，输出用Softmax分类，单个输入长度大小为7，batch-size 50，SGD优化</p>
</li>
<li><p>三个CNN模型：</p>
<img src="/2019/07/06/文献阅读0706/2.png" title="从左到右：单任务模型、多输出多任务模型、从属多任务模型">
<ul>
<li><p>Single task model：高度为3、4、5各100个卷积核对长度为7的词窗做卷积，卷积核宽度等于词嵌入维度，为了保留位置信息没有用最大池化，卷积层到全连接层激活函数也是relu，多分类交叉熵损失函数，mini-batch 200，Adam优化，学习率1e-4，全连接层dropout 0.75</p>
</li>
<li><p>Multi-output multi-task model：共享前面的词嵌入和卷积层，后面的全连接和softmax随任务类型不同</p>
</li>
<li><p>Dependent multi-task model：NER任务运用POS信息，前面的CNN层分开训练，然后拼接后面的全连接层用来预测ner输出</p>
</li>
</ul>
</li>
</ol>
<h5 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h5><p>如图：多输出模型能够对原single模型产生较大影响(多数是效果改进，少数会效果变差)，dependent模型对single模型带来的精度提高不如多输出，但整体趋势都是稍微改进无恶化的(至少是无实质变化)，也证明了我们的预期——具有相关关系的数据集用于多任务模型中共享输入特征和权重，结果不一定会相互促进，而从属模型大多能利用辅助数据集的信息来提升精度虽然效果可能不明显。<br><img src="/2019/07/06/文献阅读0706/3.png" title="F-score比较"></p>
<p>这篇多任务模型/迁移学习可以改进的地方还有很多：比如单任务模型换成BiLSTM+CRF；多任务思路也可以变化：从共享权重变为协同训练；从迁移模型到迁移数据。。。这就引出接下来很多篇文章：</p>
<h4 id="Cross-type-Biomedical-Named-Entity-Recognition-with-Deep-Multi-Task-Learning-2018"><a href="#Cross-type-Biomedical-Named-Entity-Recognition-with-Deep-Multi-Task-Learning-2018" class="headerlink" title="Cross-type Biomedical Named Entity Recognition with Deep Multi-Task Learning(2018)"></a>Cross-type Biomedical Named Entity Recognition with Deep Multi-Task Learning(2018)</h4><h5 id="一句话总结-1"><a href="#一句话总结-1" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>双向LSTM+CRF网络通过共享字符和单词级别的嵌入层参数来提高NER精度。<a href="https://github.com/yuzhimanhua/Multi-BioNER" target="_blank" rel="noopener">项目地址</a></p>
<h5 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h5><img src="/2019/07/06/文献阅读0706/7.png" title="三种共享参数层的方式">
<p>创新点是提出了三种共享参数层权重的方式来优化总的损失函数</p>
<img src="/2019/07/06/文献阅读0706/8.png" title="The loss function L">
<p>还使用了CTD的单词信息作补充，用在两个地方：一是充当dictionary feature提供N-gram信息（结果并没有带来精度提高），二是用于后处理匹配那些被预测为O-label的单词，从而降低FN值(被预测为负的正样本)，但最后结果反而变差了，因为大大提高了FP值（有些单词表面和词典一样但是意思不同）。</p>
<h5 id="训练参数设置"><a href="#训练参数设置" class="headerlink" title="训练参数设置"></a>训练参数设置</h5><p>词嵌入200维，字符嵌入30维，两个双向LSTM隐藏层都是200，初始学习率0.01，其他大部分参数都和Neural architectures for named entity recognition这篇文章一样</p>
<h5 id="可以考虑改进的地方"><a href="#可以考虑改进的地方" class="headerlink" title="可以考虑改进的地方"></a>可以考虑改进的地方</h5><p>没有经过normalization所以很难判断预测的实体边界；虽然是一起训练的但对每个数据集都会学习得到一个模型，然后预测原始文本的时候是这些模型分别预测生成各自的预测结果，很容易造成冲突。</p>
<h4 id="CollaboNet-collaboration-of-deep-neural-networks-for-biomedical-named-entity-recognition-2018"><a href="#CollaboNet-collaboration-of-deep-neural-networks-for-biomedical-named-entity-recognition-2018" class="headerlink" title="CollaboNet: collaboration of deep neural networks for biomedical named entity recognition(2018)"></a>CollaboNet: collaboration of deep neural networks for biomedical named entity recognition(2018)</h4><h5 id="一句话总结-2"><a href="#一句话总结-2" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>在不同数据集上训练好的模型轮流充当主任务模型，其他数据集作为辅助模型来训练，这样协同训练来整体降低多义词的误分类，降低FP值。<a href="https://github.com/wonjininfo/CollaboNet" target="_blank" rel="noopener">项目地址</a></p>
<h5 id="方法-2"><a href="#方法-2" class="headerlink" title="方法"></a>方法</h5><ol>
<li><img src="/2019/07/06/文献阅读0706/9.png">
<p>单任务模型结构如图，和前一篇文章不同的地方是在Character Level Word Embedding (CLWE)层用的是text-CNN而不是Bi-LSTM，用dclwe个卷积核捕获窗口为k的字符（经最大池化），得到dclwe维的字符嵌入向量，然后和词向量拼接在一起作为双向LSTM-CRF的输入。</p>
<p>整个过程的公式如下图左侧所示(因为推导的比较清晰所以都列出来了)：</p>
<img src="/2019/07/06/文献阅读0706/10.png">
</li>
<li><p>多任务模型结构如上图右，训练方式为：P0 Phase先把各数据集上的单个模型都训练一遍，在Pn phase，训练target模型时，将target模型的数据集和前一phase其他模型在target数据集上训练的输出加权合并做输入，用来训练前一phase的target模型，得到当前时刻的target输出。</p>
</li>
</ol>
<h5 id="训练参数设置-1"><a href="#训练参数设置-1" class="headerlink" title="训练参数设置"></a>训练参数设置</h5><p>词嵌入200维，字符向量初始化30维，然后采用3、5、7的卷积核各200个，所以得到字符嵌入维度200*3，拼接后经过前后各300维的LSTM，mini-batch为10，dropout分别是(CLWE)0.5和(BiLSTM)0.3，初始学习率0.01</p>
<h5 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h5><img src="/2019/07/06/文献阅读0706/11.png">
<p>在相关数据集上取得了比上一篇文章更高的F值</p>
<h4 id="An-attention-based-BiLSTM-CRF-approach-to-document-level-chemical-named-entity-recognition-2017"><a href="#An-attention-based-BiLSTM-CRF-approach-to-document-level-chemical-named-entity-recognition-2017" class="headerlink" title="An attention-based BiLSTM-CRF approach to document-level chemical named entity recognition(2017)"></a>An attention-based BiLSTM-CRF approach to document-level chemical named entity recognition(2017)</h4><h5 id="一句话总结-3"><a href="#一句话总结-3" class="headerlink" title="一句话总结"></a>一句话总结</h5><p>传统神经网络NER是基于句子级别的标注，容易造成同一文档不同句子间标注不一致性的问题，通常需要在后处理时加规则强行统一。本文提出了一个基于注意力机制的双向LSTM+CRF网络，能够实现文档级别的标注，在BC4CHEMDNER和BC5CDR数据集上都取得了SOTA的结果。<a href="https://github.com/lingluodlut/Att-ChemdNER" target="_blank" rel="noopener">项目地址</a></p>
<h5 id="方法-3"><a href="#方法-3" class="headerlink" title="方法"></a>方法</h5><ol>
<li><p>input features：word embedding and character embedding作为基本，POS, chunking and dictionary embedding作额外补充。word embedding是用查询”chemical”关键词得到的文献在word2vec基础上训练的（很具有启发性）；POS和chunking是用GENIA tagger获得的；dictionary embedding是用化学词典（Jochem，ChEBI和CTD）匹配实体得到的。</p>
</li>
<li><p>基础的BiLSTM-CRF模型和改进的Att-BiLSTM-CRF模型：</p>
<img src="/2019/07/06/文献阅读0706/4.png">
<p>双向LSTM层能够捕获序列的当前输入时刻上下文相关的信息；然后经过一个注意力层，用global vector捕获LSTM的输出加权，attention层的权重计算用了四种：曼哈顿距离、欧几里得距离、余弦相似度和感知机tanh(Wa[Xt;Xj])，attention 层的输出是global vector和LSTM输出状态向量的拼接然后通过一个tanh激活函数得到的，即Zt = tanh(Wg[gt;ht])，Zt然后再经一个tanh输出即tanh(WeZt)；CRF层是在输出概率矩阵上增加了状态转移矩阵建立隐藏状态的依赖，最终CRF输出也是经softmax用分类交叉熵计算损失，然后维特比算法递推优化。</p>
</li>
</ol>
<h5 id="训练参数设置-2"><a href="#训练参数设置-2" class="headerlink" title="训练参数设置"></a>训练参数设置</h5><img src="/2019/07/06/文献阅读0706/5.png">
<h5 id="结果-2"><a href="#结果-2" class="headerlink" title="结果"></a>结果</h5><img src="/2019/07/06/文献阅读0706/6.png">
<p>加入了额外特征的模型在两个数据集上都达到了SOTA的效果，优于NCBI的taggerone</p>
]]></content>
  </entry>
  <entry>
    <title>脑空间信息知识图谱技术</title>
    <url>/2019/05/08/%E8%84%91%E7%A9%BA%E9%97%B4%E4%BF%A1%E6%81%AF%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%8A%80%E6%9C%AF/</url>
    <content><![CDATA[<p>因为马上去上海开会交流还要做PPT展示，所以先写篇文章总结下目前这个项目我能实现的技术和卡住的地方，也算是对入手这个项目一年多的技术总结。</p>
<p>脑空间信息学是以脑连接的基本结构与功能单元为研究对象，揭示脑连接空间信息机制，引导脑疾病防治与智能技术发展的新兴交叉学科。脑空间信息知识库能够为研究认知和行为的神经活动机制等脑科学问题提供帮助。由于脑科学的研究范围涵盖面广，领域跨度大，研究人员很难具有跨自身专业的知识；此外脑科学领域的文献数目逐年剧增，了解并跟进前沿知识费时费力，催生了自动化获取文献信息的需求。项目目标是综合运用文本挖掘、自然语言处理和机器学习技术，建立一个服务脑科学研究人员，整合已有的知识库，能实现文献语义级别搜索的知识图谱。</p>
<p>知识图谱的构建流程大致是知识抽取、知识融合、知识加工、知识存储和知识表示。我的工作主要是前面的知识抽取和知识融合中的技术细节，主要做了文献挖掘、实体抽取和消歧、关系/事件抽取、知识库和本体整合等内容。详细如下：</p>
 <a id="more"></a>
<ol>
<li><p>文献获取和预处理</p>
<ul>
<li>以E-utilities向PubMed发送查询并下载文献，以关键词核团”nucleus accumbens”为例，共下载19572篇摘要，以txt的格式返回它们的，标识符pmids</li>
<li>使用StanfordNLP进行文本清洗：包括分词、过滤字符、去除停用词、词干化处理、词性标注等</li>
<li>统计词频、语言模型、tf-idf等</li>
</ul>
<p>遇到的问题及难点：PMC全文的获取有困难，缺少文献数据库</p>
</li>
<li><p>知识抽取</p>
<img src="/2019/05/08/脑空间信息知识图谱技术/1.png">
<p>知识抽取根据不同的数据源分别进行，我们主要针对的是文献中的纯文本数据。三个子任务包括：</p>
<ul>
<li>实体抽取：识别文本中有意义的实体，通常是名词</li>
<li>关系抽取：SPO三元组，事件抽取相当于多元关系的抽取</li>
<li>属性抽取：三元组中一个谓词和两个形参各自的属性，也可以简化为关系抽取</li>
</ul>
<p>实体抽取：</p>
<p>采用端到端的深度学习算法，基于多任务学习的双向LSTM+CRF网络，采用的训练数据集为CoNLL格式的金标准生物医学NER数据集，该系统能够识别基因、蛋白质、细胞类型、化学物质、疾病实体。</p>
<p>模型和训练数据集详细信息如图：</p>
<img src="/2019/05/08/脑空间信息知识图谱技术/2.png">
<img src="/2019/05/08/脑空间信息知识图谱技术/3.png">
<p>目前识别出的结果如下：</p>
<img src="/2019/05/08/脑空间信息知识图谱技术/6.png">
<p>后期准备整合Pubtator和UMLS对识别出来的实体按照实体类型和MESH条款进行管控。</p>
<p>遇到的问题及难点：</p>
<ul>
<li>难以识别出脑区核团、神经元等一些金标准训练集中未标定出来的实体类型。</li>
<li>金标准训练集的数量太少，训练模型易欠拟合，泛化能力差，难以对大量的文献文本做实体预测，且难以对预测结果的精确度进行定量评估。</li>
</ul>
<p>关系抽取：</p>
<p>同样是基于端到端的方法，模型是加入自注意力机制的双向LSTM网络，主要抽取的是基因-化学物质-疾病这三者相互之间的关系。</p>
<p>模型和训练数据集详细信息如图：</p>
<img src="/2019/05/08/脑空间信息知识图谱技术/7.png">
<img src="/2019/05/08/脑空间信息知识图谱技术/8.png">
<p>目前的训练精度很差，准备加入句法依存树解决句法依赖的问题，还有配合生物医学本体建立层级间的关联，加入远程监督和多实例学习解决训练样本太少的问题。</p>
<p>遇到的问题及难点：</p>
<ul>
<li>关系抽取难度大，主流的最好的模型精度也只有60-70％</li>
<li>划定关系类型没有统一的标准，严重依赖本体层级的划分，需要神经科学背景的研究人员提供指导</li>
<li>关系抽取得到的结果中包含的主语和宾语实体需要与实体抽取的结果匹配，可以考虑采用联合模型同时抽取实体和它们间的关系。</li>
</ul>
</li>
<li><p>知识融合</p>
<p>主要做了SemMedDB关系数据库(NIH抽取PubMed摘要得到的9400多万个关系)与本体层面的融合。</p>
<p>采用的本体是biolink，是一个以节点、边、槽表示生物医学实体和关系的本体层架构。SemMedDB数据库以MySQL的形式存储，先将其通过UMLS进行规范化，然后用biolink在模式层管理这些关系。</p>
<p>遇到的问题及难点：</p>
<ul>
<li>原SemMedDB是采用SemRep系统抽取，它的精确度、召回率和F值分别为 0.73, 0.55, 0.63</li>
</ul>
</li>
</ol>
<ul>
<li>由于精确度较低，所以需要先过滤总共只出现很少次的关系类型，在次数的选择上也需要折中考虑关系的误判和漏判<ul>
<li>由于系统只能做到抽取单个句子中的实体间的关系，很难抽取到跨句子级别的关系，所以召回率很低，但是PubMed摘要中的很多关系只出现在整个文档级别中</li>
</ul>
</li>
<li>biolink并不一定是最适合我们的生物医学本体，需要内部专家修改和评估。</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>SemMedDB2Biolink：PubMed语义数据的规范和展示</title>
    <url>/2019/04/20/SemMedDB2Biolink%EF%BC%9APubMed%E8%AF%AD%E4%B9%89%E6%95%B0%E6%8D%AE%E7%9A%84%E8%A7%84%E8%8C%83%E5%92%8C%E5%B1%95%E7%A4%BA/</url>
    <content><![CDATA[<p>这个月的工作是对SemMedDB与Biolink Model的初探和尝试。SemMedDB(The Semantic MEDLINE Database)是一个以SPO三元组存储了PubMed中文本语义关系的数据库，大概有9400万条预测关系。而Biolink Model是一个高级别的生物医学实体和关系框架。通过把SemMedDB的关系型数据用Biolink Model规范与展示，可以实现更广泛的生物医学知识推理和运用。转换过程中还用到了一体化医学语言系统UMLS的术语映射。正文如下：</p>
 <a id="more"></a>
<p>生物医学领域文献繁多，隐藏的语义信息庞大而繁杂，如何通过文献挖掘为生物医学研究人员提取一些常识性的论断，还有如何高效地在文献中找到自己所需要的关键信息是知识库构建的初衷。PubMed提供了NLM的MEDLINE数据库千万篇文献摘要和索引信息，而SemMedDB是NLM用SemRep系统抽取PubMed语义信息得到的数据库，核心是近1亿条以主语-谓语-宾语的形式存储的生物医学关系表，存储在MySQL数据库里，模式scheme如下：</p>
<img src="/2019/04/20/SemMedDB2Biolink：PubMed语义数据的规范和展示/1.png" title="PREDICATION table of SemMedDB">
<p>主语信息以SUBJECT_NAME, SUBJECT_CUI（概念标识符）, SUBJECT_SEMTYPE和SUBJECT_NOVELTY管控，谓语信息是PREDICATE表示的，宾语信息同主语信息，并提供了出现该语义关系的PubMed文献的PMID值及出现的句子位置。</p>
<p>SemMedDB的实体概念是以UMLS管控的，UMLS全称统一医学语言系统，它的元叙事表是由各种受控词表和术语以及它们之间的关系所构成的集合，包括医学主题词MeSH等，是生物医学信息的规范。</p>
<p>Biolink模型是生物医学实体和关系的本体层架构，它以节点、边和槽定义模式，节点代表实体；边就是实体间的关系；槽分为节点属性、边属性和关系类型。如图是用本体软件protege展示的节点层：</p>
<img src="/2019/04/20/SemMedDB2Biolink：PubMed语义数据的规范和展示/2.png" title="Entity (Node) Types">
<p>它是一个更高层管理数据的方式，类似于用RDF或OWL描述的本体层，如图：</p>
<img src="/2019/04/20/SemMedDB2Biolink：PubMed语义数据的规范和展示/3.jpg" title="PREDICATION table of SemMedDB">
<p>我的思路是将SemMedDB提供的生物医学关系与Biolink模型的模式建立映射关系，从而可以用SparQL实现语义级别的查询，还有用图数据库Neo4j展示。</p>
<p>分为如下几部实现：</p>
<ol>
<li>原始数据清洗</li>
<li>使用biolink对实体类型规范化</li>
<li>依实体类型过滤关系类型</li>
<li>限定领域范围，依照本体修改mapping</li>
<li>关联外部数据库</li>
<li>用Neo4j图数据库展示</li>
</ol>
<p>下面依次介绍：</p>
<ol>
<li><p>原始数据清洗：</p>
<p>首先提取SemMedDB数据库的生物医学关系，其中’SUBJECT_CUI’, ‘PREDICATE’, ‘OBJECT_CUI’, ‘PMID’这几项内容是我需要的。然后将含有同一SPO三元组的PMID值合并；加入NEG一列指示关系类型为positive或negative，处理后将其保存为关系数据表。</p>
<img src="/2019/04/20/SemMedDB2Biolink：PubMed语义数据的规范和展示/4.png" title="SemMedDB ——> 关系表：22,280,924条关系">
<p>实体数据表则是由UMLS的受控词表MRCONSO_ENG.RRF清洗得到。其中的ID值对应SemMedDB的SUBJECT_CUI或OBJECT_CUI，label是ID对应的实体名称。</p>
<img src="/2019/04/20/SemMedDB2Biolink：PubMed语义数据的规范和展示/5.png" title="UMLS ——> 实体表：259,227个实体">
</li>
<li><p>使用biolink对实体类型规范化：</p>
<p>这一步按照biolink模型的框架描述过滤了实体表。首先在实体表中加入实体的类型值umls_type，这是从UMLS的另一张受控词表MRSTY.RRF得到的。然后将umls_type映射到biolink本体的node_type，实现实体类型的规范化。剔除了不含有node_type的节点。</p>
<img src="/2019/04/20/SemMedDB2Biolink：PubMed语义数据的规范和展示/6.png" title="映射后的实体表">
<img src="/2019/04/20/SemMedDB2Biolink：PubMed语义数据的规范和展示/7.png" title="用biolink的node_type过滤后的实体类型排序">
</li>
<li><p>过滤实体类型和关系类型：</p>
<p>这一步用上一步过滤好的实体类型和biolink模型自身的关系类型过滤第一步存储的边表。移除了不含有UMLS类型的主语或宾语构成的SPO三元组关系，移除了[‘compared_with’, ‘higher_than’, ‘lower_than’, ‘different_from’, ‘different_than’, ‘same_as’,’OCCURS_IN’, ‘PROCESS_OF’, ‘DIAGNOSES’, ‘METHOD_OF’, ‘USES’,’AUGMENTS’, ‘ADMINISTERED_TO’, ‘COMPLICATES’]这些biolink模型不含有的关系，再用筛选后的关系表反过来过滤实体表。这一步后剩余165,765个实体和14,066,229条关系。</p>
<img src="/2019/04/20/SemMedDB2Biolink：PubMed语义数据的规范和展示/8.png" title="过滤后的关系类型数目排序">
</li>
<li><p>限定领域范围，依照本体修改mapping：</p>
<p>先在关系表中加入关系的领域domain和范围range，如图：</p>
<img src="/2019/04/20/SemMedDB2Biolink：PubMed语义数据的规范和展示/9.png" title="修改后的关系表">
<p>然后依照领域和范围，用自定义的知识库对预测领域范围做限定，根据自己本体修改mapping文件，这一步实现了本体层和数据层的融合，即用本体模式实现了对数据的管控，便于之后的查询和展示。</p>
<img src="/2019/04/20/SemMedDB2Biolink：PubMed语义数据的规范和展示/10.png" title="最终得到的关系表">
</li>
<li><p>关联外部数据库：</p>
<p>Chemical and Drugs：链接到FDA</p>
<p>Anatomy：链接到uberon</p>
<p>Disease：链接到Disease Ontology</p>
<p>Proteins：UMLS自带的MESH条款管控</p>
<p>biological_process_or_activity/activity_and_behavior：UMLS自带的GO(gene ontology)</p>
<p>Gene：链接到HGNC（人类基因组数据库） and OMIM（人类遗传学数据库）</p>
<p>结果展示：</p>
<img src="/2019/04/20/SemMedDB2Biolink：PubMed语义数据的规范和展示/11.png" title="关联外部数据库的:实体表">
</li>
<li><p>用Neo4j图数据库展示：</p>
<img src="/2019/04/20/SemMedDB2Biolink：PubMed语义数据的规范和展示/12.png">
<img src="/2019/04/20/SemMedDB2Biolink：PubMed语义数据的规范和展示/13.png">
</li>
</ol>
<p>整个清理流程的结果：</p>
<p>Start:<br>20,620,113 edges<br>268,918 nodes<br>32 predicates<br>15 node types</p>
<p>End:<br>14,033,126 edges<br>165,658 nodes<br>18 predicates<br>13 node types</p>
]]></content>
  </entry>
  <entry>
    <title>生物医学知识库技术调研</title>
    <url>/2019/04/09/%E7%94%9F%E7%89%A9%E5%8C%BB%E5%AD%A6%E7%9F%A5%E8%AF%86%E5%BA%93%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/</url>
    <content><![CDATA[<p>最近对我找到的一些成型的生物医学领域的知识库进行了系统调研，文章大多是谷歌搜索”biomedical knowledge graph OR base”找到的，有的是对一个成熟系统的功能介绍文章，有的是一些面向生物信息挖掘和图谱构建过程的技术文章，依据这些技术还追根溯源找到了很多可用的工具。简单记几个重要的跟我们预期功能类似的：</p>
<a id="more"></a>
<h4 id="SemMedDB-a-PubMed-scale-repository-of-biomedical-semantic-predications-2012"><a href="#SemMedDB-a-PubMed-scale-repository-of-biomedical-semantic-predications-2012" class="headerlink" title="SemMedDB: a PubMed-scale repository of biomedical semantic predications(2012)"></a>SemMedDB: a PubMed-scale repository of biomedical semantic predications(2012)</h4><p>隶属NIH的The Semantic Knowledge Representation project，基于<a href="https://www.nlm.nih.gov/research/umls/" target="_blank" rel="noopener">UMLS</a>标准化的，用<a href="https://semrep.nlm.nih.gov" target="_blank" rel="noopener">SemRep</a>（基于规则）抽取PubMed的标题和摘要得到的关系数据库，包含大约9400万条预测关系，具有一个网页应用<a href="https://skr3.nlm.nih.gov/SemMed/index.html" target="_blank" rel="noopener">SemMed</a></p>
<p>The Semantic MEDLINE Database (SemMedDB) is a repository of <strong>semantic predications</strong> (subject-predicate-object triples) extracted from PubMed abstracts by SemRep, a semantic interpreter of biomedical text.</p>
<p>SemRep extracts 30 predicate types, largely relating to clinical medicine (e.g. TREATS, DIAGNOSES, ADMINISTERED_TO, PROCESS_OF), substance interactions(e.g. INTERACTS_WITH, INHIBITS, STIMULATES), genetic etiology of disease (e.g. ASSOCIATED_WITH, CAUSES, PREDISPOSES) and pharmacogenomics (e.g. AFFECTS, AUGMENTS, DISRUPTS).</p>
<p>MySQL database： <a href="http://skr3.nlm.nih.gov/SemMedDB" target="_blank" rel="noopener">http://skr3.nlm.nih.gov/SemMedDB</a></p>
<h4 id="Knowledge-Bio-A-Web-application-for-exploring-building-and-sharing-webs-of-biomedical-relationships-mined-from-PubMed-2016"><a href="#Knowledge-Bio-A-Web-application-for-exploring-building-and-sharing-webs-of-biomedical-relationships-mined-from-PubMed-2016" class="headerlink" title="Knowledge.Bio: A Web application for exploring, building and sharing webs of biomedical relationships mined from PubMed(2016)"></a>Knowledge.Bio: A Web application for exploring, building and sharing webs of biomedical relationships mined from PubMed(2016)</h4><p>基于SemMedDB和Implicitome的生物医学关系可视化平台，能够以图的方式展示实体间的关系。</p>
<ol>
<li><p>知识范围：以实体构成的关系，实体来源于UMLS中的生物医学概念和Entrez Gene中的基因类别</p>
</li>
<li><p>构建工具：Python Django框架，Jade templates, Bootstrap.js和DataTable.js模板，后台是以MySQL存储的SemmedDB和UMLS</p>
<img src="/2019/04/09/生物医学知识库技术调研/7.png" title="搜索界面">
</li>
</ol>
<h4 id="Thalia-semantic-search-engine-for-biomedical-abstracts-2018"><a href="#Thalia-semantic-search-engine-for-biomedical-abstracts-2018" class="headerlink" title="Thalia: semantic search engine for biomedical abstracts(2018)"></a>Thalia: semantic search engine for biomedical abstracts(2018)</h4><p>能识别八种生物医学实体的PubMed标注工具和语义搜索引擎。</p>
<ol>
<li><p>高亮、集成和链接信息的范围：daily updated PubMed</p>
</li>
<li><p>标注信息：chemicals, diseases, drugs, genes, metabolites, proteins, species and anatomical entities.对属于多个类别的同一实体进行了多重标注。</p>
</li>
<li><p>工具：Argo, a text mining workflow system.基于词典和条件随机场的监督学习NER工具</p>
</li>
<li><p>实体规范化本体：ChEBI (chemicals), DrugBank (drugs), HMDB (metabolites), HGNC (genes), UMLS Metathesaurus (dis- eases), UniProt (proteins), NCBI (species) and CARO (anatomical)</p>
</li>
<li><p>语义搜索：呈现基于实体共现的标准化后的实体链接</p>
</li>
<li><p>语义查询工具：基于Lucene的Elasticsearch搭建，REST API 地址：<a href="http://nactem-copious.man.ac.uk/thalia-wsgi/api" target="_blank" rel="noopener">http://nactem-copious.man.ac.uk/thalia-wsgi/api</a></p>
</li>
<li><p>网站：<a href="http://nactem-copious.man.ac.uk/Thalia/" target="_blank" rel="noopener">http://nactem-copious.man.ac.uk/Thalia/</a></p>
<img src="/2019/04/09/生物医学知识库技术调研/5.png" title="Thalia搜索界面一览">
<img src="/2019/04/09/生物医学知识库技术调研/6.png" title="单个PubMed词条高亮实体标注信息">
</li>
</ol>
<h4 id="iTextMine-integrated-text-mining-system-for-large-scale-knowledge-extraction-from-the-literature-2018"><a href="#iTextMine-integrated-text-mining-system-for-large-scale-knowledge-extraction-from-the-literature-2018" class="headerlink" title="iTextMine: integrated text-mining system for large-scale knowledge extraction from the literature(2018)"></a>iTextMine: integrated text-mining system for large-scale knowledge extraction from the literature(2018)</h4><p>这是一篇整合现有的信息抽取工具然后规范化输出结果以实现大规模生物知识抽取的文章。</p>
<ol>
<li><p>知识抽取范围：all Medline abstracts and PMC open access full-length articles</p>
</li>
<li><p>目标知识：gene/protein &amp; disease/therapy relations.</p>
</li>
<li><p>工具：RLIMS-P, eFIP, miRTex, eGARD</p>
</li>
<li><p>其他整合工具： PubTator(for entity normalization), SemRep, BioContext and Turku Event Extraction System</p>
</li>
<li><p>数据输出格式：JSON </p>
</li>
<li><p>构建流程：文本输入—&gt;平行处理—&gt;后处理—&gt;存储—&gt;数据输出—&gt;网页展示</p>
<img src="/2019/04/09/生物医学知识库技术调研/4.png" title="iTextMine system overview">
<ul>
<li>文本输入：从PubMed获取的全文以分段的XML表示</li>
<li>平行处理：在docker环境的Spark框架下，以chunk的形式用工具处理输入文本，然后用Hirschberg’s algorithm解决文本对齐问题。</li>
<li>后处理：用PubTator和本体库解决实体标准化的问题，用UniProt AC和NCBI管理本体和进行ID mapping，这一步主要是做实体消歧和指代消解。</li>
<li>存储：MongoDB存储标准化的 JSON格式数据。</li>
<li>数据输出：REST API  <a href="https://research.bioinformatics.udel.edu/itextmine/api/" target="_blank" rel="noopener">https://research.bioinformatics.udel.edu/itextmine/api/</a>  通过key和PMIDs管理结果。</li>
<li>网页展示：Apache Lucene</li>
</ul>
</li>
<li><p>网站：<a href="https://research.bioinformatics.udel.edu/itextmine/integrate" target="_blank" rel="noopener">https://research.bioinformatics.udel.edu/itextmine/integrate</a></p>
<img src="/2019/04/09/生物医学知识库技术调研/1.png" title="iTextMine搜索界面一览">
<img src="/2019/04/09/生物医学知识库技术调研/2.png" title="关键词查询结果展示">
<img src="/2019/04/09/生物医学知识库技术调研/3.png" title="单条文章结果展示">
</li>
</ol>
]]></content>
      <tags>
        <tag>知识图谱</tag>
      </tags>
  </entry>
  <entry>
    <title>BioRE文献阅读报告</title>
    <url>/2019/01/08/BioRE%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/</url>
    <content><![CDATA[<p>上篇是生物医学命名实体识别(BioNER)的文献调研报告，这篇是关系抽取(BioRE)的调研。<br>正文如下：<br> <a id="more"></a></p>
<h3 id="Biomedical-Relation-Extraction-From-Binary-to-Complex"><a href="#Biomedical-Relation-Extraction-From-Binary-to-Complex" class="headerlink" title="Biomedical Relation Extraction: From Binary to Complex"></a>Biomedical Relation Extraction: From Binary to Complex</h3><p>这篇文章从生物医学领域关系抽取的主要内容出发综述。NRE早期多侧重于二元关系的抽取，如PPI(protein- protein interactions)，如今多侧重于复杂的生物分子关系。文章描述了基本的抽取框架、抽取系统和方法。<br><img src="/2019/01/08/BioRE文献阅读报告/1.png" title="The general framework of a relation extraction system"></p>
<ol>
<li><p>二元关系抽取</p>
<ul>
<li><p>基于规则和模式：”PROTEIN1.∗ not (interact|associate|bind|complex)..∗ PROTEIN2.”</p>
</li>
<li><p>基于机器学习方法：转为分类问题(POS tagging, syntactic parsing, and dependency parsing)</p>
<img src="/2019/01/08/BioRE文献阅读报告/2.png" title="General procedure of a PPI extraction system employing different methodologies">
</li>
<li><p>可用数据集：</p>
<img src="/2019/01/08/BioRE文献阅读报告/15.png" title="Available annotated corpora for binary relation extraction in the biomedical domain.">
</li>
</ul>
</li>
<li><p>复杂关系抽取</p>
<ul>
<li><p>生物关系事件触发词识别：</p>
<ul>
<li>基于词典</li>
<li>基于规则：NN/NNS + of + PROTEIN and VBN + PROTEIN<img src="/2019/01/08/BioRE文献阅读报告/3.png" title="An example of identifying trigger words based on the predefined pattern."></li>
<li>基于机器学习</li>
</ul>
</li>
<li><p>事件抽取：</p>
<ul>
<li><p>基于规则：句法解析及修剪，候选词识别，模式抽取</p>
<img src="/2019/01/08/BioRE文献阅读报告/16.png" title="Feature-based approaches to event extraction">
</li>
<li><p>基于特征的分类器：SVM, decision tree</p>
</li>
<li><p>基于核函数：𝐾 (𝑥, 𝑦) = 𝑒^(−𝛾(edit_distance(𝑥,𝑦))) </p>
</li>
</ul>
</li>
<li><p>联合抽取：通过一系列二元提取实现</p>
</li>
<li>可用数据集<br>BioNLP’09, The BioNLP’11 shared task, The GENIA task, The EPI task</li>
</ul>
</li>
</ol>
<h3 id="A-Survey-of-Deep-Learning-Methods-for-Relation-Extraction"><a href="#A-Survey-of-Deep-Learning-Methods-for-Relation-Extraction" class="headerlink" title="A Survey of Deep Learning Methods for Relation Extraction"></a>A Survey of Deep Learning Methods for Relation Extraction</h3><p>这篇文章详述了关系抽取领域使用的深度学习模型，虽然不是具体针对生物医学领域的。</p>
<p>背景自不必多提，关系抽取是提取结构化信息构建知识库的关键。传统的抽取方法机器学习模型主要分为基于特征和基于核函数的方法，缺点是需要手动构建特征工程。<br>早期采用的数据集需要密集的人工标注，如ACE 2005和SemEval-2010 Task 8数据集。后来提出了远程监督的方法，其假设（两个实体如果在知识库中存在某种关系，则包含该两个实体的非结构化句子均能表示出这种关系）会导致大量的带噪声数据的产生，而后为了减少噪声，又加入多实例学习（如果一个实体对有某种关联，那么该实体对包中至少一个文档能表现出这种关联）。最常使用的远程监督数据集是New York Times corpus(NYT)。</p>
<p>深度学习采用的模型也发生了演变，从最初仅使用词嵌入的TextCNN，到引入Position Embedding，再到PCNN（按实体位置分割句子，CNN对分割后的词对作最大池化而不是对整个句子），引入多实例学习（将包含相同实体对的句子划分成一个包，通过置信度来对包中的句子进行选择分类），最后到句子级别的注意力机制（计算包中每条句子的权重，再将包中的句子根据权重进行加和计算，得到包级别的特征表示）。</p>
<p>按照运用的模型可以分为以下几类，按照通用领域模型改进——&gt;生物医学领域应用的方式组织：</p>
<p><strong><em>1. CNN  and PCNN , with attention:</em></strong></p>
<h3 id="Distant-Supervision-for-Relation-Extraction-via-Piecewise-Convolutional-Neural-Networks-2015"><a href="#Distant-Supervision-for-Relation-Extraction-via-Piecewise-Convolutional-Neural-Networks-2015" class="headerlink" title="Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks(2015)"></a>Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks(2015)</h3><p>A novel model based on PCNN(Piecewise Convolutional Neural Networks) with multi-instance learning to address two problems: one is the wrong heuristic alignment results caused by distant supervision, the other is feature enginerring noise.</p>
<p>自动化所的文章，创新点：PCNN提出分段进行max pooling，从而考虑两个Entity之间的结构特征：</p>
<img src="/2019/01/08/BioRE文献阅读报告/4.png" title="The architecture of PCNNs">
<p>加入multi-instance learning，用来解决远程监督引发的错误标签问题：</p>
<p>T bags {M1,M2,···, MT}，每个bag包含qi个实例，然后feed bags到神经网络学习，找到每个bag中最大的第j个实例，更新梯度，迭代收敛。</p>
<h3 id="Neural-Relation-Extraction-with-Selective-Attention-over-Instances-2016"><a href="#Neural-Relation-Extraction-with-Selective-Attention-over-Instances-2016" class="headerlink" title="Neural Relation Extraction with Selective Attention over Instances(2016)"></a>Neural Relation Extraction with Selective Attention over Instances(2016)</h3><p>A sentence-level attention-based model over multiple instances for relation extraction to alleviate distant supervision noise.</p>
<p>清华大学的文章，由于普通pcnn的multi-instance learning假设只要实体对的一个句子具有特定关系，那么该实体对也就具有该关系，并且选取置信度高的句子作为预测和训练，会丢失大量负例的有用信息。本文采用 Selective Attention over Instances，模型如下：</p>
<img src="/2019/01/08/BioRE文献阅读报告/6.png" title="The architecture of sentence-level attention-based CNN">
<p>该框架首先通过 CNN 编码后得到句子向量xi，再通过计算xi与实体关系s向量的相似度来赋予xi在整个句子集合中的权重。所采用的PCNN模型同上，将经过加权取和后的池化结果通过softmax输出label</p>
<p>GitHub源码地址：<a href="https://github.com/thunlp/OpenNRE" target="_blank" rel="noopener">https://github.com/thunlp/OpenNRE</a></p>
<h3 id="Distant-Supervision-for-Relation-Extraction-with-Sentence-Level-Attention-and-Entity-Descriptions-2017"><a href="#Distant-Supervision-for-Relation-Extraction-with-Sentence-Level-Attention-and-Entity-Descriptions-2017" class="headerlink" title="Distant Supervision for Relation Extraction with Sentence-Level Attention and Entity Descriptions(2017)"></a>Distant Supervision for Relation Extraction with Sentence-Level Attention and Entity Descriptions(2017)</h3><p>A sentence-level attention model to select the valid instances, which makes full use of the supervision information from knowledge bases(Freebase and Wikipedia)</p>
<p>自动化所的文章，是对2015年组内那篇PCNN的改进，同上篇清华大学的文章类似，也是基于句子级别的Attention模型，对分类正确或错误的句子选取不同的权重，找到尽可能多的对关系分类有帮助的句子，弥补多实例学习不能充分运用每条句子信息的不足。</p>
<img src="/2019/01/08/BioRE文献阅读报告/5.png" title="The architecture of APCNNs model">
<p>其中PCNNs模型依然是词嵌入（Word2Vec + Position Embeddings）- 卷积 - 分段池化几部分，句子级别的注意力模型则是后面的权重提取和softmax部分，计算带权特征值，预测每个bag的label。</p>
<p>其中 attention 权值的计算方法是：利用“实体-实体=关系”的方法表示实体间关系，同时利用卷积神经网络捕获实体描述页面特征，丰富实体表示，最后通过计算实体间关系与句子间的相似度赋予句子不同的权重。</p>
<h3 id="Relation-Classification-via-Multi-Level-Attention-CNNs-2016"><a href="#Relation-Classification-via-Multi-Level-Attention-CNNs-2016" class="headerlink" title="Relation Classification via Multi-Level Attention CNNs(2016)"></a>Relation Classification via Multi-Level Attention CNNs(2016)</h3><p>A CNN model relying on two levels of attention in order to better discern patterns in heterogeneous contexts.</p>
<p>具体分析参考这篇：<a href="http://www.shuang0420.com/2018/09/15/知识抽取-实体及关系抽取/" target="_blank" rel="noopener">Att-CNN</a></p>
<p><strong><em>2. LSTM with Attention:</em></strong></p>
<h3 id="A-Bi-LSTM-RNN-Model-for-Relation-Classification-Using-Low-Cost-Sequence-Features-2016"><a href="#A-Bi-LSTM-RNN-Model-for-Relation-Classification-Using-Low-Cost-Sequence-Features-2016" class="headerlink" title="A Bi-LSTM-RNN Model for Relation Classification Using Low-Cost Sequence Features(2016)"></a>A Bi-LSTM-RNN Model for Relation Classification Using Low-Cost Sequence Features(2016)</h3><p>A Bi-LSTM model based on low-cost sequence features to address relation classification, which using sentence segment to divide a sentence to two target entities and their three contexts, proving the context between two target entities can be used as an approximate replacement of the shortest dependency path when dependency parsing is not used.</p>
<p>武大计算机系的文章，双向LSTM模型，用五部分表征句子序列建模：两个目标实体和三个上下文</p>
<img src="/2019/01/08/BioRE文献阅读报告/10.png" title="The architecture of model">
<p>关系抽取时使用五种特征表征输入：pre-trained word features, random word features, character features, POS features and WordNet hypernym features</p>
<img src="/2019/01/08/BioRE文献阅读报告/11.png" title="Feature usage in Bi-LSTM-RNN">
<p>在SemEval-2010 Task 8和BioNLP-ST 2016 Task BB3做了测试。测试结果：</p>
<img src="/2019/01/08/BioRE文献阅读报告/12.png" title="Comparisons of neural network models in SemEval-2010">
<p>GitHub源码地址：<a href="https://github.com/foxlf823/semeval-2010-task-8" target="_blank" rel="noopener">https://github.com/foxlf823/semeval-2010-task-8</a></p>
<h3 id="Attention-Based-Bidirectional-Long-Short-Term-Memory-Networks-for-Relation-Classification（2016）"><a href="#Attention-Based-Bidirectional-Long-Short-Term-Memory-Networks-for-Relation-Classification（2016）" class="headerlink" title="Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification（2016）"></a>Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification（2016）</h3><p>自动化所的文章。前面已经有了CNN+Attn，其中CNN 可以处理文本较短的输入，但是长距离的依赖还是需要 LSTM，这一篇就是中规中矩的 BiLSTM+Attn 来做关系分类任务。</p>
<img src="/2019/01/08/BioRE文献阅读报告/17.png" title="Att-BLSTM model struction">
<h3 id="Semantic-Relation-Classification-via-Bidirectional-LSTM-Networks-with-Entity-aware-Attention-using-Latent-Entity-Typing-2019"><a href="#Semantic-Relation-Classification-via-Bidirectional-LSTM-Networks-with-Entity-aware-Attention-using-Latent-Entity-Typing-2019" class="headerlink" title="Semantic Relation Classification via Bidirectional LSTM Networks with Entity-aware Attention using Latent Entity Typing(2019)"></a>Semantic Relation Classification via Bidirectional LSTM Networks with Entity-aware Attention using Latent Entity Typing(2019)</h3><p>A novel end-to-end recurrent neural model which incorporates an entity-aware attention mechanism with a latent entity typing (LET) method.  State-of- the-art on the SemEval-2010 Task 8.</p>
<p>GitHub源码地址：<a href="https://github.com/roomylee/entity-aware-relation-classification" target="_blank" rel="noopener">https://github.com/roomylee/entity-aware-relation-classification</a></p>
<p><strong><em>3.  joint model:</em></strong></p>
<h3 id="Joint-Extraction-of-Entities-and-Relations-Based-on-a-Novel-Tagging-Scheme-2017"><a href="#Joint-Extraction-of-Entities-and-Relations-Based-on-a-Novel-Tagging-Scheme-2017" class="headerlink" title="Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme(2017)"></a>Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme(2017)</h3><p>A end-to-end Bi-LSTM encoding model based on a novel tagging scheme to extract entities and their relations simultaneously.</p>
<p>自动化所的文章。联合抽取方法使用一个模型同时实现实体抽取和关系抽取，能更好的整合实体及其关系之间的信息。但是现有的联合抽取方法大多都是基于特征来实现的，并且非常依赖其他可能会引入误差的NLP工具。为了减少人工抽取特征工作，提出基于神经网络的end-to-end模型来联合抽取实体和关系。本论文提出将一种新的标注方法和end-to-end的Bi-LSTM方法相结合的模型来解决联合抽取任务，将联合抽取问题转化为标注问题，这样就可以避免复杂的特征工程。</p>
<p>本文提出的标签类型主要由三部分组成：实体标签-关系标签-三元组标签，实体标签用经典的“BIES” (Begin, Inside, End, Single)指代实体，关系标签用有限个关系种类指代关系，三元组标签把相同关系标签的实体联系起来，1代表第一个实体，2代表第二个实体。</p>
<img src="/2019/01/08/BioRE文献阅读报告/7.png" title="Gold standard annotation for an example sentence based on our tagging scheme">
<p>采用的端到端模型如下：</p>
<img src="/2019/01/08/BioRE文献阅读报告/8.png" title="An illustration of our model">
<h3 id="A-neural-joint-model-for-entity-and-relation-extraction-from-biomedical-text-2017"><a href="#A-neural-joint-model-for-entity-and-relation-extraction-from-biomedical-text-2017" class="headerlink" title="A neural joint model for entity and relation extraction from biomedical text(2017)"></a>A neural joint model for entity and relation extraction from biomedical text(2017)</h3><p>A Bi-LSTM-RNN neural joint model to extract biomedical entities as well as their relations simultaneously.</p>
<p>武大计算机系的文章，是上面那篇A Bi-LSTM-RNN Model for Relation Classification Using Low-Cost Sequence Features 组内的改进，从pipeline models变到neural joint model，模型分为两部分，NER和RE：</p>
<img src="/2019/01/08/BioRE文献阅读报告/13.png" title="The Bi-LSTM-RNN for NER and RE">
<p>在drug and disease relations和bacteria and location relations上做了测试，发现结果精度只有略微提高。</p>
<p>GitHub源码地址（基于C++）：<a href="https://github.com/foxlf823/njmere" target="_blank" rel="noopener">https://github.com/foxlf823/njmere</a></p>
<h3 id="CoType-Joint-Extraction-of-Typed-Entities-and-Relations-with-Knowledge-Bases-2017"><a href="#CoType-Joint-Extraction-of-Typed-Entities-and-Relations-with-Knowledge-Bases-2017" class="headerlink" title="CoType: Joint Extraction of Typed Entities and Relations with Knowledge Bases.(2017)"></a>CoType: Joint Extraction of Typed Entities and Relations with Knowledge Bases.(2017)</h3><p>Joint extraction of typed entities and distant supervision. Including modeling automatically-labeled training corpus, joint entity and relation embedding, entity and relation inference.</p>
<p>GitHub源码地址：<a href="https://github.com/INK-USC/DS-RelationExtraction" target="_blank" rel="noopener">https://github.com/INK-USC/DS-RelationExtraction</a></p>
<h3 id="Attending-to-All-Mention-Pairs-for-Full-Abstract-Biological-Relation-Extraction-2017"><a href="#Attending-to-All-Mention-Pairs-for-Full-Abstract-Biological-Relation-Extraction-2017" class="headerlink" title="Attending to All Mention Pairs for Full Abstract Biological Relation Extraction(2017)"></a>Attending to All Mention Pairs for Full Abstract Biological Relation Extraction(2017)</h3><p>MIT的大神Patrick提出的bran，同时考虑所有的实体对并作出关系预测而不仅仅是单个句子内的；直接将全文用Transformer自注意编码器编码，以实现跨句子的实体关系抽取；用双向仿射变换操作来实现成对预测；joint model同时预测实体和关系。没有使用辅助的知识库，在Biocreative V CDR Dataset(chemical-disease)上实现了最高精度（不用特征工程，不加额外知识库）</p>
<img src="/2019/01/08/BioRE文献阅读报告/14.png">
<h3 id="Simultaneously-Self-Attending-to-All-Mentions-for-Full-Abstract-Biological-Relation-Extraction-2018"><a href="#Simultaneously-Self-Attending-to-All-Mentions-for-Full-Abstract-Biological-Relation-Extraction-2018" class="headerlink" title="Simultaneously Self-Attending to All Mentions for Full-Abstract Biological Relation Extraction(2018)"></a>Simultaneously Self-Attending to All Mentions for Full-Abstract Biological Relation Extraction(2018)</h3><p>A  bi-affine relation attention network that simultaneously scores all mention pairs within a document. Present a new CTD dataset constructed using strong-distant supervision.</p>
<p>对上面那篇文章的完善和补充，还在Biocreative VI CDR Dataset(chemical-protein)上做了测试，并创建了一个新的数据量很大的CTD Dataset(chemical-gene-disease)造福BioNRE领域orz…</p>
<p>GitHub源码地址：<a href="https://github.com/patverga/bran" target="_blank" rel="noopener">https://github.com/patverga/bran</a></p>
<p><strong><em>4. Others:</em></strong></p>
<h3 id="Bridging-semantics-and-syntax-with-graph-algorithms—state-of-the-art-of-extracting-biomedical-relations-2017"><a href="#Bridging-semantics-and-syntax-with-graph-algorithms—state-of-the-art-of-extracting-biomedical-relations-2017" class="headerlink" title="Bridging semantics and syntax with graph algorithms—state-of-the-art of extracting biomedical relations(2017)"></a>Bridging semantics and syntax with graph algorithms—state-of-the-art of extracting biomedical relations(2017)</h3><p>一篇用图模型提取生物医学关系的综述。</p>
<h3 id="Deep-Residual-Learning-for-Weakly-Supervised-Relation-Extraction-2017"><a href="#Deep-Residual-Learning-for-Weakly-Supervised-Relation-Extraction-2017" class="headerlink" title="Deep Residual Learning for Weakly-Supervised Relation Extraction(2017)"></a>Deep Residual Learning for Weakly-Supervised Relation Extraction(2017)</h3><p>用图像处理领域大火的深度残差网络ResNet做关系抽取。</p>
<p>GitHub源码地址：<a href="https://github.com/darrenyaoyao/ResCNN_RelationExtraction" target="_blank" rel="noopener">https://github.com/darrenyaoyao/ResCNN_RelationExtraction</a></p>
<h3 id="Weakly-Supervised-Neural-Text-Classification-2018"><a href="#Weakly-Supervised-Neural-Text-Classification-2018" class="headerlink" title="Weakly-Supervised Neural Text Classification(2018)"></a>Weakly-Supervised Neural Text Classification(2018)</h3><p>A weakly-supervised method that addresses the lack of training data in neural text classification.<br>Including a pseudo-document generator, and a bootstraped self-training module.</p>
<p>GitHub源码地址：<a href="https://github.com/yumeng5/WeSTClass" target="_blank" rel="noopener">https://github.com/yumeng5/WeSTClass</a></p>
<h3 id="Open-Information-Extraction-with-Meta-pattern-Discovery-in-Biomedical-Literature-2018"><a href="#Open-Information-Extraction-with-Meta-pattern-Discovery-in-Biomedical-Literature-2018" class="headerlink" title="Open Information Extraction with Meta-pattern Discovery in Biomedical Literature(2018)"></a>Open Information Extraction with Meta-pattern Discovery in Biomedical Literature(2018)</h3><p>A novel paradigm to automatically extract structured information from unstructured text with no or little supervision. Using Clause+Pattern-guided information extraction system.</p>
<p><strong><em>5. Supplement:</em></strong></p>
<h3 id="Painless-Relation-Extraction-with-Kindred-2017"><a href="#Painless-Relation-Extraction-with-Kindred-2017" class="headerlink" title="Painless Relation Extraction with Kindred(2017)"></a>Painless Relation Extraction with Kindred(2017)</h3><p>一个用于生物医学关系抽取的Python工具包，集成了PubAnnotation, PubTator, and BioNLP Shared Task data</p>
<p>GitHub源码地址：<a href="https://github.com/jakelever/kindred" target="_blank" rel="noopener">https://github.com/jakelever/kindred</a></p>
]]></content>
  </entry>
  <entry>
    <title>BioNER文献阅读报告</title>
    <url>/2019/01/07/BioNER%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/</url>
    <content><![CDATA[<p>过了这么久调研，大项目终于启动了。从NER开始做起，读了两月文献（才怪）这篇文章是对生物医学领域命名实体识别系统文章的模型综述和总结，涵盖CNN,biLSTM+CRF,多任务模型，迁移学习等知识。正文如下：<br> <a id="more"></a></p>
<h3 id="A-convolution-neural-network-multi-task-learning-model"><a href="#A-convolution-neural-network-multi-task-learning-model" class="headerlink" title="A convolution neural network multi-task learning model"></a>A convolution neural network multi-task learning model</h3><ol>
<li><p>Purpose: use task-specific, manually-annotated datasets together to improve NER performance.</p>
</li>
<li><p>Corpora: 15 biomedical datasets containing multiple named entities including Anatomy, Chemical, Disease, Gene/Protein and Species. Plus one dataset on biomedical POS tagging.</p>
<img src="/2019/01/07/BioNER文献阅读报告/1.png" title="The datasets and details of their annotations">
</li>
<li><p><a href="https://github.com/cambridgeltl/MTL-Bioinformatics-2016" target="_blank" rel="noopener">Models</a>: </p>
<ul>
<li><p>Baseline model:</p>
<p>A feed-forward neural network with a hidden Rectified Linear Unit (ReLU) activation layer leading to an output layer with Softmax activation.</p>
</li>
<li><p>Single task model:</p>
<ul>
<li><p>Input layer:2n + 1 words</p>
</li>
<li><p>Convolution layer: multiple filter sizes, ReLU activation</p>
</li>
<li>No max-pooling after the convolution layer to remain the positional information.</li>
<li>Fully connected layer</li>
<li>Softmax layer</li>
</ul>
</li>
<li><p>Multi-output multi-task model: </p>
<p>A private output layer with Softmax activation represents each task but all tasks share the rest of the model.</p>
</li>
<li><p>Dependent multi-task model: </p>
<p>Concatenate the fully connected layers of the model trained for the auxiliary task(such as POS tagging) and the one trained for the main task(NER in our example).</p>
<img src="/2019/01/07/BioNER文献阅读报告/2.png" title="Architecture of single-task model, multi-output multi-task model, multi-task dependent model">
</li>
</ul>
</li>
</ol>
<h3 id="A-cross-type-multi-task-learning-model-based-on-LM-LSTM-CRF"><a href="#A-cross-type-multi-task-learning-model-based-on-LM-LSTM-CRF" class="headerlink" title="A cross-type multi-task learning model(based on LM-LSTM-CRF)"></a>A cross-type multi-task learning model(based on <a href="https://github.com/LiyuanLucasLiu/LM-LSTM-CRF" target="_blank" rel="noopener">LM-LSTM-CRF</a>)</h3><ol>
<li><a href="https://github.com/yuzhimanhua/Multi-BioNER" target="_blank" rel="noopener">Models</a>:<ul>
<li>Single task model<img src="/2019/01/07/BioNER文献阅读报告/4.png" title="Architecture of a single-task neural network"></li>
<li>Three multi-task models:<img src="/2019/01/07/BioNER文献阅读报告/5.png" title="Three multi-task learning neural network models">
(a) MTM-C: multi-task learning neural network with a shared character layer and a task-specific word layer, (b): MTM-W: multi-task learning neural network with a task-specific character layer and a shared word layer, (c) MTM-CW: multi-task learning neural network with shared character and word layers.</li>
</ul>
</li>
<li>Datasets: 5 datasets based on Crichton et al.(the first article)<img src="/2019/01/07/BioNER文献阅读报告/6.png" title="Biomedical NER datasets used in the experiments"></li>
<li>Results:<img src="/2019/01/07/BioNER文献阅读报告/7.png" title="Performance of the proposed MTM-CW model">
</li>
</ol>
<h3 id="A-collaboration-of-deep-neural-networks-model"><a href="#A-collaboration-of-deep-neural-networks-model" class="headerlink" title="A collaboration of deep neural networks model"></a>A collaboration of deep neural networks model</h3><p>In CollaboNet, models trained on a different dataset are connected to each other so that a target model obtains information from other collaborator models to reduce false positives. Every model is an expert on their target entity type and takes turns serving as a target and a collaborator model during training time.</p>
<ol>
<li><p><a href="https://github.com/wonjininfo/CollaboNet" target="_blank" rel="noopener">Models</a>:</p>
<img src="/2019/01/07/BioNER文献阅读报告/8.png" title="Single-task model structure: Character level word embedding using CNN and an overview of Bidirectional LSTM with Conditional Random Field (BiLSTM-CRF)">
<img src="/2019/01/07/BioNER文献阅读报告/9.png" title="Structure of CollaboNet: Arrows show the flow of information when target model M_Target is training. The models in CollaboNet take turns in being the target model">
</li>
<li><p>Datasets: 5 datasets(BC2GM, BC4CHEMD, BC5CDR, JNLPBA, NCBI) based on Crichton et al.(the first article)</p>
</li>
<li><p>Results:</p>
<img src="/2019/01/07/BioNER文献阅读报告/10.png" title="Performances of single-task models comparing with others">
<img src="/2019/01/07/BioNER文献阅读报告/11.png" title="Performance of CollaboNet and the Multi-Task Model by Wang et al.">
</li>
</ol>
<h3 id="A-transfer-learning-model-based-on-NeuroNER-with-silver-standard-corpora-SSCs"><a href="#A-transfer-learning-model-based-on-NeuroNER-with-silver-standard-corpora-SSCs" class="headerlink" title="A transfer learning model(based on NeuroNER with silver-standard corpora(SSCs))"></a>A transfer learning model(based on <a href="https://github.com/ Franck-Dernoncourt/NeuroNER/" target="_blank" rel="noopener">NeuroNER</a> with silver-standard corpora(SSCs))</h3><ol>
<li><p><a href="https://github.com/BaderLab/Transfer-Learning-BNER-Bioinformatics-2018" target="_blank" rel="noopener">Corpus</a>:</p>
<ul>
<li><p>Gold standard corpora: 23 datasets on four entity types: chemicals, diseases, species and genes/proteins</p>
<img src="/2019/01/07/BioNER文献阅读报告/14.png" title="Gold standard corpora (GSCs) used in this work">
</li>
<li><p>Silver standard corpora: 50 000 abstracts (from a total of 174 999) at random from the CALBC-SSC-III-Small corpus for each of the four entity types it annotates: chemicals and drugs (CHED), diseases (DISO), living beings (LIVB) and genes/proteins (PRGE).</p>
</li>
</ul>
</li>
<li><p>Innovation: It’s like an <strong>instance-based transfer learning</strong> method instead of <strong>parameter-transfer learning</strong> method used in multi-task learning model used in Crichton et al.(the first article) or <strong>feature-representation transfer learning</strong>.</p>
</li>
</ol>
<h3 id="A-Neural-Multi-Task-Learning-Framework-to-Jointly-Model-Medical-Named-Entity-Recognition-and-Normalization"><a href="#A-Neural-Multi-Task-Learning-Framework-to-Jointly-Model-Medical-Named-Entity-Recognition-and-Normalization" class="headerlink" title="A Neural Multi-Task Learning Framework to Jointly Model Medical Named Entity Recognition and Normalization"></a>A Neural Multi-Task Learning Framework to Jointly Model Medical Named Entity Recognition and Normalization</h3><p>Medical named entity normalization (MEN) is to map obtained medical named entities into a controlled vocabulary. It is usually considered as a follow-up task of MER. In this paper, we consider MEN and MER as parallel tasks instead of hierarchical tasks.</p>
<ol>
<li><p><a href="https://github.com/SendongZhao/Multi-Task-Learning-for-MER-and-MEN" target="_blank" rel="noopener">Model</a>:</p>
<img src="/2019/01/07/BioNER文献阅读报告/13.png" title="The main architecture of our neural multi-task learning model with two explicit feedback strategies for MER and MEN">
<ul>
<li>CNN for character-level representation</li>
<li>Sequence-labeling with Bi-LSTM</li>
<li>Multi-task mode with explicit feedback strategies</li>
</ul>
</li>
<li><p>Corpus: BC5CDR task corpus  and the NCBI Disease corpus. To map disease mentions to MeSH/OMIM concepts (IDs), we use the Comparative Toxicogenomics Database (CTD) MEDIC disease vocabulary.</p>
</li>
<li><p>Error analysis: Out-of-Vocabulary words (OOV) performance  on Bi-LSTM-CNNs-CRF and feedback added.</p>
</li>
</ol>
<h3 id="An-attention-based-BiLSTM-CRF-approach-to-document-level-chemical-named-entity-recognition"><a href="#An-attention-based-BiLSTM-CRF-approach-to-document-level-chemical-named-entity-recognition" class="headerlink" title="An attention-based BiLSTM-CRF approach to document-level chemical named entity recognition"></a>An attention-based BiLSTM-CRF approach to document-level chemical named entity recognition</h3><ol>
<li><p><a href="https://github.com/lingluodlut/Att-ChemdNER" target="_blank" rel="noopener">Models</a>：</p>
<p>Att-BiLSTM-CRF to document-level chemical NER model for enforcing tagging consistency </p>
<img src="/2019/01/07/BioNER文献阅读报告/12.png" title="The basic sentence-level BiLSTM-CRF model and our document-level Att-BiLSTM-CRF model">
<ul>
<li>Features: Word embedding, Character embedding and Additional features(POS and chunking generated by the GENIA tagger)</li>
<li>BiLSTM-CRF model</li>
<li>Att-BiLSTM-CRF model: Introduced an attention matrix A to calculate the similarity between the current target word and all words in the document, not only in one sentence. The each value of A is derived by calculating  similarty of the words.</li>
</ul>
</li>
<li><p>Results: </p>
<p>State-of-the-art result in BioCreative IV chemical compound and drug name recognition (CHEMDNER) corpus and the BioCreative V chemical-disease relation (CDR) task corpus(the F-scores of 91.14 and 92.57%, respectively) </p>
</li>
</ol>
<p>我们的思路暂时是：字符级别的CNN+词级别的BiLSTM+CRF，用协同多任务学习来提高精度，效果不好用银标准数据迁移学习，因为是工程项目，预处理和数据主导，模型新颖性要求不太高，有成型的模型最好，所以暂时不考虑attention</p>
]]></content>
      <tags>
        <tag>NER</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP与ML的关键技术综述</title>
    <url>/2018/10/22/NLP%E4%B8%8EML%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0/</url>
    <content><![CDATA[<p>好久没写了，果然flag IS flag。最近在系统调研和了解技术，所以这篇文章综述了知识库构建过程中需要用到的关键技术，主要是NLP和统计机器学习方面的，参考资料先列在这里：</p>
<ul>
<li>吴军《数学之美》</li>
<li>李航《统计学习方法》</li>
<li>Natural Language Processing course by Michael Collins</li>
<li>CS224n(Natural Language Processing with Deep Learning)</li>
<li>其他相关博客<a id="more"></a>
</li>
</ul>
<h3 id="语言模型-Language-Modeling-的演化及相关算法"><a href="#语言模型-Language-Modeling-的演化及相关算法" class="headerlink" title="语言模型(Language Modeling)的演化及相关算法"></a>语言模型(Language Modeling)的演化及相关算法</h3><p>统计语言模型是自然语言处理的基础，用来描述词、语句乃至于整个文档的概率分布模型。<br>单个句子出现的概率$P(S) = P(w<em>1,w_2,…,w_n)$，其中$w_i$表示单个词。用条件概率展开为$P(w_1,w_2,…,w_n) = P(w_1)·P(w_2|w_1)·P(w_3|w_1,w_2)···P(w_n|w_1,w_2,…,w</em>{n-1})$。主要模型的提出和演化过程如图：<br><img src="/2018/10/22/NLP与ML的关键技术综述/1.png" title="基于Language Model的NLP主要模型演化"></p>
<h4 id="词袋模型Bag-of-Word-BOW-及相关算法"><a href="#词袋模型Bag-of-Word-BOW-及相关算法" class="headerlink" title="词袋模型Bag of Word(BOW)及相关算法"></a>词袋模型Bag of Word(BOW)及相关算法</h4><p>BOW认为一篇文档是由一组词构成的一个集合，词与词之间没有顺序以及先后的关系，词向量与原来文本中单词出现的顺序没有关系，而是词典中每个单词在文本中出现的频率。一篇文档可以包含多个主题，文档中每一个词都由其中的一个主题生成。</p>
<h5 id="TF-IDF（词频-逆文档矩阵）统计词频"><a href="#TF-IDF（词频-逆文档矩阵）统计词频" class="headerlink" title="TF-IDF（词频-逆文档矩阵）统计词频"></a>TF-IDF（词频-逆文档矩阵）统计词频</h5><p>TF-IDF可以反映字词于一个文件集而非单个文档的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在文件集中出现的频率成反比下降。<br><img src="/2018/10/22/NLP与ML的关键技术综述/2.png" title="TF-IDF计算公式"></p>
<h5 id="LSA-潜在语义分析-构建主题模型"><a href="#LSA-潜在语义分析-构建主题模型" class="headerlink" title="LSA(潜在语义分析)构建主题模型"></a>LSA(潜在语义分析)构建主题模型</h5><p>LSA假设如果两个词多次出现在同一文档中，则这两个词在语义上具有相似性。这种模型遍历语料库得到全部词的TF-IDF，然后以行表词，列表文件，构成词语-文档权重矩阵X，通过SVD（奇异值分解）降维化解，得到U（文档-主题矩阵）和V（词语-主题矩阵）。有了相似度则可以对文本和文档进行聚类，LSA的目标是从文本中发现文章隐含的主题或者概念。<br><img src="/2018/10/22/NLP与ML的关键技术综述/4.png" title="SVD分解词语-文档矩阵"></p>
<h5 id="PLSA（概率潜在语义分析）"><a href="#PLSA（概率潜在语义分析）" class="headerlink" title="PLSA（概率潜在语义分析）"></a>PLSA（概率潜在语义分析）</h5><p>PLSA引入概率模型代替SVD解决问题，其核心思想是找到一个潜在主题的概率模型，该模型可以生成我们在文档-术语矩阵中观察到的数据。如下图所示：<br><img src="/2018/10/22/NLP与ML的关键技术综述/3.jpg" title="PLSA模型"><br>我们有两种方式产生语言模型。<br>给定文档d，我们先根据p(z|d)从中选取d的一个主题z，再根据p(w|z)的概率产生一个单词w，即$ P(w,d) = P(d)\sum P(z|d)P(w|z) $<br>其中P(d)可直接由语料库确定，多项式分布P(z|d)和P(w|z)采用最大似然估计模型参数值，通过最大熵算法（EM）来迭代收敛。<br>给定主题z，我们用P(d|z)和P(w|z)单独生成文档，即$ P(w,d) = P(z)\sum P(d|z)P(w|z) $，这里产生了PLSA与LSA的对应关系：给定主题P(d|z) 的文档概率对应于文档-主题矩阵U，给定主题 P(w|z) 的单词概率对应于词语-主题矩阵V。</p>
<h5 id="LDA（隐含狄利克雷分布）"><a href="#LDA（隐含狄利克雷分布）" class="headerlink" title="LDA（隐含狄利克雷分布）"></a>LDA（隐含狄利克雷分布）</h5><p>LDA是PLSA的贝叶斯版本，它是对PLSA中的两个多项式分布P(c|d)和P(w|c)引入先验得到的，是一种无监督学习算法，在训练时不需要手工标注的训练集，需要的仅仅是文档集以及指定主题的数量。LDA为三层贝叶斯概率模型，包含词、主题（隐含）和文档三层结构，文档-主题分布和词语-主题分布分别是从两个狄利克雷分布α和β中抽样得到，由此生成文档。模型如下：<br><img src="/2018/10/22/NLP与ML的关键技术综述/5.png" title="LDA贝叶斯网络"><br>LDA比PLSA具有更好的泛化能力。在PLSA中，文档概率是数据集中的一个固定点，如果没有看到那个文件，我们就没有那个数据点。然而，在LDA中，数据集作为训练数据用于文档-主题分布的狄利克雷分布。即使没有看到某个文件，我们可以很容易地从狄利克雷分布中抽样得来，并继续接下来的操作。</p>
<h4 id="语言模型系列之N-Gram、NNLM及word2vec"><a href="#语言模型系列之N-Gram、NNLM及word2vec" class="headerlink" title="语言模型系列之N-Gram、NNLM及word2vec"></a>语言模型系列之N-Gram、NNLM及word2vec</h4><p>语言模型一般采用马尔可夫假设简化为N元模型，以三元模型(Trigram Model)为例，$P(S)=P(w<em>1)·P(w_2|w_1)·P(w_3|w_1,w_2)···P(w_i|w</em>{i-2},w<em>{i-1})···P(w_n|w</em>{n-2},w<em>{n-1})$，然后我们通过计算语料库中词组的频次比例，得到模型参数，如$P(w_n|w</em>{n-2},w<em>{n-1}) = \frac{c(w</em>{n-2},w<em>{n-1},w_n)}{c(w</em>{n-2},w_{n-1})}$。此外，为了消除样本不足时的零概率问题，引入插值法和折扣法。</p>
<h5 id="N-Gram"><a href="#N-Gram" class="headerlink" title="N-Gram"></a>N-Gram</h5><p>N-Gram的基本思想是将文本里面的内容按照字节进行大小为N的滑动窗口操作，形成长度是N的字节片段序列。N-Gram可与贝叶斯公式结合应用于分词和词性标注，特别是中文CN-WordSeg和CN-POS。</p>
<h5 id="NNLM（神经网络语言模型）"><a href="#NNLM（神经网络语言模型）" class="headerlink" title="NNLM（神经网络语言模型）"></a>NNLM（神经网络语言模型）</h5><p>Bengio提出的三层神经网络语言模型如图：<br><img src="/2018/10/22/NLP与ML的关键技术综述/6.png" title="神经网络语言模型"><br>首先介绍word embedding（单词嵌入）的概念：就是将文档单词序列的单一向量表示映射到低维连续密集向量空间表示。即从One hot representation到Distributed representation的转换。<br>神经网络(NN)的输入层为当前词wt的前n-1个词各自的词向量拼接，合计共(n−1)m维，隐藏层以tanh作为激活函数，输出层为当前词的softmax归一化概率。NN不仅学习Weight和bias，还还同时对输入层的词向量进行训练。方法是用梯度下降法最小化模型损失函数$ L = -\sum log(P(w<em>t|w</em>{t-n+1},w<em>{t-n+2}…w</em>{t-1})) $</p>
<h5 id="CBOW-amp-Skip-gram"><a href="#CBOW-amp-Skip-gram" class="headerlink" title="CBOW &amp; Skip-gram"></a>CBOW &amp; Skip-gram</h5><p>CBOW和Skip-gram去掉了NNLM的隐藏层，而着重突出输入层one-hot到word embedding这一步，即word2vec（word embedding的一种）。先给出区别：<br>如果是拿一个词语的上下文作为输入，来预测这个词语本身，则是CBOW模型。其优化的目标函数为$ L = \sum log(P(w|Context(w)))$，如图：<br><img src="/2018/10/22/NLP与ML的关键技术综述/8.jpg" title="多词上下文CBOW模型"><br>而如果是用一个词语作为输入，来预测它周围的上下文，那这个模型叫做Skip-gram模型，其优化的目标函数为$ L = \sum log(P(Context(w)|w)) $，如图：<br><img src="/2018/10/22/NLP与ML的关键技术综述/9.jpg" title="多词上下文Skip-gram模型"><br>此外，通过hierarchical softmax和negative sampling两种方法优化词向量，分别使用了Huffman编码和负采样算法。</p>
<h5 id="lda2vec：word2vec-和-LDA-的扩展"><a href="#lda2vec：word2vec-和-LDA-的扩展" class="headerlink" title="lda2vec：word2vec 和 LDA 的扩展"></a>lda2vec：word2vec 和 LDA 的扩展</h5><p>lda2vec 不直接用单词向量来预测上下文单词，而是使用上下文向量来进行预测。单词向量由前面的skip-gram模型生成。而文档向量是文档权重矩阵和主题矩阵的加权组合。lda2vec不仅能学习单词的词嵌入（和上下文向量嵌入），还同时学习主题表征和文档表征。如图：<br><img src="/2018/10/22/NLP与ML的关键技术综述/13.gif" title="lda2vec实例"></p>
<h4 id="RNN及后续，LSTM和Attention"><a href="#RNN及后续，LSTM和Attention" class="headerlink" title="RNN及后续，LSTM和Attention"></a>RNN及后续，LSTM和Attention</h4><p>NLP领域一个重大的突破是引入深度学习，通常用来进行结构化预测的自然语言任务，如词性标记、命名实体识别等。所以这里也综述一下神经网络的架构。</p>
<h5 id="全连接前馈神经网络（每个神经元只与前一层的神经元相连）"><a href="#全连接前馈神经网络（每个神经元只与前一层的神经元相连）" class="headerlink" title="全连接前馈神经网络（每个神经元只与前一层的神经元相连）"></a>全连接前馈神经网络（每个神经元只与前一层的神经元相连）</h5><p>如多层感知机MLP。MLP可以被看作是一个有向图，由多个的节点层所组成，每一层都全连接到下一层。除了输入节点，每个节点都是一个带有非线性激活函数的神经元，MLP采用反向传播算法训练。<br><img src="/2018/10/22/NLP与ML的关键技术综述/14.jpg" title="有一个隐藏层的多层感知器"></p>
<h5 id="具有卷积和池化层的前馈神经网络"><a href="#具有卷积和池化层的前馈神经网络" class="headerlink" title="具有卷积和池化层的前馈神经网络"></a>具有卷积和池化层的前馈神经网络</h5><p>如CNN，被设计来识别大型结构中的指示性局部预测因子，<br>以一个经典的LeNet-5网络为例，包含卷积层、池化层和最后的全连接层。卷积层的作用局部特征提取；池化层的作用是对原始特征信号进行抽象，大幅度减少训练参数，减轻模型过拟合程度。<br><img src="/2018/10/22/NLP与ML的关键技术综述/15.png" title="神经网络语言模型"><br>CNN 对诸如情感分析（Sentiment analysis）这样的分类NLP任务非常有效，因为带有显著情感极性的词组会对结果有比较关键的影响。</p>
<h5 id="循环神经网络（Recurrent-Neural-Networks）"><a href="#循环神经网络（Recurrent-Neural-Networks）" class="headerlink" title="循环神经网络（Recurrent Neural Networks）"></a>循环神经网络（Recurrent Neural Networks）</h5><p>RNN带有自循环结构，将状态在自身网络中循环传递，使得信息可以保存延续到下个时刻，因此可以接受更广泛的时间序列结构输入，流程如图：<br><img src="/2018/10/22/NLP与ML的关键技术综述/10.png" title="RNN 中的序列处理过程"></p>
<h5 id="LSTM和GRU"><a href="#LSTM和GRU" class="headerlink" title="LSTM和GRU"></a>LSTM和GRU</h5><p>由于RNN进行的是一级一级的传递，所以缺陷是很难应用与所需预测位置较远的相关信息，改进后的长短期记忆网络LSTM很好解决了这个问题。它通过左到右依次为遗忘门、传入门、输出门三类门结构来改变cell状态和输出值，如图：<br><img src="/2018/10/22/NLP与ML的关键技术综述/11.png" title="LSTM基本结构"><br>GRU把细胞状态和隐藏状态进行了合并，省去了细胞结构，只采用了重置门和更新门<br><img src="/2018/10/22/NLP与ML的关键技术综述/16.png" title="LSTM的变种GRU"></p>
<h5 id="双向RNN"><a href="#双向RNN" class="headerlink" title="双向RNN"></a>双向RNN</h5><p>另一个对RNN的改进是Bidirectional RNN，它隐藏层要保存两个值，A参与正向计算，A’参与反向计算，最终的输出值y取决于A和A’。其中方框A内的序列也可以替换为LSTM或CNN等<br><img src="/2018/10/22/NLP与ML的关键技术综述/17.png" title="双向RNN"></p>
<h5 id="递归神经网络（Recursive-Neural-Networks）"><a href="#递归神经网络（Recursive-Neural-Networks）" class="headerlink" title="递归神经网络（Recursive Neural Networks）"></a>递归神经网络（Recursive Neural Networks）</h5><p>用循环神经网络来建模的话就是假设句子后面的词的信息和前面的词有关，在神经网络结构上表现为后面的神经网络的隐藏层的输入是前面的神经网络的隐藏层的输出；而用递归神经网络建模的话，就是假设句子是一个树状结构，由几个部分(主语，谓语，宾语）组成，而每个部分又可以在分成几个小部分，即某一部分的信息由它的子树的信息组合而来，整句话的信息由组成这句话的几个部分组合而来。<br><img src="/2018/10/22/NLP与ML的关键技术综述/18.png" title="递归神经网络"></p>
<h5 id="序列变换模型（seq2seq-models）"><a href="#序列变换模型（seq2seq-models）" class="headerlink" title="序列变换模型（seq2seq models）"></a>序列变换模型（seq2seq models）</h5><p>基于注意力的模型相比基于RNN的模型需要训练的资源更少，如Attention机制。它的出发点是将注意力集中在部分显著或者是感兴趣的点上，对自然语言处理的语言模型P(w|Context(w))来说，在选择恰当而非全部的context并用它生成下一个状态。有点类似于卷积神经网络处理学习图像细节，如图：<br><img src="/2018/10/22/NLP与ML的关键技术综述/19.jpg" title="Attention based model"></p>
<h3 id="概率图模型"><a href="#概率图模型" class="headerlink" title="概率图模型"></a>概率图模型</h3><p>统计概率图体系如下：<br><img src="/2018/10/22/NLP与ML的关键技术综述/20.jpg" title="图模型"><br>在概率图模型中，数据由公式G=(Y,E)建模表示，Y表节点（随机变量），E表边（依赖关系）。可以看出，贝叶斯网络是有向的，适合为有单向依赖的数据建模；马尔可夫网络无向，适合实体之间互相依赖的建模。它们的核心差异表现在如何求P=(Y)，即怎么表示$ Y=（y<em>{1},\cdots,y</em>{n}）$这个联合概率。其中有向图采用条件概率乘积求概率，无向图采用分解然后求势函数乘积的方式求概率。<br>另外解释一下判别式模型和生成式模型的区别：<br>判别式模型学习数据的features然后预测label，是对P(Y|X)建模，典型是逻辑回归LR。而生成式模型先从训练样本中学习联合概率分布，之后给定样本X通过条件概率得到Y的分布：P(Y|X) = P(X,Y)/P(X) = P(X|Y)P(Y)/P(X)，典型是朴素贝叶斯NB。<br>主要介绍HMM和CRF，它们在解决序列标注问题上有很大用途，过程是：先预先建立模型，然后在给定的数据上训练模型，确定参数，最后用确定的模型做序列标注问题。</p>
<h4 id="隐马尔可夫模型HMM"><a href="#隐马尔可夫模型HMM" class="headerlink" title="隐马尔可夫模型HMM"></a>隐马尔可夫模型HMM</h4><p>有向图，典型生成式模型，解决编码和解码问题。其中Wi是可见状态，ti是隐含状态，隐含状态间有一个叫转移概率，隐含状态和可见状态之间有一个叫输出概率，如图：<br><img src="/2018/10/22/NLP与ML的关键技术综述/21.png" title="HMM"><br>$ P(t<em>1,t_2,…,w_1,w_2,…) = \prod P(t_i|t</em>{i-1})·P(w<em>i|t_i)$<br>而$ P(w_i|t_i) = \frac{P(w_i,t_i)}{P(t_i)} $可以通过语料库标注值计算，$P(t_i|t</em>{i-1}) = \frac{P(t<em>i,t</em>{i-1})}{t_{i-1}} $按照统计语言模型的训练方法得到。<br>HMM模型中存在两个假设：一是输出观察值之间严格独立，二是状态的转移过程中当前状态只与前一状态有关。<br>训练过程（模型参数确定）采用鲍姆-韦尔奇算法（类似于期望最大化算法），就是先初始化一套值，然后不断迭代估计新的模型参数，使输出概率最大化。<br>训练好模型参数后，给定模型和输出序列找到可能产生输出的状态序列，这就是序列标注问题：在新的观测序列上找出一条概率最大最可能的隐状态序列，采用viterbi算法解决。</p>
<h4 id="条件随机场CRF"><a href="#条件随机场CRF" class="headerlink" title="条件随机场CRF"></a>条件随机场CRF</h4><p>无向图。条件随机场是在给定的随机变量X的条件下，随机变量（隐含状态序列）的马尔可夫随机场，它解决了HMM输出独立性假设的问题。它需要预先定义特征函数</p>
<h4 id="LSTM-CRF"><a href="#LSTM-CRF" class="headerlink" title="LSTM+CRF"></a>LSTM+CRF</h4><p>这是我们将要在命名实体识别和关系抽取中用到的关键性算法。<br>它是LSTM和CRF的合体。其中LSTM是依靠神经网络的超强非线性拟合能力来为每个token预测一个label，而CRF的特征函数可以学习限定窗口下词的关系特征，LSTM+CRF的实质是让LSTM在CRF的特征限定下学习，得到新的非线性变化空间。</p>
]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>林轩田机器学习技法笔记（一）</title>
    <url>/2018/09/14/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<p>（nlp和dl要紧，ml的知识以后再看吧，未完待之后补充）</p>
<p>这几篇是林老师《机器学习基石》的进阶版《机器学习技法》的笔记，更深入地探讨机器学习一些高级算法和技巧。<br>照例先放上课程教学大纲：<br><a id="more"></a><br><img src="/2018/09/14/林轩田机器学习技法笔记（一）/1.png" title="Syllabus"></p>
<p>整个课程分为三个部分：<br><img src="/2018/09/14/林轩田机器学习技法笔记（一）/2.png" title="Three Major Techniques"><br>第一部分讲述如何规范和扩展更多的特征，这个引出了支持向量机；第二部分讲述如何将特征进行融合，这个引出了自适应推进算法；第三部分讲述如何辨别和学习隐藏的特征，这里引出了一些类神经网络的算法，比如深度学习。</p>
<p>这篇是第一部分的笔记，具体包含了线性支持向量机、对偶支持向量机、核型支持向量机、软边界支持向量机、核型逻辑回归、支持向量回归。笔记如下：</p>
<h4 id="Linear-Support-Vector-Machine"><a href="#Linear-Support-Vector-Machine" class="headerlink" title="Linear Support Vector Machine"></a>Linear Support Vector Machine</h4><p>解决的问题是对于PLA，如何使分类线的容忍误差程度最大，即 margin 最大<br><img src="/2018/09/14/林轩田机器学习技法笔记（一）/3.png" title="Large-Margin Separating Hyperplane"><br>这就转化成求点到直线的距离问题，<br><img src="/2018/09/14/林轩田机器学习技法笔记（一）/4.png"><br><img src="/2018/09/14/林轩田机器学习技法笔记（一）/5.png"><br><img src="/2018/09/14/林轩田机器学习技法笔记（一）/6.png" title="Final Condition and An Example"><br>可以看出求解线性 SVM 的过程是一个求解二次规划问题的过程。Support Vector Machine(SVM)这个名字的由来是：分类面仅仅由分类面的两边距离它最近的几个点决定的，其它点对分类面没有影响。决定分类面的几个点称之为支持向量(Support Vector)，好比这些点“支撑”着分类面。</p>
<p>SVM 与之前介绍的正则化regularization思想很类似。regularization的目标是$E<em>{in}$最小化，条件是$ w^Tw \leq C $;SVM的目标是$w^Tw$最小化，条件是$ y_n(w^Tx_n+b)\geq1 $，即保<br>证了$E</em>{in} = 0$。有趣的是，regularization与 SVM 的目标和限制条件分别对调了。</p>
<p>此外，由于 SVM 是 Large-Margin 的，会比 PLA 有更低的模型复杂度和更好的模型泛化能力。<br><img src="/2018/09/14/林轩田机器学习技法笔记（一）/7.png" title="Large-Margin Restricts Dichotomies"></p>
<h4 id="Dual-Support-Vector-Machine"><a href="#Dual-Support-Vector-Machine" class="headerlink" title="Dual Support Vector Machine"></a>Dual Support Vector Machine</h4><p>上节 Linear SVM 的目标是找出最“胖”的分割线进行正负类的分离，方法是使用二次规划来求出分类线。不过对于非线性分类问题，需要运用特征转换增加z域的维度d+1。当模型很复杂时，d很大，求解问题困难，所以我们想要使求解SVM不依赖d，这就是对偶支持向量机(Dual Support Vector Machine)。<br><img src="/2018/09/14/林轩田机器学习技法笔记（一）/8.png" title="SVM without d"></p>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>蓝凡《电影论》读书笔记</title>
    <url>/2018/08/30/%E8%93%9D%E5%87%A1%E3%80%8A%E7%94%B5%E5%BD%B1%E8%AE%BA%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>终于写了一篇实验室课题外的笔记。。。。</p>
<blockquote><p>蓝凡编著的《电影论：对电影学的总体思考（上下）》分为“电影论”和“影像类型论”上下两编。上编从哲学、美学、艺术学、社会学、产业和创意理论等方面，考察和探讨了电影这一因科技发明而诞生的艺术类型，并从电影的影像原理和影像特征上，探讨了电影的影像动作、影像时空、影像真实、影像身体、影像产业、影像权力、影像传播和后现代影像等问题；下编分章节介绍了电影类型基础以及股市片、纪录片、动画片、悲喜剧片、戏曲片、歌舞片、色情片等电影类别/类型与类型片。</p>
<footer><strong>豆瓣</strong><cite><a href="https://book.douban.com/subject/25783648/" target="_blank" rel="noopener">蓝凡《电影论》 内容简介</a></cite></footer></blockquote>
<a id="more"></a>
<h3 id="电影论（上）：电影的艺术、技术、商业与影像力量"><a href="#电影论（上）：电影的艺术、技术、商业与影像力量" class="headerlink" title="电影论（上）：电影的艺术、技术、商业与影像力量"></a>电影论（上）：电影的艺术、技术、商业与影像力量</h3><h4 id="导论：电影的世纪发现与辉煌"><a href="#导论：电影的世纪发现与辉煌" class="headerlink" title="导论：电影的世纪发现与辉煌"></a>导论：电影的世纪发现与辉煌</h4><h5 id="电影的发明与艺术的适应性"><a href="#电影的发明与艺术的适应性" class="headerlink" title="电影的发明与艺术的适应性"></a>电影的发明与艺术的适应性</h5><p>19世纪西方工业社会的确立，尤其是摄影的产生，对音乐、绘画、雕塑等领域的艺术形式造成了极大的冲击，促使其流派的转变，而电影作为摄影的衍生品，是随着机器文明而出现的，其艺术上和形态上的变动都与人类社会体制、经济和文化的变化丝丝相扣。</p>
<h5 id="作为大众媒介的电影"><a href="#作为大众媒介的电影" class="headerlink" title="作为大众媒介的电影"></a>作为大众媒介的电影</h5><p>电影从娱乐到传播媒介和艺术的身份转变，体现了工业社会向信息社会的转型；电影作为新的艺术形态的出现，体现了现代艺术对传统艺术的借鉴和影响，和传统艺术不断求变以期适应现代社会的需求。电影在本质上可以分为用影像动作叙事的艺术类型和用影像记录的记录类型，这体现了电影作为大众媒介和艺术的双重性。</p>
<h5 id="作为艺术的电影"><a href="#作为艺术的电影" class="headerlink" title="作为艺术的电影"></a>作为艺术的电影</h5><p>从哲学上讲，人维世界是存在世界、生存世界和精神世界的统一，从而使科学有了自然科学、社会科学和艺术科学的分化。世界是以人为主体的世界：存在是人的语言，空间是人的感觉，时间是人的记忆。艺术是为了人类感性精神的交流而创造的。艺术就是为了叙事，所以电影最根本的意义是叙事。电影艺术的本质，就是如何电影化地叙事，而在叙事方式上的突破正是电影区别于传统艺术的地方。“电影性”是在两个维度上三位一体性基础上的影像叙事性，即既是艺术、技术与商业——影像动作性、影像技术性与影像资本性纠缠在一起的影像“叙事性”，又是故事、记录与动画——故事片的表演影像、纪录片的实录影像与动画片的造型影像紧密关联的影像。</p>
<h4 id="原理论：电影叙事的特殊性"><a href="#原理论：电影叙事的特殊性" class="headerlink" title="原理论：电影叙事的特殊性"></a>原理论：电影叙事的特殊性</h4><h5 id="电影原理的一体性"><a href="#电影原理的一体性" class="headerlink" title="电影原理的一体性"></a>电影原理的一体性</h5><p>电影原理的规定性，也就是构成电影的基本要素与前提——材（媒材）、技（技艺）和事（叙事），但因其三位一体的电影性，同传统艺术相比具有特殊性。材的特殊性体现在：融合了技术、资本与艺术的光声色；技的特殊性体现在：影像叙事的发展同技术与资本的支持息息相关；事的特殊性体现在题材、表达和对象上的“无所不包”和“无所不能”。film、cinema、movie这三个词就是分别从技术性、艺术性和商业性三个方面描述电影的。</p>
<h5 id="光影魔术：电影的技术性原理"><a href="#光影魔术：电影的技术性原理" class="headerlink" title="光影魔术：电影的技术性原理"></a>光影魔术：电影的技术性原理</h5><p>光、声、色的电影化再现了叙事世界，影像的装配与合成（数字合成化）再造了叙事世界。</p>
<h5 id="电影化叙事：电影的艺术性原理"><a href="#电影化叙事：电影的艺术性原理" class="headerlink" title="电影化叙事：电影的艺术性原理"></a>电影化叙事：电影的艺术性原理</h5><ul>
<li>电影的艺术性原理：影像叙事的特殊性<ul>
<li>影像叙事材料上的特殊性：声光色储存和放映</li>
<li>影像叙事技艺上的特殊性：叙事方法的电影化，如镜头的景别、剪辑和运动，光影气氛、构图、造型、特技、音效等</li>
<li>影像叙事商业上的特殊性：资本对电影化叙事的控制</li>
</ul>
</li>
<li>电影化叙事的四大基本原则<ul>
<li>画面性原则：对景象的复制（但不等同）和展现</li>
<li>同态性原则：影像最真实地体现人类生活</li>
<li>选择性原则：影像对观众的“给予性”，通过剪辑和叙事蒙太奇实现</li>
<li>强制性原则：最终呈现的影片是资本、制片、编剧、导演、演员合力的产物，观众只能被动地观看影片的镜头和快慢动作</li>
</ul>
</li>
<li>电影化叙事的三大修饰词<ul>
<li>影像剪辑：电影叙事最基本的文法，即蒙太奇（montage），分为对比、平行、象征、交叉、主题反复五类，作用是组织情节、结构时空、安排节奏和塑造人物，体现叙事动作的节奏感和连贯性</li>
<li>影像特写：即魅力镜头，作用是：对银幕形象夸张的放大，使镜头极具表象化和深刻化；突出放大和集中聚焦的形象细部和动作细节，使人产生强烈的“通感”和“移感”；强化影像动作的诱导性，揭示内心的真实活动。</li>
<li>影像奇观：即魔力声光，极大地发挥了对时空和想象的叙事世界的创造力</li>
</ul>
</li>
<li>情感世界：情感体现是电影化叙事的意义所在<br>艺术的本质是为了叙事，叙事的目的是精神的交流——情感和理念的抒发。声音与影像的配合，加强了影片的叙事能力和情感指向。</li>
</ul>
<h5 id="商业美学：电影的资本性原理"><a href="#商业美学：电影的资本性原理" class="headerlink" title="商业美学：电影的资本性原理"></a>商业美学：电影的资本性原理</h5><p>商业对电影生产、电影明星和对电影技术的资本性控制，使得最终的电影呈现是政治与经济、意识形态与市场票房的辩证性统一。</p>
<h5 id="电影原理与电影观看"><a href="#电影原理与电影观看" class="headerlink" title="电影原理与电影观看"></a>电影原理与电影观看</h5><ul>
<li>观影的哲学根性是一种“假如性”的设定，是“旁观”和“代入感”统一的根性</li>
<li>期望心理（精神的慰藉，情感的投射）、观影习惯（孤独的自由，人性的思考）与观影行为（花钱的娱乐，感官的刺激），是电影观看的三重结构</li>
<li>影像既是无门槛的交流艺术语言，又具备国家、地区、民族和个人的差异性</li>
</ul>
<h4 id="动作论：电影性与戏剧性的比较研究"><a href="#动作论：电影性与戏剧性的比较研究" class="headerlink" title="动作论：电影性与戏剧性的比较研究"></a>动作论：电影性与戏剧性的比较研究</h4><h5 id="导论：戏剧性、电影性、动作性"><a href="#导论：戏剧性、电影性、动作性" class="headerlink" title="导论：戏剧性、电影性、动作性"></a>导论：戏剧性、电影性、动作性</h5><p>戏剧的本性和外部表现就是动作性，电影的本质也应该是动作性。戏剧是表演动作的艺术，电影是影像动作的艺术。镜头和画面是动作的单位，也是电影的最基本单位，镜头是摄影机在摄影过程中的一种处理方式，画面是后期制作剪辑过程中对胶片的一种处理方式。电影由每秒24格地放映静止画面构成，呈现在银幕上和观众眼中是活动的影像。</p>
<h5 id="构成：表演动作与影像动作"><a href="#构成：表演动作与影像动作" class="headerlink" title="构成：表演动作与影像动作"></a>构成：表演动作与影像动作</h5><ul>
<li>动作的构成：戏剧动作是“自性”在舞台的体现，而电影动作是“自性”（物象运动）与“他性”（镜头运动和画面运动）在银幕上的叠加与综合</li>
<li>动作的原则——统一性：一致性、完整性和合理性</li>
</ul>
<h5 id="动作的规定：动作的本质、逻辑与辩证"><a href="#动作的规定：动作的本质、逻辑与辩证" class="headerlink" title="动作的规定：动作的本质、逻辑与辩证"></a>动作的规定：动作的本质、逻辑与辩证</h5><ul>
<li>冲突是动作的本质，戏剧冲突是电影性和戏剧性的共性</li>
<li>时空是电影与戏剧的动作逻辑</li>
<li>视点是电影与戏剧的动作个性，包括客观视点（全知镜头）、主观视点（主观镜头）和自身观点</li>
</ul>
<h5 id="思维：表演-影像与动作-语言"><a href="#思维：表演-影像与动作-语言" class="headerlink" title="思维：表演/影像与动作/语言"></a>思维：表演/影像与动作/语言</h5><p>戏剧的动作性与电影的动作性的差异原因体现在动作构成元素、叙事形态、商业属性和观看方式。如果说动作是戏剧的逻辑起点，那么动中之动才是电影的逻辑起点。</p>
<h4 id="时空论：电影的时空哲学"><a href="#时空论：电影的时空哲学" class="headerlink" title="时空论：电影的时空哲学"></a>时空论：电影的时空哲学</h4><h5 id="时空密码：人类的时间与空间"><a href="#时空密码：人类的时间与空间" class="headerlink" title="时空密码：人类的时间与空间"></a>时空密码：人类的时间与空间</h5><p>时空的三个维度：科学时空（测定的时间与空间）、思想时空（心理、生理、哲学的时间与空间）、情感时空（艺术的时间与空间）</p>
<h5 id="影像时空叙事：电影时空的复杂表述"><a href="#影像时空叙事：电影时空的复杂表述" class="headerlink" title="影像时空叙事：电影时空的复杂表述"></a>影像时空叙事：电影时空的复杂表述</h5><p>电影时空的特殊性，是影像时空“同态性”（还原性）和“超验性”（无限可能性）的复杂、高度统一，电影剪辑、蒙太奇的使用，转换的是时间与空间。电影时空的逻辑是颠覆时空的逻辑，是创造人类常态不能认知的时间哲学的逻辑，而这实际上是为了电影叙事能力的提高，最终是为了情感的生成。</p>
<h5 id="德勒兹概念：运动影像与时间影像"><a href="#德勒兹概念：运动影像与时间影像" class="headerlink" title="德勒兹概念：运动影像与时间影像"></a>德勒兹概念：运动影像与时间影像</h5><ul>
<li>福柯曾戏称德勒兹的哲学为“德勒兹时代”，这是因为福柯的哲学是考古的哲学，从历史来看当代；德勒兹的哲学是“创造概念”的哲学，是从未来来看当代——人有别于其他生物，就是依靠寻找差异，生成概念，逃逸定见。对德勒兹来说，电影就是哲学，哲学就是电影，重要的不是我们知道什么，确定什么，而是我们相信什么，以及是否内心还心存信仰。现代电影或时间-影像的意义，在于超越了其他有思想的界限向我们打开了消除封闭“我思”的无限可能，重建我们对世界和人生的信仰。</li>
<li>德勒兹概念的核心是运动的“概念”，基础是物本身内在的“差异”，核心批判对象是“定见”，逻辑起点是“欲望”，主张是反俄狄浦斯的（反自我），推崇尼采的权力意志，终极目标是无拘无束的“千高原”。</li>
<li>电影的力量不仅在于运动的画面创造的空间世界，更在于生成的时间世界，这是一个有思想、有价值、有意义的“千高原”世界，所以在某种意义上说，时间的影像是人类进入“块茎”思维的产物，也是电影就是哲学的依据——电影是不断生成的概念世界。</li>
</ul>
<h5 id="虚拟的光影：电影时空的意义"><a href="#虚拟的光影：电影时空的意义" class="headerlink" title="虚拟的光影：电影时空的意义"></a>虚拟的光影：电影时空的意义</h5><ul>
<li>虚拟的光影创造了虚拟的时空，成为了人们“眼见为实”的世界</li>
<li>电影为人类赋予了情感时间（通过时间的拉长或者停滞表达内心情感）和政治空间（空间是社会关系演变的产物，表现出政治经济的痕迹）</li>
<li>电影中时间的记忆，通过影像的空间使我们“感觉”甚至“触摸”到了</li>
</ul>
<h4 id="真实论：蒙太奇与长镜头的历史辩证"><a href="#真实论：蒙太奇与长镜头的历史辩证" class="headerlink" title="真实论：蒙太奇与长镜头的历史辩证"></a>真实论：蒙太奇与长镜头的历史辩证</h4><h5 id="导论：真实之维——哲学的思考与艺术的思考"><a href="#导论：真实之维——哲学的思考与艺术的思考" class="headerlink" title="导论：真实之维——哲学的思考与艺术的思考"></a>导论：真实之维——哲学的思考与艺术的思考</h5><ul>
<li>真实，是人类在三个世界（存在、生存、精神）中的一种感官判断</li>
<li>生活真实，是一种状态的描述，是在存在世界和生存世界的感知与想象；艺术真实，是一种理想的选择，是在精神世界的艺术加工</li>
<li>影像真实是对电影真实的特殊描述与选择，电影是感官上感觉“真实”的幻觉</li>
<li>在反映影像真实上蒙太奇（镜头和画面剪辑）和长镜头（连续镜头拍摄）的实践与争辩</li>
</ul>
<h5 id="作为叙事文法的蒙太奇"><a href="#作为叙事文法的蒙太奇" class="headerlink" title="作为叙事文法的蒙太奇"></a>作为叙事文法的蒙太奇</h5><ul>
<li>对蒙太奇的理解和运用可以分为两个学派：以格里菲斯为代表的“欧美学派”和以艾森斯坦为代表的苏俄学派。前者将注意力放在无声电影的影像电影表现上（仅有声像素材的分解重组），后者将注意力放在影像动作组接的意义生成上（将一系列不同地点、不同距离和角度，以不同方法拍摄的镜头组接以产生新的含义——“库里肖夫实验”）。</li>
<li>1 + 1 = 2 是剪辑，1 + 1 &gt; 2 是蒙太奇。蒙太奇的意义在于其表达意图的风格和传输思想的方式上：通过两个镜头的撞击确立一个思想，把一系列思想贯注到观众的意识中，造成一种情感状态，并促成一种精神状态，使观众对人民打算传输给他的思想产生共鸣。“蒙太奇”的真实是认识论的“真实”。</li>
<li>随着艾森斯坦蒙太奇理论的推广，其作用被无限夸大，从认识化叙事的一种文法（修辞）上升到了认识论本身，其不仅出现在电影银幕上，甚至成为了意识形态的体现，出现在政治生活中。</li>
<li>蒙太奇的原理基础是人类思维的联想性，是对影像画面和动作的联想。</li>
</ul>
<h5 id="纪实的长镜头"><a href="#纪实的长镜头" class="headerlink" title="纪实的长镜头"></a>纪实的长镜头</h5><ul>
<li>作为一种电影理论或电影美学，长镜头指的是巴赞电影真实美学的形式概括和称谓，其主张采用长镜头和景深镜头结构影片，“透明”地展示现实，以体现电影的本体论（真实性）和现实主义。</li>
<li>长镜头只是方法论的范畴，不是认识论的范畴，即用“长镜头”理念和方法“制作”的影像，不是真实的影像。电影不是关于现实的，而是关于它自己的内部世界，生活的逻辑并不一定就是电影的逻辑。</li>
</ul>
<h5 id="影像的写实、真实与现实主义"><a href="#影像的写实、真实与现实主义" class="headerlink" title="影像的写实、真实与现实主义"></a>影像的写实、真实与现实主义</h5><ul>
<li>蒙太奇和长镜头理论都是为了追求电影叙事的“真实”。对蒙太奇来说，是如何通过镜头与镜头的组接来创造“真实”讲故事，从而“体现”作者和导演的思想态度和倾向；对长镜头来说，是如何通过镜头的“照相”本能来达到对象的“木乃伊”式的“再现”，从而在“真实”地反映现实中，将作者和导演的思想意“隐藏”在这种镜头的记录之中。两者的认识论和美学观是一致的，方法论是相异的。</li>
<li>蒙太奇是创造真实，是电影的“艺术真实”，逻辑是“变异”哲学，创造的背后是苏维埃新政权意识形态的需要；长镜头是再现真实，是电影的“存在真实”，逻辑是“透明”哲学，实录的背后是而二战后社会状况的意识形态需要。但蒙太奇理论和长镜头理论在哲学上是基本一致的：历史面貌都是以意识形态为先导的面貌，价值取向都是以革命为本的价值取向，都属于政治哲学，都是形式为内容服务的叙事手法。</li>
<li>现实主义只是一种艺术风格，不是直录现实，电影不是现实世界的镜子，而是现实世界的解释。相信影像，就是相信影像能创造真实；相信真实，就是相信影像能还原真实。</li>
<li>蒙太奇和长镜头都是人类精神交流的镜头性的体现而已，并无实质的理论区别。</li>
</ul>
<h4 id="身体论：电影的肉身与肉身性"><a href="#身体论：电影的肉身与肉身性" class="headerlink" title="身体论：电影的肉身与肉身性"></a>身体论：电影的肉身与肉身性</h4><h5 id="导论：身体作为意识"><a href="#导论：身体作为意识" class="headerlink" title="导论：身体作为意识"></a>导论：身体作为意识</h5><ul>
<li>人作为可能精神交流的物种，依赖于肉与灵的不可分离。而随着柏拉图哲学在西方哲学占据主导地位和基督教在西方宗教占统治地位，上帝与魔鬼，灵魂与肉体，美德与罪恶，精神的爱与肉体的爱都分立开来，身体作为阻碍人的灵魂和神相遇的屏障，是罪恶之源和堕落之根，要由心智严格控制。以笛卡尔、康德、黑格尔为代表的西方理性主义思想的出现，依然没有摆脱宗教和哲学的双重压迫。直到费尔巴哈和尼采对“一切从身体出发”，“我就是肉体，就是灵魂”的呼唤，人类对身体的认识才不再有贬低感和压抑感。</li>
<li>与思想和哲学对人的肉身的看法相异，西方的艺术总是与身体的描绘和表现有关，肉身总是作为有形实体来表征性与死亡。艺术领域与思想领域的这种悖论，正体现了人的精神交流是身体-意识的展开这一双向的过程。</li>
<li>电影影像身体的出现，寄居了感觉、意识、思想和感情，打开了观众的无意识之门，是对“本我”的替换，观众通过观看影像身体，进而对自身的身体加以确认，获得情感的感知和交流。</li>
</ul>
<h5 id="电影肉身的矛盾哲学"><a href="#电影肉身的矛盾哲学" class="headerlink" title="电影肉身的矛盾哲学"></a>电影肉身的矛盾哲学</h5><ul>
<li>电影身体的基本矛盾是一般特征（日常的身体特征）与特殊特征的矛盾（影像的身体特征），这是电影身体的外部矛盾；电影本身的同构（电影身体与正常身体的一致性）与异构（与日常身体相比，电影身体作为符号的所指的特定性）的矛盾，这是电影身体的内部矛盾，电影身体差异化的背后，隐含的是电影身体之间的情感表达的差异矛盾、电影身体背后隐藏的文化差异矛盾以及电影身体显示的权力争夺。电影身体的哲学，就是如何寻求这种差异以及破解差异。</li>
<li>电影身体的五个面向：窥探的身体，惊艳的身体，想象的身体，精神的身体，未来的身体。肉身性是电影的影像身体的特殊性，表达的是电影身体的艺术功能性：电影成为身体的媒介、实验、想象、思想、未来。</li>
<li>电影的肉身修辞学，在于电影自身的艺术特殊性，在于电影观看的心理特殊性，在于声光影色的感官“通感性”</li>
</ul>
<h5 id="电影身体的政治学"><a href="#电影身体的政治学" class="headerlink" title="电影身体的政治学"></a>电影身体的政治学</h5><ul>
<li>影像身体的看与被看，实际上是进行一种真实的社会身份和观看的影像身份的互相认同与确认。</li>
<li>电影肉身作为权力场，是人类暴力和政治的体现。</li>
<li>女性导演的视角——身体即战场（凝视与窥探的双重动力学）</li>
</ul>
<h4 id="后现代电影"><a href="#后现代电影" class="headerlink" title="后现代电影"></a>后现代电影</h4><h5 id="从现代到后现代"><a href="#从现代到后现代" class="headerlink" title="从现代到后现代"></a>从现代到后现代</h5><p>从现代派艺术，到波普艺术（先锋、前卫艺术），再到后现代艺术</p>
<h5 id="后现代电影版图"><a href="#后现代电影版图" class="headerlink" title="后现代电影版图"></a>后现代电影版图</h5><ul>
<li>现代主义电影：印象派电影、达达主义电影（抽象电影）、超现实主义电影和表现主义电影、“新浪潮”电影、新好莱坞电影</li>
<li>波普电影：先锋派电影、地下电影</li>
<li>后现代电影：政治电影、女权主义电影、性解放电影、反抗压迫电影、民族电影</li>
<li>常见后现代电影：《雌雄大盗》、《落水狗》、《ET》、《阿甘正传》、《低俗小说》、《天生杀人狂》、《七宗罪》、《侏罗纪公园》、《黑客帝国》（最典型代表）、《搏击俱乐部》、《穆赫兰道》、《杀死比尔》、《发条橙》、《迷墙》、《闪灵》、《猜火车》、《这个杀手不太冷》、《狗镇》《黑暗中的舞者》、《甲方乙方》、《一个都不能少》、《疯狂的石头》、《让子弹飞》、《阿飞正传》、《大话西游》、《重庆森林》、《东邪西毒》、《春光乍泄》、《少林足球》《功夫》《情书》、《燕尾蝶》、《菊次郎的夏天》</li>
<li>后现代电影导演：史蒂文·斯皮尔伯格、大卫·林奇、昆汀·瓦伦蒂诺、大卫·芬奇、伍迪艾伦、拉斯·冯·提尔、斯坦利·库布里克、姜文、冯小刚、王家卫、周星驰、岩井俊二</li>
</ul>
<h5 id="后现代电影特征与手法"><a href="#后现代电影特征与手法" class="headerlink" title="后现代电影特征与手法"></a>后现代电影特征与手法</h5><ul>
<li>七大特征：无题材限制，突破传统观念，试一切手法，贴流行原则，非艺术效果，泛风格追求，零预设对象</li>
<li>挪用混搭：波普风格的拼贴混置（代表：昆汀·瓦伦蒂诺）</li>
<li>闹戏噱头：无厘头想象的娱乐游戏（代表：周星驰）</li>
<li>高新玩法：影像奇观的另类表达（代表：《黑客帝国》）</li>
<li>飙时挪移：电影叙事的时间魔方（代表：低俗小说）</li>
</ul>
<h5 id="例证：后现代电影的叙事转身"><a href="#例证：后现代电影的叙事转身" class="headerlink" title="例证：后现代电影的叙事转身"></a>例证：后现代电影的叙事转身</h5><p>后现代电影的叙事转身，是一种在不确定中的解构——宇宙、世界、社会，甚至制度、契约、规章，以及人生的意义，都成为了“薛定谔的猫”，充满了不确定性。</p>
<ul>
<li>解构生存：作为人的生存困境（代表：《黑客帝国》、《银翼杀手》）</li>
<li>解构英雄：人与社会英雄（颠覆传统英雄主义的价值观）（代表：《阿甘正传》、《第八日》）</li>
<li>解构两性：性的情与欲（代表：《罗拉快跑》、《这个杀手不太冷》《真爱至上》）</li>
<li>解构社会：社会秩序的结构性批判（代表：《低俗小说》、《发条橙》、《猜火车》、《教义》、《落水狗》等）</li>
</ul>
<h5 id="后现代电影的现象级文化：色情、暴力、黑色与流行、大众、消费"><a href="#后现代电影的现象级文化：色情、暴力、黑色与流行、大众、消费" class="headerlink" title="后现代电影的现象级文化：色情、暴力、黑色与流行、大众、消费"></a>后现代电影的现象级文化：色情、暴力、黑色与流行、大众、消费</h5><ul>
<li>后现代电影具有波普化（大众化、反精英意识、时尚消费意识），量子化（反常规时空、反常规逻辑），酷儿化（性、同性、女性），游戏化（观念、传播与大众流行），无叙化（反逻辑、反常规的故事）的特点</li>
<li>后现代主义是一种非常复杂的社会历史文化现象，它的复杂性体现在不确定性、无边界性和通俗性：不确定是指它本身作为集社会、历史、文化和思想心态的因素于一身的复杂事物固有的性质；无边界性是指描述的对象无边界，形式的表现无边界，题材的选择和技巧手法的应用无限开放；通俗性是指通过各种时尚、流行、通俗元素的混搭而作为艺术电影与生俱来的娱乐性和大众性。</li>
</ul>
]]></content>
      <tags>
        <tag>读书笔记</tag>
        <tag>电影</tag>
      </tags>
  </entry>
  <entry>
    <title>林轩田机器学习基石笔记（三）</title>
    <url>/2018/08/28/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89/</url>
    <content><![CDATA[<p>这篇笔记是林老师机器学习课程最后一部分的内容：How Can Machine Learn Better?<br>主要介绍一些机器学习的技巧，如正则化解决过拟合等。详细内容如下：<br><a id="more"></a></p>
<h4 id="Hazard-of-Overfitting"><a href="#Hazard-of-Overfitting" class="headerlink" title="Hazard of Overfitting"></a>Hazard of Overfitting</h4><p>目标多项式的模型复杂度增加会产生过拟合的问题，即$E<em>{in}$很小但$E</em>{out}$很大，泛化能力差。<br>如果数据量 N 很大的时候，低阶模型和高阶模型对应的 $E<em>{in}$ 和 $E</em>{out}$ 都比较接近，但是对于高阶模型，z域中的特征很多的时候，需要的样本数量N很大，容易发生维度灾难。<br><img src="/2018/08/28/林轩田机器学习基石笔记（三）/1.png" title="Curse of Dimensionality"><br>一般来说，VC Dimension、noise、N这三个因素是影响过拟合现象的关键。其中模型复杂度越高（确定性噪声越大），随机噪声强度越大，数据量越小，还有当目标多项式阶数较低但拟合模型越为复杂，越容易发生过拟合。</p>
<p>几个处理过拟合的方法：数据清理/裁剪(data cleaning/pruning)，数据提示(data hinting)，正则化(regularization)和加入验证（validation）</p>
<h4 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h4><p>正则化的思路是将高阶的 hypothesis sets 转换为低阶的 hypothesis sets。也就是说，对于高阶的hypothesis，为了防止过拟合，我们可以将其高阶部分的权重 w 设置一个宽泛的限制条件，例如这里使用的是L2 Regularizer，使所有的权重 w 的平方和的大小不超过一个值。这样变成了最小化该函数：</p>
<script type="math/tex; mode=display">E_{aug}(w) = E_{in}(w)+\frac{\lambda}{N} * w^Tw</script><p>其实可以把$\lambda$看成是一种对模型复杂度的penality，$\lambda$越小，惩罚越小，越容易过拟合，$\lambda$越大，拟合曲线就越平滑，越容易欠拟合。</p>
<h4 id="Validation"><a href="#Validation" class="headerlink" title="Validation"></a>Validation</h4><p>解决的问题是从 M 个不同的 hypothesis sets 中选择一个模型。方法是从训练数据中拿出一部分作为验证集，整个流程如图：<br><img src="/2018/08/28/林轩田机器学习基石笔记（三）/2.jpg" title="Model Selection by Best Eval"><br>两种交叉验证方法：k 折交叉验证和留一交叉验证，常用的是 k 折交叉验证。<br>留一交叉验证是指假设有 N 个样本，将每一个样本作为测试样本，其它 N-1 个样本作为训练样本。这样得到 N 个分类器，N 个测试结果。用这 N 个结果的平均值来衡量模型的性能。普通交叉验证是指将所有样本分成 k 份，一般每份样本的数量相等或相差不多。取一份作为测试样本，剩余 k-1 份作为训练样本。这个过程重复 k 次，最后的平均测试结果可以衡量模型的性能。</p>
<h4 id="Three-Learning-Principles"><a href="#Three-Learning-Principles" class="headerlink" title="Three Learning Principles"></a>Three Learning Principles</h4><p>最后介绍了机器学习中的三个“锦囊妙计”。</p>
<ul>
<li>奥卡姆剃刀定律(Occam’s Razor: Entities must not be multiplied unnecessarily)：选取简单的模型。其一方面是指简单的 h，例如多项式阶数比较少，另一方面指的是模型 H 包含的 hypothesis 数目有限，不会太多。</li>
<li>抽样偏差(Sampling Bias)：训练数据和验证数据要服从同一个分布，最好都是独立同分布<br>的，这样训练得到的模型才能更好地具有代表性。</li>
<li>数据窥探(data-snooping)：在机器学习过程中，避免“偷窥数据”非常重要。在使用训练数据的任何过程间接地偷看到数据，在模型的选择或者决策中就会增加模型复杂度。</li>
</ul>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>林轩田机器学习基石笔记（二）</title>
    <url>/2018/08/23/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[<p>这篇笔记是林老师机器学习课程第三部分的内容：How Can Machine Learn?<br>主要涵盖三个线性分类模型：linear classification、linear regression、logistic regression，和一个非线性变换 nonlinear transformation。详细内容如下：<br><a id="more"></a></p>
<h4 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h4><p>解决曲线拟合问题。设输入 X 为 d 维，加上常数项维度为 d+1，与权重w的线性组合即为 Hypothesis，记为h(x)，即 $h(x) = w^T * X$，误差为均方误差。</p>
<p>最小化$ E<em>{in}(w) = \frac{1}{N} * \sum</em>{n=1}^N (w^Tx<em>n-y_n)^2$，利用线性最小二乘法的求解方法（矩阵求解或者迭代法随机梯度下降）求解得到权重向量 $ w = (X^T X)^{-1} X^TY $，一般情况下，只要满足样本数量N远大于样本特征维度d+1，矩阵就是非奇异的（可逆的）。然后证明了同 linear classification 类似，当N足够大的时候，抽样样本的误差值 $E</em>{in}$和总体样本的误差值$E_{out}$都收敛于平均噪声，满足VC Dimension，可以机器学习。<br></p>
<h4 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h4><p>二分类问题。使用将特征加权和 $ s = \sum<em>{i=0}^dw_ix_i $限制在 [0,1] 间的 Sigmoid Function  $\theta(s) = \frac{1}{1+e^{-s}}$，带入得到 hypothesis 为 $h(x) = \theta(w^Tx) = \frac{1}{1+e^{-w^Tx}}$。<br>其误差函数用cross­-entropy error表示，最小化$ E</em>{in}(w) = \frac{1}{N} * \sum<em>{n=1}^N ln(1+exp(-y_nw^Tx_n)) $，最后还是通过梯度下降法计算$ \nabla E</em>{in}(w) \approx 0$时的对应的 w 值。<br>交叉熵的介绍见这篇文章：<a href="https://blog.csdn.net/tsyccnh/article/details/79163834" target="_blank" rel="noopener">关于交叉熵在loss函数中使用的理解</a></p>
<h4 id="Linear-Models-for-Classification"><a href="#Linear-Models-for-Classification" class="headerlink" title="Linear Models for Classification"></a>Linear Models for Classification</h4><p>几种线性分类模型如下：<br><img src="/2018/08/23/林轩田机器学习基石笔记（二）/2.jpg" title="Linear Models Revisited"><br>本节讲了用线性回归和逻辑回归来反过来求解线性分类问题的思路。<br>分别用 $error<em>{0/1}$，$error</em>{SQR}$，$error_{CE}$ 表示上述三类模型的误差，下图对这三类误差进行了直观展示：<br><img src="/2018/08/23/林轩田机器学习基石笔记（二）/3.jpg" title="Visualizing Error Functions"></p>
<p>可以看出$error_{0/1}$的上界是由logistic regression模型的error function决定的，所以可以用逻辑回归来求解线性分类问题。不过 PLA 是binary classification，logistic regression是软性分类(可以看作是soft-PLA)，计算某点属于该类的概率。</p>
<p>本节还介绍了随机梯度下降的方法（Stochastic Gradient Descent），每次只迭代一个点。关于其与批量梯度下降（Batch Gradient Descent）的区别<a href="https://www.cnblogs.com/sirius-swu/p/6932583.html" target="_blank" rel="noopener">详见这篇文章</a></p>
<p>最后本节介绍了用线性分类的方法求解多分类问题的两种方法：One­Versus­All(OVA)和One­Versus­One(OVO)。对 OVA，分成多个计算二分类问题，用逻辑回归计算每个点属于各类的概率，取最大概率的类别即可。OVO 主要为了解决类别很多时，正负数据不平衡的问题。它每次只取两类进行binary classification，按照单个点被分成某类的次数排序，取次数最多的即可。</p>
<h4 id="Nonlinear-Transformation"><a href="#Nonlinear-Transformation" class="headerlink" title="Nonlinear Transformation"></a>Nonlinear Transformation</h4><p>本节介绍用非线性的模型来解决分类问题。思路是用特征转换（Feature Transform）把线性不可分的x空间的点特征映射到z空间中去，在 z 空间完成线性分类后再用 x 的多项式替换。例如对于二次的 hypothesis，它包含二次项、一次项和常数项1，即$\phi<em>2(x)$ = (1,$x_1$,$x_2$,$x_1x_2$,$x_1^2$,$x_2^2$)，应使z域的维度与x域中多项式的个数一致，即样本特征维度 d+1 = 6维。<br>非线性变换带来的问题是时间复杂度和空间复杂度的增加，所以应在 $E</em>{in}$满足条件的情况下尽量选用最简单的模型。</p>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>林轩田机器学习基石笔记（一）</title>
    <url>/2018/08/22/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<p>最近在想用哪个机器学习课程入门。说是“入门”，其实掉在坑里很久了，一直都没爬出来，原因是没有系统地把机器学习常用的算法都学一下。上学期选了一门《机器学习》课，老师讲得啰嗦又没有课程逻辑，经常上一秒是朴素贝叶斯下一秒到了深度学习。Andrew Ng的 ML 课程很好，也认真看完了，但还是感觉没学到位，而且课程的算法实现到作业都基于 Matlab，我更想找一个是 Python 的。<a id="more"></a> 虽然也有自己用 Python 重新实现的代码比如这个<a href="https://yoyoyohamapi.gitbooks.io/mit-ml/content/" target="_blank" rel="noopener">Python 版的笔记</a>，但我还是看得有点吃力（好吧我承认自己 Python 基础也很辣鸡）。周志华老师的西瓜书很好，但还是有中式教材的通病，不适合自学，适合做老师讲解的教材补充。最后千辛万苦找到了台大林轩田老师的两门课《机器学习基石》和《机器学习技法》，一看 syllabus 就疯狂爱上。于是决定写博客记笔记督促自己看完吧。<br><img src="/2018/08/22/林轩田机器学习基石笔记（一）/1.jpg" title="The Syllabus of Course"></p>
<p>废话不多说，整个课程分为四个部分：</p>
<ul>
<li>When Can Machine Learn?</li>
<li>Why Can Machine Learn?</li>
<li>How Can Machine Learn?</li>
<li>How Can Machine Learn Better?</li>
</ul>
<p>粗略看了一下 lecture1-8，对 when 和 why 有了相对清晰的认识，笔记如下：</p>
<h4 id="The-Learning-Problem"><a href="#The-Learning-Problem" class="headerlink" title="The Learning Problem"></a>The Learning Problem</h4><p>讲了机器学习的定义：根据模型 H，使用算法 A，在训练样本 D 上进行训练，得到最好的 h，其对应的假设 g 就是我们最后需要的机器学习的模型函数，一般 g 接近于目标函数 f(x—&gt;y)。<br><img src="/2018/08/22/林轩田机器学习基石笔记（一）/2.jpg" title="Practical Definition of Machine Learning"></p>
<h4 id="Learning-to-Answer-Yes-No"><a href="#Learning-to-Answer-Yes-No" class="headerlink" title="Learning to Answer Yes-No"></a>Learning to Answer Yes-No</h4><p>介绍了感知机模型PLA，要解决的问题是点线性分类，目的是求出权值w和阈值threshold($w_0$)，思路是先随机选一条直线分类，对$h(x) = sign(w_0 + w_1x_1 + w_2x_2)$与labels y符号不同的点，即分类错误的点做更正。算法伪代码为：</p>
<blockquote>
<p>if <script type="math/tex">y_n(t) * w_t^Tx_n(t) < 0</script>:<br>    then <script type="math/tex">w_{t+1} \leftarrow w_t + y_n(t) * x_n(t)</script></p>
</blockquote>
<p>然后证明了只要点是线性可分的，PLA总可以停下来并正确分类的。对线性不可分的情况转为 Packet Algorithm，即取分类错误点个数最小的直线作为最终选择的分类直线。</p>
<h4 id="Types-of-Learning"><a href="#Types-of-Learning" class="headerlink" title="Types of Learning"></a>Types of Learning</h4><p>介绍了按照不同方式分类的机器学习：Out Space、Data Label、Protocol、Input Space</p>
<ul>
<li>按照结构输出空间分类：<strong>二元分类</strong>、多元分类、<strong>回归</strong>、结构化学习等</li>
<li>按照输出标签y分类：监督式学习、非监督式学习、半监督式学习和增强学习等</li>
<li>按照学习协议分类：batch learning, online learning, active learning</li>
<li>按照输入X的 features 分类：concrete features, raw features（如图片的像素mxn), abstract features，后两种需要通过 Feature Transform 转为具体的特征</li>
</ul>
<h4 id="Feasibility-of-Learning"><a href="#Feasibility-of-Learning" class="headerlink" title="Feasibility of Learning"></a>Feasibility of Learning</h4><p>分析了机器学习的可行性。首先用没有免费午餐定理(NFL)说明了无法保证一个机器学习算法在训练数据以外的数据集上一定能分类或预测正确，即算法 A 总是最优能使假设 g = f。但根据霍夫丁不等式(Hoeffding’s inequality)，如果固定 h，只要样本 D 的个数 N 足够大，样本和总体的错误率就是PAC的。对 h 个数很多的情况，只要 h 个数 M 是有限的，且 N 足够大，就能保证 $E<em>{in} \approx E</em>{out}$，证明机器学习是可行的。<br><img src="/2018/08/22/林轩田机器学习基石笔记（一）/3.jpg" title="The Statistical Learning Flow"></p>
<h4 id="Training-versus-Testing"><a href="#Training-versus-Testing" class="headerlink" title="Training versus Testing"></a>Training versus Testing</h4><img src="/2018/08/22/林轩田机器学习基石笔记（一）/4.jpg" title="Two Central Questions">
<p>由课程第一部分引出来机器学习的两个核心问题：$E<em>{in}(g) = E</em>{out}(g)$和$E_{in}(g) = 0$，这两个问题刚好对应 test 和 train 两个过程。<br>为解决第一个问题，探讨了 M 个 hypothesis 到底可以划分为多少种。记 M = effective(N)，定义了成长函数$m_H(H)$：使 N 个点二分类个数最多的分类值（上界是 2^N），定义了截断点（break point）：成长函数不等于2^N 时的最小 N。我们猜测成长函数和 N 存在多项式关系，这样对霍夫丁不等式：</p>
<script type="math/tex; mode=display">P[|E_{in}(g) - E_{out}(g)| > \epsilon] \leqslant 2 * M * exp(-2\epsilon^2 N)]</script><p>由于 M 相对 N 的增长速度远小于2^N，第一个问题基本满足。而对足够多的 M 总能找到一个 h 满足$E_{in}(g) \approx 0$，机器学习就是可行的。</p>
<h4 id="Theory-of-Generalization"><a href="#Theory-of-Generalization" class="headerlink" title="Theory of Generalization"></a>Theory of Generalization</h4><p>这节证明了只要存在break point其成长函数$m_H(H)$就满足poly(N)。并将$m_H(H)$代入了Hoffding不等式中，推导出了 VC bound，证明了机器学习的可行性。（具体证明过程略）</p>
<h4 id="The-VC-Dimension"><a href="#The-VC-Dimension" class="headerlink" title="The VC Dimension"></a>The VC Dimension</h4><p>引入了 VC Dimension 的定义：某假设集H能够shatter的最多inputs的个数，即最大完全正确的分类能力，等于break point数减一。以 PLA 算法为例，2D 下$d<em>{vc}$ = 3，高维的$d</em>{vc}$个数为维数加1。从物理意义上讲，VC Dimension 代表了假设空间的分类能力，即反映了H的自由度，也就等于features的个数。<br>$E<em>{out}$, $E</em>{in}$和模型复杂度随$d<em>{vc}$变化的曲线如图：<br><img src="/2018/08/22/林轩田机器学习基石笔记（一）/5.jpg" title="Error vs. VC Dimension"><br>所以选择合适的$d</em>{vc}$，或者features才能使模型具有很好的泛化能力。如2D PLA 下的线性分类，W = {$w_0,w_1,w_2$}，也就是说 input 只需要 3 个 features 就能很好学习。</p>
<h4 id="Noise-and-Error"><a href="#Noise-and-Error" class="headerlink" title="Noise and Error"></a>Noise and Error</h4><p>这节讲了数据集有噪声时机器学习的适用性。<br>引入噪声后，数据集在某点处不是确定分布而变成按照P(y|x)概率分布，加入误差测量后，机器学习模型变为：<br><img src="/2018/08/22/林轩田机器学习基石笔记（一）/6.jpg" title="The New Learning Flow"><br>机器学习cost function常用的 Error 有0/1 error和squared error两类，训练的目的是使得Error($E_{in}$)不断变小。实际问题中，对false accept（误把负类当成正类）和false reject（误把正类当成负类）两类错误应该选择不同的权重进行错误惩罚。</p>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>小鼠脑科学知识库</title>
    <url>/2018/08/17/%E5%B0%8F%E9%BC%A0%E8%84%91%E7%A7%91%E5%AD%A6%E7%9F%A5%E8%AF%86%E5%BA%93/</url>
    <content><![CDATA[<img src="/2018/08/17/小鼠脑科学知识库/1.png">
<blockquote><p>Knowledge Graph: things, not strings.</p>
<footer><strong>Google</strong><cite><a href="https://googleblog.blogspot.com/2012/05/introducing-knowledge-graph-things-not" target="_blank" rel="noopener">Official Google Blog: Introducing the Knowledge Graph: things, not strings</a></cite></footer></blockquote>
 <a id="more"></a>
<h3 id="知识图谱的构建技术综述"><a href="#知识图谱的构建技术综述" class="headerlink" title="知识图谱的构建技术综述"></a>知识图谱的构建技术综述</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>结构化的语义知识库，描述实体间的相互关系，基本单位为 SPO 三元组。</p>
<h4 id="体系架构"><a href="#体系架构" class="headerlink" title="体系架构"></a>体系架构</h4><p>数据层+模式层，如图：<br><img src="/2018/08/17/小鼠脑科学知识库/2.png"></p>
<h4 id="构建流程"><a href="#构建流程" class="headerlink" title="构建流程"></a>构建流程</h4><img src="/2018/08/17/小鼠脑科学知识库/3.png">
<ol>
<li><strong>知识抽取</strong><ul>
<li>实体抽取：即命名实体识别（NER）<ul>
<li>基于词典和规则</li>
<li>基于统计机器学习<ul>
<li>有监督学习方法：SVM, HMM, ME, CRF</li>
</ul>
<ul>
<li>半/弱监督学习方法：基于Bootstrap自助抽样法</li>
</ul>
</li>
<li>基于深度学习的方法</li>
</ul>
</li>
<li>关系抽取<ul>
<li>基于特征向量或核函数的有监督学习</li>
<li>无监督聚类</li>
</ul>
</li>
<li>属性抽取/事件抽取<br>一般可转为关系抽取</li>
</ul>
</li>
<li><p><strong>知识融合</strong></p>
<ul>
<li>实体链接：从文本中抽取得到的实体链接到知识库中对应的正确实体对象<ul>
<li>实体消歧<br>基于包含语义信息的特征向量模型（词袋模型）的聚类</li>
<li>共指消解<br>计算实体相似度并进行聚类</li>
</ul>
</li>
<li>知识合并<ul>
<li>合并关系型数据库<br>RDB2RDF</li>
<li>合并外部数据库<br>数据层的融合和模式层的融合</li>
</ul>
</li>
</ul>
</li>
<li><strong>知识加工</strong><ul>
<li>本体构建<br>人工编辑手动构建，辅以统计机器学习算法自动抽取上下位关系（IsA 实体对）</li>
<li>知识推理<ul>
<li>基于逻辑的推理<br>基于本体概念层（OWL）的推理如 SWRL</li>
<li>基于图的推理<br>神经网络模型或 Path Ranking 算法</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="一个小鼠脑科学知识库的初步搭建流程和技术路线"><a href="#一个小鼠脑科学知识库的初步搭建流程和技术路线" class="headerlink" title="一个小鼠脑科学知识库的初步搭建流程和技术路线"></a>一个小鼠脑科学知识库的初步搭建流程和技术路线</h3><h4 id="知识抽取"><a href="#知识抽取" class="headerlink" title="知识抽取"></a>知识抽取</h4><ul>
<li><strong>半结构化数据</strong><br>Wikipedia 中的脑区和核团信息框：利用 Wikidata api 提取json键值对存入数据库</li>
<li><p><strong>非结构化数据</strong></p>
<ul>
<li>参考 Allen 脑解剖学命名树和 Wikipedia 上的核团介绍构建实体关系表、同义词表，构建基本的 SPO 三元组形成关系网，用图数据库neo4j存储并展示</li>
<li><p>对 pubmed 摘要和全文作自然语言处理补充关系网<br>工具：NLTK, spacy, gensim, Stanford CoreNLP, Apache OpenNLP, AllenNLP</p>
<ul>
<li>分句，分词，词干化处理，去除停用词，词性标注</li>
<li>关键词提取，常用词组统计，tf-idf</li>
<li>句法解析（parsing）和依存路径<br><a href="https://github.com/dasguptar/treelstm.pytorch" target="_blank" rel="noopener">Tree LSTM</a></li>
<li>NER<ol>
<li><a href="https://github.com/Franck-Dernoncourt/NeuroNER" target="_blank" rel="noopener">NeuroNER</a></li>
<li><a href="https://github.com/glample/tagger" target="_blank" rel="noopener">LSTM+CRF</a><br>参考论文<br><a href="https://arxiv.org/pdf/1603.01360.pdf" target="_blank" rel="noopener">Neural Architectures for Named Entity Recognition</a></li>
<li><a href="https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html" target="_blank" rel="noopener">Bi-LSTM+CRF</a><br>参考论文<br><a href="https://arxiv.org/pdf/1508.01991.pdf" target="_blank" rel="noopener">Bidirectional LSTM-CRF Models for Sequence Tagging</a></li>
<li>标注语料库<ul>
<li><a href="http://www.geniaproject.org/genia-corpus" target="_blank" rel="noopener">GENIA corpus</a><br>参考论文<br><a href="https://link.springer.com/content/pdf/10.1007%2F11573036_36.pdf" target="_blank" rel="noopener">Developing a Robust Part-of-Speech Tagger for Biomedical Text</a></li>
</ul>
</li>
</ol>
</li>
<li><p>NRE</p>
<p>基于生物医学本体库的远程监督</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="知识融合"><a href="#知识融合" class="headerlink" title="知识融合"></a>知识融合</h4><ul>
<li><strong>构建领域知识本体</strong><ul>
<li>构建工具：protégé</li>
<li>参考本体资源<ul>
<li>鼠脑结构：<a href="https://bioportal.bioontology.org/ontologies/ABA-AMB" target="_blank" rel="noopener">Allen鼠脑图谱本体</a></li>
<li>鼠脑模拟信息：<a href="https://bioportal.bioontology.org/ontologies/MA" target="_blank" rel="noopener">成年小鼠解剖学本体</a></li>
</ul>
</li>
<li>手工构建流程<br>将核团实体和之间的关系抽象得到本体类，定义分类概念和层级，用 protégé 填充相关联的类别、关系、属性等，确定数据属性和对象属性的取值。然后把本体交由具有神经科学相关知识的研究人员审核。</li>
</ul>
</li>
<li><strong>本体匹配</strong><ul>
<li>模式匹配</li>
<li>实例匹配</li>
</ul>
</li>
</ul>
<h4 id="知识存储"><a href="#知识存储" class="headerlink" title="知识存储"></a>知识存储</h4><ul>
<li><strong>存储方式</strong><br>关系型数据库 MySQL(用 OWL 形式化表达 ER 图(D2RQ))<br>图数据库 Neo4j</li>
<li><strong>分布式计算框架</strong>(可选)<br>Hadoop</li>
</ul>
<h4 id="知识表示"><a href="#知识表示" class="headerlink" title="知识表示"></a>知识表示</h4><ul>
<li><strong>web 服务框架</strong><br>使用 ElasticSearch（动态） 或 Apache Solr（静态）构建</li>
<li><strong>知识推理及排序</strong><br>SWRL</li>
<li><strong>信息展示</strong><br>按照 anatomy(structure, neuron types, input and output, neurochemistry), function, clinical significance三部分，建立类似于<a href="http://www.linked-neuron-data.org/connectome.jsp?link=link3" target="_blank" rel="noopener">LBD</a>的直观连接图，以链接数据的形式展示脑区结构、功能和关联疾病间的关系。</li>
<li><strong>查询方式</strong><ul>
<li>建立查找索引，直接搜索</li>
<li>使用 SPARQL 查询 RDF</li>
</ul>
</li>
</ul>
<h3 id="补充材料：一些机器学习和-NLP-原理算法（待添加）"><a href="#补充材料：一些机器学习和-NLP-原理算法（待添加）" class="headerlink" title="补充材料：一些机器学习和 NLP 原理算法（待添加）"></a>补充材料：一些机器学习和 NLP 原理算法（待添加）</h3><ol>
<li><a href="https://blog.csdn.net/tiffanyrabbit/article/details/72650606" target="_blank" rel="noopener">NLP中的语言模型及文本特征提取算法</a></li>
<li><a href="https://blog.csdn.net/TiffanyRabbit/article/details/72654180" target="_blank" rel="noopener">语言模型系列之N-Gram、NPLM及Word2vec</a></li>
<li><a href="https://blog.csdn.net/TiffanyRabbit/article/details/76445909" target="_blank" rel="noopener">利用sklearn训练LDA主题模型及调参详解</a></li>
<li><a href="https://blog.csdn.net/TiffanyRabbit/article/details/76574009" target="_blank" rel="noopener">利用scikit-learn训练经典分类模型（算法原理与实现）</a></li>
<li><a href="https://blog.csdn.net/scotfield_msn/article/details/69075227" target="_blank" rel="noopener">词嵌入模型</a></li>
<li><a href="https://blog.csdn.net/baimafujinji/article/details/77836142" target="_blank" rel="noopener">Word Embedding与Word2Vec</a></li>
<li><a href="https://www.cnblogs.com/xueyinzhe/p/6936305.html" target="_blank" rel="noopener">文档主题生成模型LDA</a></li>
<li><a href="https://blog.csdn.net/Dark_Scope/article/details/63683686" target="_blank" rel="noopener">隐含马尔可夫模型HMM</a></li>
<li><a href="https://transwarpio.github.io/teaching_ml/2017/08/15/最大熵模型/" target="_blank" rel="noopener">最大熵模型</a></li>
<li><a href="https://www.cnblogs.com/Gabby/p/5344658.html" target="_blank" rel="noopener">EM算法</a></li>
<li><a href="https://blog.csdn.net/aws3217150/article/details/68935789" target="_blank" rel="noopener">条件随机场CRF</a></li>
<li><a href="https://blog.csdn.net/qq_28031525/article/details/79187080" target="_blank" rel="noopener">句法分析方法详解</a></li>
</ol>
]]></content>
      <tags>
        <tag>知识图谱</tag>
      </tags>
  </entry>
  <entry>
    <title>第一篇博客</title>
    <url>/2018/08/12/firstblog/</url>
    <content><![CDATA[<p>在这个互联网+自媒体时代，写博客可能是一件最吃力不讨好的事。</p>
<p>不同于朋友圈和豆瓣上的单纯记录生活，这是件需要投入大量精力“输出”的事，相比快速关注并获取信息的公众号信息流也更传统更笨拙些，不过这也是一个自我整理，锻炼整体逻辑性和文字表达能力的过程。<br> <a id="more"></a><br>而且相较于微信公众号user-unfriendly的文本编辑工具，需要时时费心的排版，还有随处要面临的审查机制，时常404的页面，静态博客可能更好些。</p>
<p>我个人一直很喜欢博客。<br>我觉得博客上的写作者普遍更真诚些，更像一个活生生的人。他们只在自己的领地写自己的生活思考和技能分享，不考虑热点、不考虑粉丝想听什么，单纯地为自己而写。哪怕没有人看，日复一日地总结自己，我觉得这也是件很可贵的事。</p>
<p>第一篇博客，至少要先立个 flag 吧：<br>以后再忙，也要坚持每周至少写一篇。</p>
<p>（懒得排版了，就这样发出去吧。）</p>
<p>mathjs test:<br>Simple inline $a = b + c$.</p>
<script type="math/tex; mode=display">\frac{\partial u}{\partial t}
= h^2 \left( \frac{\partial^2 u}{\partial x^2} +
\frac{\partial^2 u}{\partial y^2} +
\frac{\partial^2 u}{\partial z^2}\right)</script>]]></content>
  </entry>
</search>
